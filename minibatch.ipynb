{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"minibatch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPOR4U8z2GF2rxgD4oOVeEe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0wHEijayUl-","executionInfo":{"status":"ok","timestamp":1639622159812,"user_tz":-540,"elapsed":16197,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"cae8dd3c-f80e-4104-c18a-6c2c6f0b399a"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01BYNxqeyYxD","executionInfo":{"status":"ok","timestamp":1639622160192,"user_tz":-540,"elapsed":388,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"6041da21-94c8-4fb8-da74-bf51cc355893"},"source":["%cd /content/gdrive/MyDrive/ColabNotebooks/brixia\n","# root = \"/content/gdrive/MyDrive/ColabNotebooks/brixia\""],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/ColabNotebooks/brixia\n"]}]},{"cell_type":"code","metadata":{"id":"4Kcz3IVmyY3E"},"source":["#!unzip Brixia_jpg.zip -d /content/gdrive/MyDrive/ColabNotebooks/brixia/DB"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBxD7UzByY7N","executionInfo":{"status":"ok","timestamp":1639539649695,"user_tz":-540,"elapsed":4645,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"a71fa198-47f7-463a-91f2-bc6dd9486362"},"source":["!pip install tensorboardX"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorboardX\n","  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n","\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 20.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.4.1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EnePcR7wydOV","executionInfo":{"status":"ok","timestamp":1639539649696,"user_tz":-540,"elapsed":8,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"0876a022-0718-4307-8226-84e87ec613b1"},"source":["!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/ColabNotebooks/brixia\n"]}]},{"cell_type":"code","metadata":{"id":"C4QYAc0MydQb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"602a4fd7-c5bf-4de2-db75-26ff563e8a15","executionInfo":{"status":"ok","timestamp":1639543168633,"user_tz":-540,"elapsed":3503464,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}}},"source":["import jhmodel32 as M\n","\n","# Train from ChestXpert\n","PATH_TO_IMAGES = \"/content/gdrive/MyDrive/ColabNotebooks/brixia/DB\"\n","WEIGHT_DECAY = 0\n","LEARNING_RATE = 0.00001\n","M.train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY, fine_tune=True, regression=True, freeze=True, adam=True, initial_model_path='/content/gdrive/MyDrive/ColabNotebooks/brixia/checkpoints/checkpoint_best')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Available GPU count:1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["DataParallel(\n","  (module): DenseNet(\n","    (features): Sequential(\n","      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (denseblock1): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition1): _Transition(\n","        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock2): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition2): _Transition(\n","        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock3): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer17): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer18): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer19): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer20): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer21): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer22): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer23): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer24): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition3): _Transition(\n","        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock4): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (classifier): Sequential(\n","      (0): Linear(in_features=1024, out_features=1, bias=True)\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n",")\n","Epoch 1/50\n","----------\n","train epoch 1:loss 69.5873 with data size 3924\n","MAE:  6.051003050650238\n","val epoch 1:loss 47.3680 with data size 564\n","saving\n","Training complete in 2m 7s\n","Epoch 2/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 2:loss 41.0893 with data size 3924\n","MAE:  4.945026233287013\n","val epoch 2:loss 31.7487 with data size 564\n","saving\n","Training complete in 3m 16s\n","Epoch 3/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 3:loss 29.4771 with data size 3924\n","MAE:  4.380525363754508\n","val epoch 3:loss 25.4521 with data size 564\n","saving\n","Training complete in 4m 25s\n","Epoch 4/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 4:loss 22.8172 with data size 3924\n","MAE:  3.724671778942835\n","val epoch 4:loss 19.1636 with data size 564\n","saving\n","Training complete in 5m 33s\n","Epoch 5/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 5:loss 18.7112 with data size 3924\n","MAE:  3.3934205376298716\n","val epoch 5:loss 16.4299 with data size 564\n","saving\n","Training complete in 6m 41s\n","Epoch 6/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 6:loss 15.8379 with data size 3924\n","MAE:  3.1504394770566875\n","val epoch 6:loss 14.7604 with data size 564\n","saving\n","Training complete in 7m 49s\n","Epoch 7/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 7:loss 14.0742 with data size 3924\n","MAE:  3.0460303408351352\n","val epoch 7:loss 13.8552 with data size 564\n","saving\n","Training complete in 8m 57s\n","Epoch 8/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 8:loss 12.2525 with data size 3924\n","MAE:  2.9456935425071005\n","val epoch 8:loss 12.9537 with data size 564\n","saving\n","Training complete in 10m 5s\n","Epoch 9/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 9:loss 11.1132 with data size 3924\n","MAE:  3.038647433069158\n","val epoch 9:loss 13.5173 with data size 564\n","Training complete in 11m 13s\n","Epoch 10/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 10:loss 9.9844 with data size 3924\n","MAE:  2.589641102186754\n","val epoch 10:loss 10.4622 with data size 564\n","saving\n","Training complete in 12m 20s\n","Epoch 11/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 11:loss 8.9677 with data size 3924\n","MAE:  2.5484971408302903\n","val epoch 11:loss 10.0876 with data size 564\n","saving\n","Training complete in 13m 29s\n","Epoch 12/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 12:loss 7.9947 with data size 3924\n","MAE:  2.507026848964499\n","val epoch 12:loss 9.8836 with data size 564\n","saving\n","Training complete in 14m 37s\n","Epoch 13/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 13:loss 7.3191 with data size 3924\n","MAE:  2.3886105138676386\n","val epoch 13:loss 9.0618 with data size 564\n","saving\n","Training complete in 15m 45s\n","Epoch 14/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 14:loss 6.7755 with data size 3924\n","MAE:  2.4955944726705974\n","val epoch 14:loss 9.8562 with data size 564\n","Training complete in 16m 53s\n","Epoch 15/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 15:loss 6.1618 with data size 3924\n","MAE:  2.3469596661574452\n","val epoch 15:loss 8.7108 with data size 564\n","saving\n","Training complete in 18m 1s\n","Epoch 16/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 16:loss 5.6530 with data size 3924\n","MAE:  2.289358036024562\n","val epoch 16:loss 8.2332 with data size 564\n","saving\n","Training complete in 19m 9s\n","Epoch 17/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 17:loss 5.4616 with data size 3924\n","MAE:  2.396796834701342\n","val epoch 17:loss 9.1214 with data size 564\n","Training complete in 20m 17s\n","Epoch 18/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 18:loss 4.7617 with data size 3924\n","MAE:  2.379711647088646\n","val epoch 18:loss 8.9246 with data size 564\n","Training complete in 21m 25s\n","Epoch 19/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 19:loss 4.2876 with data size 3924\n","MAE:  2.239804118660325\n","val epoch 19:loss 8.0333 with data size 564\n","saving\n","Training complete in 22m 33s\n","Epoch 20/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 20:loss 3.8291 with data size 3924\n","MAE:  2.2729005963667066\n","val epoch 20:loss 8.1643 with data size 564\n","Training complete in 23m 41s\n","Epoch 21/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 21:loss 3.7064 with data size 3924\n","MAE:  2.220469930915968\n","val epoch 21:loss 7.7972 with data size 564\n","saving\n","Training complete in 24m 50s\n","Epoch 22/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 22:loss 3.3963 with data size 3924\n","MAE:  2.223972520771179\n","val epoch 22:loss 7.7776 with data size 564\n","saving\n","Training complete in 25m 58s\n","Epoch 23/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 23:loss 2.8019 with data size 3924\n","MAE:  2.2361753431829157\n","val epoch 23:loss 7.9022 with data size 564\n","Training complete in 27m 7s\n","Epoch 24/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 24:loss 3.0521 with data size 3924\n","MAE:  2.269089636307025\n","val epoch 24:loss 8.0824 with data size 564\n","Training complete in 28m 15s\n","Epoch 25/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 25:loss 2.5475 with data size 3924\n","MAE:  2.175852328327531\n","val epoch 25:loss 7.5515 with data size 564\n","saving\n","Training complete in 29m 24s\n","Epoch 26/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 26:loss 2.5084 with data size 3924\n","MAE:  2.1514057016119046\n","val epoch 26:loss 7.3096 with data size 564\n","saving\n","Training complete in 30m 33s\n","Epoch 27/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 27:loss 2.1879 with data size 3924\n","MAE:  2.1762767645484167\n","val epoch 27:loss 7.6037 with data size 564\n","Training complete in 31m 42s\n","Epoch 28/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 28:loss 1.7182 with data size 3924\n","MAE:  2.191231492761496\n","val epoch 28:loss 7.5395 with data size 564\n","Training complete in 32m 50s\n","Epoch 29/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 29:loss 1.7684 with data size 3924\n","MAE:  2.2004275267005813\n","val epoch 29:loss 7.6078 with data size 564\n","Training complete in 33m 59s\n","Epoch 30/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 30:loss 1.7274 with data size 3924\n","MAE:  2.123884873927062\n","val epoch 30:loss 7.2242 with data size 564\n","saving\n","Training complete in 35m 8s\n","Epoch 31/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 31:loss 1.5190 with data size 3924\n","MAE:  2.1462610378161284\n","val epoch 31:loss 7.3399 with data size 564\n","Training complete in 36m 16s\n","Epoch 32/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 32:loss 1.4447 with data size 3924\n","MAE:  2.14372159053195\n","val epoch 32:loss 7.3407 with data size 564\n","Training complete in 37m 25s\n","Epoch 33/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 33:loss 1.5036 with data size 3924\n","MAE:  2.168001372763451\n","val epoch 33:loss 7.4544 with data size 564\n","Training complete in 38m 34s\n","Epoch 34/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 34:loss 1.3345 with data size 3924\n","MAE:  2.1942805616145438\n","val epoch 34:loss 7.6582 with data size 564\n","Training complete in 39m 43s\n","Epoch 35/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 35:loss 1.1777 with data size 3924\n","MAE:  2.1452462033811193\n","val epoch 35:loss 7.2922 with data size 564\n","Training complete in 40m 52s\n","Epoch 36/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 36:loss 1.2952 with data size 3924\n","MAE:  2.123562364290792\n","val epoch 36:loss 7.1969 with data size 564\n","saving\n","Training complete in 42m 1s\n","Epoch 37/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 37:loss 1.5371 with data size 3924\n","MAE:  2.1353533858737204\n","val epoch 37:loss 7.3244 with data size 564\n","Training complete in 43m 10s\n","Epoch 38/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 38:loss 0.9926 with data size 3924\n","MAE:  2.0926565494097717\n","val epoch 38:loss 7.0615 with data size 564\n","saving\n","Training complete in 44m 19s\n","Epoch 39/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 39:loss 1.1485 with data size 3924\n","MAE:  2.103927896377888\n","val epoch 39:loss 7.0916 with data size 564\n","Training complete in 45m 27s\n","Epoch 40/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 40:loss 1.2108 with data size 3924\n","MAE:  2.1076214759485095\n","val epoch 40:loss 7.2074 with data size 564\n","Training complete in 46m 35s\n","Epoch 41/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 41:loss 1.1542 with data size 3924\n","MAE:  2.086401402950287\n","val epoch 41:loss 7.0123 with data size 564\n","saving\n","Training complete in 47m 44s\n","Epoch 42/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 42:loss 1.1667 with data size 3924\n","MAE:  2.1067395716482866\n","val epoch 42:loss 7.0752 with data size 564\n","Training complete in 48m 52s\n","Epoch 43/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 43:loss 1.1388 with data size 3924\n","MAE:  2.1428764553357524\n","val epoch 43:loss 7.3161 with data size 564\n","Training complete in 50m 0s\n","Epoch 44/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 44:loss 1.0041 with data size 3924\n","MAE:  2.099539763485709\n","val epoch 44:loss 7.0753 with data size 564\n","Training complete in 51m 9s\n","Epoch 45/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 45:loss 1.0909 with data size 3924\n","MAE:  2.1202683800712543\n","val epoch 45:loss 7.1814 with data size 564\n","Training complete in 52m 17s\n","Epoch 46/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 46:loss 1.0737 with data size 3924\n","MAE:  2.0788354125428707\n","val epoch 46:loss 6.8521 with data size 564\n","saving\n","Training complete in 53m 25s\n","Epoch 47/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 47:loss 1.1512 with data size 3924\n","MAE:  2.120690138733133\n","val epoch 47:loss 7.1523 with data size 564\n","Training complete in 54m 34s\n","Epoch 48/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 48:loss 0.9769 with data size 3924\n","MAE:  2.080239492739346\n","val epoch 48:loss 6.8656 with data size 564\n","Training complete in 55m 42s\n","Epoch 49/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 49:loss 1.0755 with data size 3924\n","MAE:  2.070537728937805\n","val epoch 49:loss 6.8859 with data size 564\n","Training complete in 56m 50s\n","Epoch 50/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 50:loss 1.0567 with data size 3924\n","MAE:  2.076177654748267\n","val epoch 50:loss 6.8891 with data size 564\n","Training complete in 57m 59s\n","Training complete in 57m 59s\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BnuRzTppydST","executionInfo":{"status":"ok","timestamp":1638346627046,"user_tz":-540,"elapsed":3125326,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"4a11c575-7cd0-4104-cf0a-845ffa464bdc"},"source":["import jhmodel64 as M64\n","\n","# Train from ChestXpert\n","PATH_TO_IMAGES = \"/content/gdrive/MyDrive/ColabNotebooks/brixia/DB\"\n","WEIGHT_DECAY = 0\n","LEARNING_RATE = 0.00001\n","M64.train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY, fine_tune=True, regression=True, freeze=True, adam=True, initial_model_path='/content/gdrive/MyDrive/ColabNotebooks/brixia/checkpoints/checkpoint_best')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Available GPU count:1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["DataParallel(\n","  (module): DenseNet(\n","    (features): Sequential(\n","      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (denseblock1): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition1): _Transition(\n","        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock2): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition2): _Transition(\n","        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock3): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer17): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer18): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer19): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer20): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer21): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer22): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer23): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer24): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition3): _Transition(\n","        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock4): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (classifier): Sequential(\n","      (0): Linear(in_features=1024, out_features=1, bias=True)\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n",")\n","Epoch 1/50\n","----------\n","train epoch 1:loss 81.7880 with data size 3924\n","MAE:  6.818139442447441\n","val epoch 1:loss 62.4445 with data size 564\n","saving\n","Training complete in 2m 47s\n","Epoch 2/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 2:loss 64.1011 with data size 3924\n","MAE:  5.963865192303209\n","val epoch 2:loss 51.5052 with data size 564\n","saving\n","Training complete in 3m 47s\n","Epoch 3/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 3:loss 51.3302 with data size 3924\n","MAE:  5.427936614695804\n","val epoch 3:loss 40.8812 with data size 564\n","saving\n","Training complete in 4m 47s\n","Epoch 4/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 4:loss 37.1374 with data size 3924\n","MAE:  4.849127041981787\n","val epoch 4:loss 31.6834 with data size 564\n","saving\n","Training complete in 5m 47s\n","Epoch 5/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 5:loss 29.1885 with data size 3924\n","MAE:  4.353561223398709\n","val epoch 5:loss 25.8451 with data size 564\n","saving\n","Training complete in 6m 47s\n","Epoch 6/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 6:loss 24.1991 with data size 3924\n","MAE:  3.9306328040646745\n","val epoch 6:loss 21.4794 with data size 564\n","saving\n","Training complete in 7m 47s\n","Epoch 7/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 7:loss 21.1128 with data size 3924\n","MAE:  3.6423952714242835\n","val epoch 7:loss 18.4047 with data size 564\n","saving\n","Training complete in 8m 48s\n","Epoch 8/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 8:loss 18.6826 with data size 3924\n","MAE:  3.4030959987154246\n","val epoch 8:loss 16.5208 with data size 564\n","saving\n","Training complete in 9m 48s\n","Epoch 9/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 9:loss 17.0549 with data size 3924\n","MAE:  3.3339387211578746\n","val epoch 9:loss 15.9576 with data size 564\n","saving\n","Training complete in 10m 47s\n","Epoch 10/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 10:loss 15.7775 with data size 3924\n","MAE:  3.2936277790061124\n","val epoch 10:loss 15.5519 with data size 564\n","saving\n","Training complete in 11m 47s\n","Epoch 11/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 11:loss 14.7976 with data size 3924\n","MAE:  3.2662125483472297\n","val epoch 11:loss 15.5409 with data size 564\n","saving\n","Training complete in 12m 47s\n","Epoch 12/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 12:loss 13.3414 with data size 3924\n","MAE:  3.0439157774804015\n","val epoch 12:loss 13.6763 with data size 564\n","saving\n","Training complete in 13m 47s\n","Epoch 13/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 13:loss 12.5973 with data size 3924\n","MAE:  2.975405695062157\n","val epoch 13:loss 13.1344 with data size 564\n","saving\n","Training complete in 14m 47s\n","Epoch 14/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 14:loss 11.8451 with data size 3924\n","MAE:  2.806238720603023\n","val epoch 14:loss 12.0028 with data size 564\n","saving\n","Training complete in 15m 47s\n","Epoch 15/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 15:loss 11.1773 with data size 3924\n","MAE:  2.852077287509509\n","val epoch 15:loss 12.3496 with data size 564\n","Training complete in 16m 47s\n","Epoch 16/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 16:loss 10.3975 with data size 3924\n","MAE:  2.798026512172205\n","val epoch 16:loss 11.8365 with data size 564\n","saving\n","Training complete in 17m 46s\n","Epoch 17/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 17:loss 9.7196 with data size 3924\n","MAE:  2.7030772670363703\n","val epoch 17:loss 11.2448 with data size 564\n","saving\n","Training complete in 18m 46s\n","Epoch 18/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 18:loss 9.2506 with data size 3924\n","MAE:  2.6185016831819046\n","val epoch 18:loss 10.5629 with data size 564\n","saving\n","Training complete in 19m 46s\n","Epoch 19/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 19:loss 8.5216 with data size 3924\n","MAE:  2.6099384835003114\n","val epoch 19:loss 10.5623 with data size 564\n","saving\n","Training complete in 20m 45s\n","Epoch 20/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 20:loss 8.1242 with data size 3924\n","MAE:  2.602385126529856\n","val epoch 20:loss 10.5548 with data size 564\n","saving\n","Training complete in 21m 45s\n","Epoch 21/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 21:loss 7.6059 with data size 3924\n","MAE:  2.6050398706967104\n","val epoch 21:loss 10.4739 with data size 564\n","saving\n","Training complete in 22m 45s\n","Epoch 22/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 22:loss 7.0825 with data size 3924\n","MAE:  2.6048923217235727\n","val epoch 22:loss 10.4620 with data size 564\n","saving\n","Training complete in 23m 45s\n","Epoch 23/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 23:loss 6.5744 with data size 3924\n","MAE:  2.5935711517190256\n","val epoch 23:loss 10.4701 with data size 564\n","Training complete in 24m 44s\n","Epoch 24/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 24:loss 6.1271 with data size 3924\n","MAE:  2.453324202104663\n","val epoch 24:loss 9.3634 with data size 564\n","saving\n","Training complete in 25m 43s\n","Epoch 25/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 25:loss 5.8537 with data size 3924\n","MAE:  2.4254878014326096\n","val epoch 25:loss 9.1125 with data size 564\n","saving\n","Training complete in 26m 43s\n","Epoch 26/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 26:loss 5.3144 with data size 3924\n","MAE:  2.5629621199893613\n","val epoch 26:loss 10.0813 with data size 564\n","Training complete in 27m 43s\n","Epoch 27/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 27:loss 4.9622 with data size 3924\n","MAE:  2.33269084535592\n","val epoch 27:loss 8.3711 with data size 564\n","saving\n","Training complete in 28m 42s\n","Epoch 28/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 28:loss 4.6009 with data size 3924\n","MAE:  2.512343922616742\n","val epoch 28:loss 9.8922 with data size 564\n","Training complete in 29m 42s\n","Epoch 29/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 29:loss 4.1534 with data size 3924\n","MAE:  2.3267686343784875\n","val epoch 29:loss 8.3638 with data size 564\n","saving\n","Training complete in 30m 42s\n","Epoch 30/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 30:loss 4.2142 with data size 3924\n","MAE:  2.2482646128810044\n","val epoch 30:loss 7.7570 with data size 564\n","saving\n","Training complete in 31m 42s\n","Epoch 31/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 31:loss 3.6972 with data size 3924\n","MAE:  2.282152690812417\n","val epoch 31:loss 7.9426 with data size 564\n","Training complete in 32m 42s\n","Epoch 32/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 32:loss 3.5923 with data size 3924\n","MAE:  2.285780484054951\n","val epoch 32:loss 7.8967 with data size 564\n","Training complete in 33m 41s\n","Epoch 33/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 33:loss 3.1584 with data size 3924\n","MAE:  2.36012768090194\n","val epoch 33:loss 8.6433 with data size 564\n","Training complete in 34m 41s\n","Epoch 34/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 34:loss 3.0851 with data size 3924\n","MAE:  2.3028693426902413\n","val epoch 34:loss 8.0569 with data size 564\n","Training complete in 35m 41s\n","Epoch 35/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 35:loss 2.8470 with data size 3924\n","MAE:  2.339346318855776\n","val epoch 35:loss 8.3023 with data size 564\n","Training complete in 36m 40s\n","Epoch 36/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 36:loss 2.5429 with data size 3924\n","MAE:  2.298884539528096\n","val epoch 36:loss 8.0693 with data size 564\n","Training complete in 37m 40s\n","Epoch 37/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 37:loss 2.5335 with data size 3924\n","MAE:  2.288107469202356\n","val epoch 37:loss 8.0036 with data size 564\n","Training complete in 38m 39s\n","Epoch 38/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 38:loss 2.3315 with data size 3924\n","MAE:  2.2511471389695434\n","val epoch 38:loss 7.6488 with data size 564\n","saving\n","Training complete in 39m 39s\n","Epoch 39/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 39:loss 2.2659 with data size 3924\n","MAE:  2.296705336765723\n","val epoch 39:loss 7.9965 with data size 564\n","Training complete in 40m 39s\n","Epoch 40/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 40:loss 1.9477 with data size 3924\n","MAE:  2.2277416054502868\n","val epoch 40:loss 7.4991 with data size 564\n","saving\n","Training complete in 41m 39s\n","Epoch 41/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 41:loss 1.9487 with data size 3924\n","MAE:  2.226113076645432\n","val epoch 41:loss 7.4914 with data size 564\n","saving\n","Training complete in 42m 39s\n","Epoch 42/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 42:loss 1.9250 with data size 3924\n","MAE:  2.315708340376827\n","val epoch 42:loss 8.3203 with data size 564\n","Training complete in 43m 39s\n","Epoch 43/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 43:loss 1.7522 with data size 3924\n","MAE:  2.2116732324427324\n","val epoch 43:loss 7.4926 with data size 564\n","Training complete in 44m 38s\n","Epoch 44/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 44:loss 1.7257 with data size 3924\n","MAE:  2.238666931589973\n","val epoch 44:loss 7.7505 with data size 564\n","Training complete in 45m 38s\n","Epoch 45/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 45:loss 1.4844 with data size 3924\n","MAE:  2.215313955815344\n","val epoch 45:loss 7.5724 with data size 564\n","Training complete in 46m 37s\n","Epoch 46/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 46:loss 1.2988 with data size 3924\n","MAE:  2.1744524211959635\n","val epoch 46:loss 7.1743 with data size 564\n","saving\n","Training complete in 47m 37s\n","Epoch 47/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 47:loss 1.2237 with data size 3924\n","MAE:  2.1843075630724007\n","val epoch 47:loss 7.2632 with data size 564\n","Training complete in 48m 37s\n","Epoch 48/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 48:loss 1.1724 with data size 3924\n","MAE:  2.1516823501240276\n","val epoch 48:loss 7.1211 with data size 564\n","saving\n","Training complete in 49m 37s\n","Epoch 49/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 49:loss 1.1093 with data size 3924\n","MAE:  2.159124267994301\n","val epoch 49:loss 7.3370 with data size 564\n","Training complete in 50m 37s\n","Epoch 50/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 50:loss 1.1473 with data size 3924\n","MAE:  2.1348016536563423\n","val epoch 50:loss 7.0604 with data size 564\n","saving\n","Training complete in 51m 37s\n","Training complete in 51m 37s\n"]}]},{"cell_type":"code","metadata":{"id":"dVRdgBVVR1TC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"93880d4b-d6d7-47d1-ef42-33b8a95b7590"},"source":["import jhmodel8 as M8\n","\n","# Train from ChestXpert\n","PATH_TO_IMAGES = \"/content/gdrive/MyDrive/ColabNotebooks/brixia/DB\"\n","WEIGHT_DECAY = 0\n","LEARNING_RATE = 0.00001\n","M8.train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY, fine_tune=True, regression=True, freeze=True, adam=True, initial_model_path='/content/gdrive/MyDrive/ColabNotebooks/brixia/checkpoints/checkpoint_best')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["DataParallel(\n","  (module): DenseNet(\n","    (features): Sequential(\n","      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (denseblock1): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition1): _Transition(\n","        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock2): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition2): _Transition(\n","        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock3): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer17): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer18): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer19): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer20): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer21): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer22): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer23): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer24): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition3): _Transition(\n","        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock4): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (classifier): Sequential(\n","      (0): Linear(in_features=1024, out_features=1, bias=True)\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n",")\n","Epoch 1/50\n","----------\n","train epoch 1:loss 50.9335 with data size 3924\n","MAE:  4.0525566267893245\n","val epoch 1:loss 22.3922 with data size 564\n","saving\n","Training complete in 2m 8s\n","Epoch 2/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 2:loss 21.9894 with data size 3924\n","MAE:  3.187752623560158\n","val epoch 2:loss 14.8327 with data size 564\n","saving\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n","    send_bytes(obj)\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n","    self._send_bytes(m[offset:offset + size])\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n","    self._send(header + buf)\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n","    n = write(self._handle, buf)\n","OSError: [Errno 9] Bad file descriptor\n","Exception in thread QueueFeederThread:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n","    close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n","    self._close()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n","    _close(self._handle)\n","OSError: [Errno 9] Bad file descriptor\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n","    queue_sem.release()\n","ValueError: semaphore or lock released too many times\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training complete in 4m 15s\n","Epoch 3/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 3:loss 13.7504 with data size 3924\n","MAE:  2.712121350566876\n","val epoch 3:loss 11.3094 with data size 564\n","saving\n","Training complete in 6m 23s\n","Epoch 4/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 4:loss 10.9446 with data size 3924\n","MAE:  2.467491605603103\n","val epoch 4:loss 9.4984 with data size 564\n","saving\n","Training complete in 8m 30s\n","Epoch 5/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 5:loss 9.3009 with data size 3924\n","MAE:  2.029360491637114\n","val epoch 5:loss 6.6697 with data size 564\n","saving\n","Training complete in 10m 37s\n","Epoch 6/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 6:loss 8.4120 with data size 3924\n","MAE:  2.2211926884896367\n","val epoch 6:loss 7.8106 with data size 564\n","Training complete in 12m 44s\n","Epoch 7/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 7:loss 7.7493 with data size 3924\n","MAE:  2.3960600146059448\n","val epoch 7:loss 9.1775 with data size 564\n","Training complete in 14m 51s\n","Epoch 8/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 8:loss 7.6411 with data size 3924\n","MAE:  2.230624276484158\n","val epoch 8:loss 7.9036 with data size 564\n","Training complete in 16m 58s\n","Epoch 9/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 9:loss 6.7925 with data size 3924\n","MAE:  2.1441053806255894\n","val epoch 9:loss 7.1577 with data size 564\n","Training complete in 19m 5s\n","Epoch 10/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 10:loss 5.3649 with data size 3924\n","MAE:  2.099613667739859\n","val epoch 10:loss 7.0807 with data size 564\n","Training complete in 21m 13s\n","Epoch 11/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 11:loss 4.5169 with data size 3924\n","MAE:  2.082840832174881\n","val epoch 11:loss 6.9564 with data size 564\n","Training complete in 23m 21s\n","Epoch 12/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 12:loss 4.3543 with data size 3924\n","MAE:  2.0109302039053425\n","val epoch 12:loss 6.5133 with data size 564\n","saving\n","Training complete in 25m 28s\n","Epoch 13/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 13:loss 3.8381 with data size 3924\n","MAE:  2.010941293698253\n","val epoch 13:loss 6.6287 with data size 564\n","Training complete in 27m 36s\n","Epoch 14/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 14:loss 3.5515 with data size 3924\n","MAE:  2.0316193403795046\n","val epoch 14:loss 6.7545 with data size 564\n","Training complete in 29m 44s\n","Epoch 15/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 15:loss 3.2199 with data size 3924\n","MAE:  2.102246735324251\n","val epoch 15:loss 7.0446 with data size 564\n","Training complete in 31m 51s\n","Epoch 16/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 16:loss 3.0276 with data size 3924\n","MAE:  2.087129067819473\n","val epoch 16:loss 6.9553 with data size 564\n","Training complete in 33m 59s\n","Epoch 17/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 17:loss 2.8792 with data size 3924\n","MAE:  2.483450174860075\n","val epoch 17:loss 12.1621 with data size 564\n","Training complete in 36m 6s\n","Epoch 18/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 18:loss 2.6851 with data size 3924\n","MAE:  2.060860180738547\n","val epoch 18:loss 6.8264 with data size 564\n","Training complete in 38m 13s\n","Epoch 19/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 19:loss 2.6275 with data size 3924\n","MAE:  2.213919425687046\n","val epoch 19:loss 8.6660 with data size 564\n","Training complete in 40m 21s\n","Epoch 20/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 20:loss 2.5900 with data size 3924\n","MAE:  2.078568248936893\n","val epoch 20:loss 6.9295 with data size 564\n","Training complete in 42m 28s\n","Epoch 21/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 21:loss 2.2692 with data size 3924\n","MAE:  2.1034100093343793\n","val epoch 21:loss 7.2196 with data size 564\n","Training complete in 44m 35s\n","Epoch 22/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 22:loss 2.1701 with data size 3924\n","MAE:  2.0344628637445203\n","val epoch 22:loss 6.6863 with data size 564\n","Training complete in 46m 42s\n","Epoch 23/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 23:loss 2.1783 with data size 3924\n","MAE:  2.2516254971847465\n","val epoch 23:loss 8.2360 with data size 564\n","Training complete in 48m 49s\n","Epoch 24/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 24:loss 1.9983 with data size 3924\n","MAE:  2.498715851244563\n","val epoch 24:loss 11.6663 with data size 564\n","Training complete in 50m 56s\n","Epoch 25/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 25:loss 2.0489 with data size 3924\n","MAE:  1.988984427236496\n","val epoch 25:loss 6.3804 with data size 564\n","saving\n","Training complete in 53m 4s\n","Epoch 26/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 26:loss 1.8393 with data size 3924\n","MAE:  2.007961060614028\n","val epoch 26:loss 6.4985 with data size 564\n","Training complete in 55m 11s\n","Epoch 27/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 27:loss 1.5071 with data size 3924\n","MAE:  2.0590768628192286\n","val epoch 27:loss 6.7140 with data size 564\n","Training complete in 57m 19s\n","Epoch 28/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 28:loss 1.7218 with data size 3924\n","MAE:  2.049609293434637\n","val epoch 28:loss 6.7395 with data size 564\n","Training complete in 59m 25s\n","Epoch 29/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 29:loss 1.4980 with data size 3924\n","MAE:  2.0322608054675\n","val epoch 29:loss 6.6756 with data size 564\n","Training complete in 61m 31s\n","Epoch 30/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 30:loss 1.6367 with data size 3924\n","MAE:  2.1390918956158007\n","val epoch 30:loss 7.2542 with data size 564\n","Training complete in 63m 37s\n","Epoch 31/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 31:loss 1.3726 with data size 3924\n","MAE:  1.9921405757149906\n","val epoch 31:loss 6.3583 with data size 564\n","saving\n","Training complete in 65m 44s\n","Epoch 32/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 32:loss 1.5508 with data size 3924\n","MAE:  2.044299541899921\n","val epoch 32:loss 6.7080 with data size 564\n","Training complete in 67m 50s\n","Epoch 33/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 33:loss 1.3255 with data size 3924\n","MAE:  2.003577955351178\n","val epoch 33:loss 6.4432 with data size 564\n","Training complete in 69m 56s\n","Epoch 34/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 34:loss 1.2462 with data size 3924\n","MAE:  2.057946564827828\n","val epoch 34:loss 6.8051 with data size 564\n","Training complete in 72m 2s\n","Epoch 35/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 35:loss 1.2347 with data size 3924\n","MAE:  1.998402947520322\n","val epoch 35:loss 6.4903 with data size 564\n","Training complete in 74m 8s\n","Epoch 36/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 36:loss 1.1431 with data size 3924\n","MAE:  2.0609654962276736\n","val epoch 36:loss 6.7795 with data size 564\n","Training complete in 76m 14s\n","Epoch 37/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 37:loss 1.1219 with data size 3924\n","MAE:  2.136609485121906\n","val epoch 37:loss 7.4851 with data size 564\n","Training complete in 78m 20s\n","Epoch 38/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 38:loss 1.1179 with data size 3924\n","MAE:  1.9960577889537134\n","val epoch 38:loss 6.5621 with data size 564\n","Training complete in 80m 27s\n","Epoch 39/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 39:loss 1.0252 with data size 3924\n","MAE:  2.0689071290036467\n","val epoch 39:loss 7.0574 with data size 564\n","Training complete in 82m 33s\n","Epoch 40/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 40:loss 0.9961 with data size 3924\n","MAE:  2.0140581190989004\n","val epoch 40:loss 6.5650 with data size 564\n","Training complete in 84m 40s\n","Epoch 41/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 41:loss 0.9592 with data size 3924\n","MAE:  2.0110585088271926\n","val epoch 41:loss 6.4934 with data size 564\n","Training complete in 86m 46s\n","Epoch 42/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 42:loss 0.9112 with data size 3924\n","MAE:  2.0979969141383967\n","val epoch 42:loss 7.1726 with data size 564\n","Training complete in 88m 54s\n","Epoch 43/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 43:loss 0.9725 with data size 3924\n","MAE:  2.01962735348031\n","val epoch 43:loss 6.7286 with data size 564\n","Training complete in 91m 0s\n","Epoch 44/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 44:loss 0.9061 with data size 3924\n","MAE:  1.9879937926257782\n","val epoch 44:loss 6.2883 with data size 564\n","saving\n","Training complete in 93m 8s\n","Epoch 45/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FY-PawEk4pDt","outputId":"6585376a-c9a2-49bc-f93f-d71e822c4713"},"source":["import jhmodel128 as M128\n","\n","# Train from ChestXpert\n","PATH_TO_IMAGES = \"/content/gdrive/MyDrive/ColabNotebooks/brixia/DB\"\n","WEIGHT_DECAY = 0\n","LEARNING_RATE = 0.00001\n","M128.train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY, fine_tune=True, regression=True, freeze=True, adam=True, initial_model_path='/content/gdrive/MyDrive/ColabNotebooks/brixia/checkpoints/checkpoint_best')"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["DataParallel(\n","  (module): DenseNet(\n","    (features): Sequential(\n","      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (denseblock1): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition1): _Transition(\n","        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock2): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition2): _Transition(\n","        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock3): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer17): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer18): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer19): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer20): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer21): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer22): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer23): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer24): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition3): _Transition(\n","        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock4): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (classifier): Sequential(\n","      (0): Linear(in_features=1024, out_features=1, bias=True)\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n",")\n","Epoch 1/50\n","----------\n","train epoch 1:loss 83.2544 with data size 3924\n","MAE:  7.494488923265499\n","val epoch 1:loss 70.5832 with data size 564\n","saving\n","Training complete in 1m 3s\n","Epoch 2/50\n","----------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["train epoch 2:loss 72.1938 with data size 3924\n","MAE:  6.9964748665769685\n","val epoch 2:loss 61.0519 with data size 564\n","saving\n","Training complete in 2m 7s\n","Epoch 3/50\n","----------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["train epoch 3:loss 59.2839 with data size 3924\n","MAE:  6.323935712463097\n","val epoch 3:loss 50.0139 with data size 564\n","saving\n","Training complete in 3m 10s\n","Epoch 4/50\n","----------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 4:loss 48.4268 with data size 3924\n","MAE:  5.77663442794009\n","val epoch 4:loss 42.1096 with data size 564\n","saving\n","Training complete in 4m 13s\n","Epoch 5/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 5:loss 41.2240 with data size 3924\n","MAE:  5.26315611079071\n","val epoch 5:loss 35.5534 with data size 564\n","saving\n","Training complete in 5m 15s\n","Epoch 6/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 6:loss 36.3210 with data size 3924\n","MAE:  5.087288934506264\n","val epoch 6:loss 33.4024 with data size 564\n","saving\n","Training complete in 6m 18s\n","Epoch 7/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 7:loss 32.7078 with data size 3924\n","MAE:  4.830594890348032\n","val epoch 7:loss 30.5929 with data size 564\n","saving\n","Training complete in 7m 24s\n","Epoch 8/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 8:loss 29.7448 with data size 3924\n","MAE:  4.6025783092787504\n","val epoch 8:loss 28.1159 with data size 564\n","saving\n","Training complete in 8m 27s\n","Epoch 9/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 9:loss 27.2046 with data size 3924\n","MAE:  4.486479728727053\n","val epoch 9:loss 26.9220 with data size 564\n","saving\n","Training complete in 9m 29s\n","Epoch 10/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 10:loss 25.1717 with data size 3924\n","MAE:  4.310405835803229\n","val epoch 10:loss 25.0832 with data size 564\n","saving\n","Training complete in 10m 31s\n","Epoch 11/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 11:loss 23.4396 with data size 3924\n","MAE:  4.096467597317611\n","val epoch 11:loss 23.0209 with data size 564\n","saving\n","Training complete in 11m 34s\n","Epoch 12/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 12:loss 22.0736 with data size 3924\n","MAE:  3.9864140959086676\n","val epoch 12:loss 22.0377 with data size 564\n","saving\n","Training complete in 12m 37s\n","Epoch 13/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 13:loss 20.5971 with data size 3924\n","MAE:  3.8917107719710056\n","val epoch 13:loss 21.2480 with data size 564\n","saving\n","Training complete in 13m 39s\n","Epoch 14/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 14:loss 19.7283 with data size 3924\n","MAE:  3.9389077283124974\n","val epoch 14:loss 21.8541 with data size 564\n","Training complete in 14m 41s\n","Epoch 15/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 15:loss 18.7285 with data size 3924\n","MAE:  3.7320604529099684\n","val epoch 15:loss 20.0919 with data size 564\n","saving\n","Training complete in 15m 45s\n","Epoch 16/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 16:loss 17.8728 with data size 3924\n","MAE:  3.6426731119500406\n","val epoch 16:loss 18.9027 with data size 564\n","saving\n","Training complete in 16m 47s\n","Epoch 17/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 17:loss 17.0543 with data size 3924\n","MAE:  3.5849455612189804\n","val epoch 17:loss 18.3198 with data size 564\n","saving\n","Training complete in 17m 51s\n","Epoch 18/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 18:loss 16.2035 with data size 3924\n","MAE:  3.4547342256649483\n","val epoch 18:loss 17.1377 with data size 564\n","saving\n","Training complete in 18m 53s\n","Epoch 19/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 19:loss 15.5141 with data size 3924\n","MAE:  3.4097503070712936\n","val epoch 19:loss 16.7687 with data size 564\n","saving\n","Training complete in 19m 56s\n","Epoch 20/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 20:loss 14.8543 with data size 3924\n","MAE:  3.2542956999192634\n","val epoch 20:loss 15.5824 with data size 564\n","saving\n","Training complete in 20m 58s\n","Epoch 21/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 21:loss 14.3273 with data size 3924\n","MAE:  3.2716209123584816\n","val epoch 21:loss 15.6240 with data size 564\n","Training complete in 21m 59s\n","Epoch 22/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 22:loss 13.5980 with data size 3924\n","MAE:  3.2228532682919333\n","val epoch 22:loss 15.1701 with data size 564\n","saving\n","Training complete in 23m 1s\n","Epoch 23/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 23:loss 13.1102 with data size 3924\n","MAE:  3.244620105317721\n","val epoch 23:loss 15.3774 with data size 564\n","Training complete in 24m 3s\n","Epoch 24/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 24:loss 12.5231 with data size 3924\n","MAE:  3.0594491426245116\n","val epoch 24:loss 14.0423 with data size 564\n","saving\n","Training complete in 25m 4s\n","Epoch 25/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 25:loss 12.0548 with data size 3924\n","MAE:  3.114369015163141\n","val epoch 25:loss 14.6117 with data size 564\n","Training complete in 26m 7s\n","Epoch 26/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 26:loss 11.5839 with data size 3924\n","MAE:  3.0875563750783286\n","val epoch 26:loss 14.3264 with data size 564\n","Training complete in 27m 8s\n","Epoch 27/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 27:loss 11.0511 with data size 3924\n","MAE:  3.0975479160217527\n","val epoch 27:loss 14.3719 with data size 564\n","Training complete in 28m 10s\n","Epoch 28/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 28:loss 10.5556 with data size 3924\n","MAE:  3.0164860247086125\n","val epoch 28:loss 13.9576 with data size 564\n","saving\n","Training complete in 29m 12s\n","Epoch 29/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 29:loss 10.2378 with data size 3924\n","MAE:  2.948297932585503\n","val epoch 29:loss 13.4089 with data size 564\n","saving\n","Training complete in 30m 15s\n","Epoch 30/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 30:loss 9.6996 with data size 3924\n","MAE:  2.971717359062205\n","val epoch 30:loss 13.5417 with data size 564\n","Training complete in 31m 17s\n","Epoch 31/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 31:loss 9.2518 with data size 3924\n","MAE:  2.9175330634420433\n","val epoch 31:loss 13.0585 with data size 564\n","saving\n","Training complete in 32m 19s\n","Epoch 32/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 32:loss 8.9911 with data size 3924\n","MAE:  2.931632212501891\n","val epoch 32:loss 13.4577 with data size 564\n","Training complete in 33m 21s\n","Epoch 33/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 33:loss 8.4525 with data size 3924\n","MAE:  2.9462275483665312\n","val epoch 33:loss 13.4464 with data size 564\n","Training complete in 34m 22s\n","Epoch 34/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 34:loss 8.2657 with data size 3924\n","MAE:  2.781959765240656\n","val epoch 34:loss 12.1650 with data size 564\n","saving\n","Training complete in 35m 24s\n","Epoch 35/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 35:loss 7.7953 with data size 3924\n","MAE:  2.8454146757193493\n","val epoch 35:loss 12.4791 with data size 564\n","Training complete in 36m 26s\n","Epoch 36/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 36:loss 7.5592 with data size 3924\n","MAE:  2.8185703065378447\n","val epoch 36:loss 12.4836 with data size 564\n","Training complete in 37m 28s\n","Epoch 37/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 37:loss 7.4491 with data size 3924\n","MAE:  2.868222155046801\n","val epoch 37:loss 12.8391 with data size 564\n","Training complete in 38m 30s\n","Epoch 38/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 38:loss 7.1142 with data size 3924\n","MAE:  2.8260286141794624\n","val epoch 38:loss 12.4318 with data size 564\n","Training complete in 39m 31s\n","Epoch 39/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 39:loss 6.8348 with data size 3924\n","MAE:  2.6671675748013435\n","val epoch 39:loss 11.1263 with data size 564\n","saving\n","Training complete in 40m 33s\n","Epoch 40/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 40:loss 6.7276 with data size 3924\n","MAE:  2.8914375714693508\n","val epoch 40:loss 12.9218 with data size 564\n","Training complete in 41m 35s\n","Epoch 41/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 41:loss 6.4277 with data size 3924\n","MAE:  2.771690996614754\n","val epoch 41:loss 11.9891 with data size 564\n","Training complete in 42m 37s\n","Epoch 42/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 42:loss 6.2045 with data size 3924\n","MAE:  2.633690463207292\n","val epoch 42:loss 10.9418 with data size 564\n","saving\n","Training complete in 43m 39s\n","Epoch 43/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 43:loss 5.9081 with data size 3924\n","MAE:  2.6274410269639277\n","val epoch 43:loss 10.9501 with data size 564\n","Training complete in 44m 40s\n","Epoch 44/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 44:loss 5.7083 with data size 3924\n","MAE:  2.642939145260669\n","val epoch 44:loss 10.9690 with data size 564\n","Training complete in 45m 42s\n","Epoch 45/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 45:loss 5.5075 with data size 3924\n","MAE:  2.610446974846488\n","val epoch 45:loss 10.7171 with data size 564\n","saving\n","Training complete in 46m 44s\n","Epoch 46/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 46:loss 5.2998 with data size 3924\n","MAE:  2.646839604415792\n","val epoch 46:loss 10.9748 with data size 564\n","Training complete in 47m 46s\n","Epoch 47/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 47:loss 5.0516 with data size 3924\n","MAE:  2.5707030856968665\n","val epoch 47:loss 10.3759 with data size 564\n","saving\n","Training complete in 48m 48s\n","Epoch 48/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 48:loss 5.0157 with data size 3924\n","MAE:  2.5998093314098973\n","val epoch 48:loss 10.6076 with data size 564\n","Training complete in 49m 50s\n","Epoch 49/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 49:loss 4.7469 with data size 3924\n","MAE:  2.6368555952893926\n","val epoch 49:loss 10.8510 with data size 564\n","Training complete in 50m 52s\n","Epoch 50/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","metadata":{"id":"ifUf4AQa4tVy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638434523344,"user_tz":-540,"elapsed":4504741,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"7cd84d32-0894-4d21-f2b1-3872f3f56440"},"source":["import jhmodel16 as M16\n","\n","# Train from ChestXpert\n","PATH_TO_IMAGES = \"/content/gdrive/MyDrive/ColabNotebooks/brixia/DB\"\n","WEIGHT_DECAY = 0\n","LEARNING_RATE = 0.00001\n","M16.train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY, fine_tune=True, regression=True, freeze=True, adam=True, initial_model_path='/content/gdrive/MyDrive/ColabNotebooks/brixia/checkpoints/checkpoint_best')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Available GPU count:1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["DataParallel(\n","  (module): DenseNet(\n","    (features): Sequential(\n","      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (denseblock1): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition1): _Transition(\n","        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock2): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition2): _Transition(\n","        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock3): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer17): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer18): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer19): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer20): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer21): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer22): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer23): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer24): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition3): _Transition(\n","        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock4): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (classifier): Sequential(\n","      (0): Linear(in_features=1024, out_features=1, bias=True)\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n",")\n","Epoch 1/50\n","----------\n","train epoch 1:loss 64.4693 with data size 3924\n","MAE:  5.554526600210617\n","val epoch 1:loss 39.4155 with data size 564\n","saving\n","Training complete in 5m 25s\n","Epoch 2/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 2:loss 32.2190 with data size 3924\n","MAE:  4.002160869050005\n","val epoch 2:loss 21.7416 with data size 564\n","saving\n","Training complete in 6m 50s\n","Epoch 3/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 3:loss 21.3049 with data size 3924\n","MAE:  3.464590981533315\n","val epoch 3:loss 17.0443 with data size 564\n","saving\n","Training complete in 8m 15s\n","Epoch 4/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 4:loss 15.9553 with data size 3924\n","MAE:  3.094967742060833\n","val epoch 4:loss 14.1164 with data size 564\n","saving\n","Training complete in 9m 40s\n","Epoch 5/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 5:loss 13.4780 with data size 3924\n","MAE:  3.038693627884202\n","val epoch 5:loss 13.5969 with data size 564\n","saving\n","Training complete in 11m 5s\n","Epoch 6/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 6:loss 11.2908 with data size 3924\n","MAE:  2.5421982065688633\n","val epoch 6:loss 10.1873 with data size 564\n","saving\n","Training complete in 12m 31s\n","Epoch 7/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 7:loss 9.9956 with data size 3924\n","MAE:  2.4630142792213894\n","val epoch 7:loss 9.5136 with data size 564\n","saving\n","Training complete in 13m 56s\n","Epoch 8/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 8:loss 8.9682 with data size 3924\n","MAE:  2.295174483447633\n","val epoch 8:loss 8.1417 with data size 564\n","saving\n","Training complete in 15m 21s\n","Epoch 9/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 9:loss 7.9412 with data size 3924\n","MAE:  2.15268967965129\n","val epoch 9:loss 7.2226 with data size 564\n","saving\n","Training complete in 16m 46s\n","Epoch 10/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 10:loss 6.4668 with data size 3924\n","MAE:  2.129386493236038\n","val epoch 10:loss 7.0310 with data size 564\n","saving\n","Training complete in 18m 11s\n","Epoch 11/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 11:loss 5.6224 with data size 3924\n","MAE:  2.17909800787027\n","val epoch 11:loss 7.3490 with data size 564\n","Training complete in 19m 36s\n","Epoch 12/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 12:loss 4.8609 with data size 3924\n","MAE:  2.1537354185860207\n","val epoch 12:loss 7.1395 with data size 564\n","Training complete in 21m 1s\n","Epoch 13/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 13:loss 4.4106 with data size 3924\n","MAE:  2.2045643413109137\n","val epoch 13:loss 7.6241 with data size 564\n","Training complete in 22m 26s\n","Epoch 14/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 14:loss 3.7973 with data size 3924\n","MAE:  2.1669232462737575\n","val epoch 14:loss 7.3180 with data size 564\n","Training complete in 23m 51s\n","Epoch 15/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 15:loss 3.6036 with data size 3924\n","MAE:  2.231089004176728\n","val epoch 15:loss 7.7874 with data size 564\n","Training complete in 25m 15s\n","Epoch 16/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 16:loss 3.1919 with data size 3924\n","MAE:  2.0770262956407897\n","val epoch 16:loss 6.8075 with data size 564\n","saving\n","Training complete in 26m 41s\n","Epoch 17/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 17:loss 2.9964 with data size 3924\n","MAE:  2.0833162672764867\n","val epoch 17:loss 6.9124 with data size 564\n","Training complete in 28m 5s\n","Epoch 18/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 18:loss 2.9648 with data size 3924\n","MAE:  2.0888834866226142\n","val epoch 18:loss 6.9326 with data size 564\n","Training complete in 29m 30s\n","Epoch 19/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 19:loss 2.7208 with data size 3924\n","MAE:  2.0834929468572563\n","val epoch 19:loss 6.8940 with data size 564\n","Training complete in 30m 57s\n","Epoch 20/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 20:loss 2.6853 with data size 3924\n","MAE:  2.125939584502303\n","val epoch 20:loss 7.4040 with data size 564\n","Training complete in 32m 22s\n","Epoch 21/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 21:loss 2.6631 with data size 3924\n","MAE:  2.1055621718261257\n","val epoch 21:loss 7.0488 with data size 564\n","Training complete in 33m 47s\n","Epoch 22/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 22:loss 2.3465 with data size 3924\n","MAE:  1.9887096089251497\n","val epoch 22:loss 6.4297 with data size 564\n","saving\n","Training complete in 35m 13s\n","Epoch 23/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 23:loss 1.7535 with data size 3924\n","MAE:  2.041506882360641\n","val epoch 23:loss 6.6510 with data size 564\n","Training complete in 36m 38s\n","Epoch 24/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 24:loss 1.9065 with data size 3924\n","MAE:  2.0895778151321496\n","val epoch 24:loss 7.0750 with data size 564\n","Training complete in 38m 3s\n","Epoch 25/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 25:loss 1.8112 with data size 3924\n","MAE:  2.004697495344894\n","val epoch 25:loss 6.5232 with data size 564\n","Training complete in 39m 27s\n","Epoch 26/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 26:loss 1.5870 with data size 3924\n","MAE:  1.9867572478474456\n","val epoch 26:loss 6.4621 with data size 564\n","Training complete in 40m 52s\n","Epoch 27/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 27:loss 1.5271 with data size 3924\n","MAE:  2.00603772705117\n","val epoch 27:loss 6.5839 with data size 564\n","Training complete in 42m 17s\n","Epoch 28/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 28:loss 1.5425 with data size 3924\n","MAE:  2.0027864411380802\n","val epoch 28:loss 6.4268 with data size 564\n","saving\n","Training complete in 43m 42s\n","Epoch 29/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 29:loss 1.3773 with data size 3924\n","MAE:  1.99274073361505\n","val epoch 29:loss 6.4627 with data size 564\n","Training complete in 45m 8s\n","Epoch 30/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 30:loss 1.5079 with data size 3924\n","MAE:  1.9860926976836955\n","val epoch 30:loss 6.3599 with data size 564\n","saving\n","Training complete in 46m 33s\n","Epoch 31/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 31:loss 1.5184 with data size 3924\n","MAE:  2.0033991768797663\n","val epoch 31:loss 6.4679 with data size 564\n","Training complete in 47m 58s\n","Epoch 32/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 32:loss 1.5653 with data size 3924\n","MAE:  2.003082602951966\n","val epoch 32:loss 6.5564 with data size 564\n","Training complete in 49m 23s\n","Epoch 33/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 33:loss 1.2846 with data size 3924\n","MAE:  1.9821841909107587\n","val epoch 33:loss 6.4481 with data size 564\n","Training complete in 50m 48s\n","Epoch 34/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 34:loss 1.3594 with data size 3924\n","MAE:  2.075479971872418\n","val epoch 34:loss 6.9330 with data size 564\n","Training complete in 52m 13s\n","Epoch 35/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 35:loss 1.3896 with data size 3924\n","MAE:  2.010647307896445\n","val epoch 35:loss 6.5676 with data size 564\n","Training complete in 53m 38s\n","Epoch 36/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 36:loss 1.3832 with data size 3924\n","MAE:  1.9647786953347794\n","val epoch 36:loss 6.1816 with data size 564\n","saving\n","Training complete in 55m 3s\n","Epoch 37/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 37:loss 1.3658 with data size 3924\n","MAE:  1.9350294914717476\n","val epoch 37:loss 6.1363 with data size 564\n","saving\n","Training complete in 56m 29s\n","Epoch 38/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 38:loss 1.3378 with data size 3924\n","MAE:  1.97832546145358\n","val epoch 38:loss 6.2652 with data size 564\n","Training complete in 57m 54s\n","Epoch 39/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 39:loss 1.3837 with data size 3924\n","MAE:  1.964294137571879\n","val epoch 39:loss 6.1947 with data size 564\n","Training complete in 59m 19s\n","Epoch 40/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 40:loss 1.1804 with data size 3924\n","MAE:  1.983064960344886\n","val epoch 40:loss 6.2489 with data size 564\n","Training complete in 60m 43s\n","Epoch 41/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 41:loss 1.2132 with data size 3924\n","MAE:  1.9340324963026858\n","val epoch 41:loss 6.0318 with data size 564\n","saving\n","Training complete in 62m 8s\n","Epoch 42/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 42:loss 1.3126 with data size 3924\n","MAE:  1.9622517772653\n","val epoch 42:loss 6.0499 with data size 564\n","Training complete in 63m 33s\n","Epoch 43/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 43:loss 1.3617 with data size 3924\n","MAE:  2.040095575371797\n","val epoch 43:loss 6.6537 with data size 564\n","Training complete in 64m 58s\n","Epoch 44/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 44:loss 1.1356 with data size 3924\n","MAE:  1.966534030881334\n","val epoch 44:loss 6.0807 with data size 564\n","Training complete in 66m 23s\n","Epoch 45/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 45:loss 1.1500 with data size 3924\n","MAE:  2.0324121372813875\n","val epoch 45:loss 6.5532 with data size 564\n","Training complete in 67m 48s\n","Epoch 46/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 46:loss 1.2601 with data size 3924\n","MAE:  1.993624453531935\n","val epoch 46:loss 6.1649 with data size 564\n","Training complete in 69m 12s\n","Epoch 47/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 47:loss 1.0346 with data size 3924\n","MAE:  1.9828726423653305\n","val epoch 47:loss 6.1807 with data size 564\n","Training complete in 70m 37s\n","Epoch 48/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 48:loss 1.1566 with data size 3924\n","MAE:  1.9711449335903881\n","val epoch 48:loss 6.1723 with data size 564\n","Training complete in 72m 2s\n","Epoch 49/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 49:loss 1.1175 with data size 3924\n","MAE:  1.9884801922326392\n","val epoch 49:loss 6.2969 with data size 564\n","Training complete in 73m 27s\n","Epoch 50/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 50:loss 0.9131 with data size 3924\n","MAE:  1.9966833914848083\n","val epoch 50:loss 6.3085 with data size 564\n","Training complete in 74m 51s\n","Training complete in 74m 51s\n"]}]},{"cell_type":"markdown","source":["100% 일때의 Test Code"],"metadata":{"id":"KZg67ox2NYE-"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KijD9_PUydUH","executionInfo":{"status":"ok","timestamp":1639622223515,"user_tz":-540,"elapsed":55347,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"fb58e510-6a45-45d8-e88a-dbb924ba82bf"},"source":[" #Eval Brixia\n","import torch\n","from torchvision import transforms\n","import cxr_dataset as CXR\n","\n","import eval_model as E\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","PATH_TO_IMAGES = \"/content/gdrive/MyDrive/ColabNotebooks/brixia/DB\"\n","MODEL_PATH32 = '/content/gdrive/MyDrive/ColabNotebooks/brixia/results/regression_checkpoint_best'\n","MODEL_PATH64 = '/content/gdrive/MyDrive/ColabNotebooks/brixia/64results/regression_checkpoint_best'\n","MODEL_PATH8 = '/content/gdrive/MyDrive/ColabNotebooks/brixia/8results/regression_checkpoint_best'\n","MODEL_PATH16 = '/content/gdrive/MyDrive/ColabNotebooks/brixia/16results/regression_checkpoint_best'\n","MODEL_PATH128 = '/content/gdrive/MyDrive/ColabNotebooks/brixia/128results/regression_checkpoint_best'\n","\n","\n","\n","# use imagenet mean,std for normalization\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize(224),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(224),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ]),\n","}\n","\n","# create dataloader\n","dataset = CXR.CXRDataset(\n","    path_to_images=PATH_TO_IMAGES,\n","    fold='test',\n","    transform=data_transforms['val'],\n","    fine_tune=True,\n","    regression=True)\n","dataloader = torch.utils.data.DataLoader(\n","    dataset, 32, shuffle=False, num_workers=8)\n","\n","saved_model = torch.load(MODEL_PATH32)\n","model = saved_model['model']\n","model.to(device)\n","del saved_model\n","\n","saved_model64 = torch.load(MODEL_PATH64)\n","model64 = saved_model64['model']\n","model64.to(device)\n","del saved_model64\n","\n","saved_model16 = torch.load(MODEL_PATH16)\n","model16 = saved_model16['model']\n","model16.to(device)\n","del saved_model16\n","\n","saved_model8 = torch.load(MODEL_PATH8)\n","model8 = saved_model8['model']\n","model8.to(device)\n","del saved_model8\n","\n","saved_model128 = torch.load(MODEL_PATH128)\n","model128 = saved_model128['model']\n","model128.to(device)\n","del saved_model128\n","\n","mae8, ground_truths8, preds8 = E.evaluate_mae(dataloader, model8)\n","print(\"MiniBatch : 8\")\n","print('MAE8 : ', mae8)\n","print('True_df8 : ',ground_truths8)\n","print('Pred_df8 : ', preds8)\n","\n","mae16, ground_truths16, preds16 = E.evaluate_mae(dataloader, model16)\n","print(\"MiniBatch : 16\")\n","print('MAE16 : ', mae16)\n","print('True_df16 : ',ground_truths16)\n","print('Pred_df16 : ', preds16)\n","\n","mae, ground_truths, preds = E.evaluate_mae(dataloader, model)\n","print(\"MiniBatch : 32\")\n","print('MAE : ', mae)\n","print('True_df : ',ground_truths)\n","print('Pred_df : ', preds)\n","\n","mae64, ground_truths64, preds64 = E.evaluate_mae(dataloader, model64)\n","print(\"MiniBatch : 64\")\n","print('MAE64 : ', mae64)\n","print('True_df64 : ',ground_truths64)\n","print('Pred_df64 : ', preds64)\n","\n","mae128, ground_truths128, preds128 = E.evaluate_mae(dataloader, model128)\n","print(\"MiniBatch : 128\")\n","print('MAE128 : ', mae128)\n","print('True_df128 : ',ground_truths128)\n","print('Pred_df128 : ', preds128)\n","\n","import numpy\n","\n","maes = []\n","maes.append(mae8)\n","maes.extend([mae16, mae, mae64, mae128])\n","print(maes)\n","mae_mean = numpy.mean(maes)\n","print(\"성능 : \", mae_mean)\n"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["MiniBatch : 8\n","MAE8 :  1.9615079369305988\n","True_df8 :                    Image Index  true_score\n","0    14564261561865340756.jpg         2.0\n","1    11736208667372564600.jpg         4.0\n","2     9239594870393759636.jpg         9.0\n","3    11144427466904541752.jpg         3.0\n","4    16974554766317036034.jpg         6.0\n","..                        ...         ...\n","202   3928593347333653647.jpg        14.0\n","203  11582034330052535722.jpg        15.0\n","204  14872476706053263416.jpg        15.0\n","205   1158241960099300594.jpg        18.0\n","206  10409101678672828001.jpg         1.0\n","\n","[207 rows x 2 columns]\n","Pred_df8 :                    Image Index   pred_score\n","0    14564261561865340756.jpg  [1.2356411]\n","1    11736208667372564600.jpg  [2.4630685]\n","2     9239594870393759636.jpg   [9.261431]\n","3    11144427466904541752.jpg  [3.5178986]\n","4    16974554766317036034.jpg   [6.280107]\n","..                        ...          ...\n","202   3928593347333653647.jpg  [11.351019]\n","203  11582034330052535722.jpg  [12.253511]\n","204  14872476706053263416.jpg   [17.10063]\n","205   1158241960099300594.jpg  [14.815291]\n","206  10409101678672828001.jpg  [5.5535517]\n","\n","[207 rows x 2 columns]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["MiniBatch : 16\n","MAE16 :  2.048825287156635\n","True_df16 :                    Image Index  true_score\n","0    14564261561865340756.jpg         2.0\n","1    11736208667372564600.jpg         4.0\n","2     9239594870393759636.jpg         9.0\n","3    11144427466904541752.jpg         3.0\n","4    16974554766317036034.jpg         6.0\n","..                        ...         ...\n","202   3928593347333653647.jpg        14.0\n","203  11582034330052535722.jpg        15.0\n","204  14872476706053263416.jpg        15.0\n","205   1158241960099300594.jpg        18.0\n","206  10409101678672828001.jpg         1.0\n","\n","[207 rows x 2 columns]\n","Pred_df16 :                    Image Index   pred_score\n","0    14564261561865340756.jpg        [0.0]\n","1    11736208667372564600.jpg  [1.6048521]\n","2     9239594870393759636.jpg   [9.971664]\n","3    11144427466904541752.jpg  [4.6117563]\n","4    16974554766317036034.jpg  [5.7151084]\n","..                        ...          ...\n","202   3928593347333653647.jpg   [9.872752]\n","203  11582034330052535722.jpg  [11.407163]\n","204  14872476706053263416.jpg  [15.951415]\n","205   1158241960099300594.jpg  [15.599548]\n","206  10409101678672828001.jpg  [3.8639786]\n","\n","[207 rows x 2 columns]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["MiniBatch : 32\n","MAE :  2.0151413777024274\n","True_df :                    Image Index  true_score\n","0    14564261561865340756.jpg         2.0\n","1    11736208667372564600.jpg         4.0\n","2     9239594870393759636.jpg         9.0\n","3    11144427466904541752.jpg         3.0\n","4    16974554766317036034.jpg         6.0\n","..                        ...         ...\n","202   3928593347333653647.jpg        14.0\n","203  11582034330052535722.jpg        15.0\n","204  14872476706053263416.jpg        15.0\n","205   1158241960099300594.jpg        18.0\n","206  10409101678672828001.jpg         1.0\n","\n","[207 rows x 2 columns]\n","Pred_df :                    Image Index    pred_score\n","0    14564261561865340756.jpg         [0.0]\n","1    11736208667372564600.jpg         [0.0]\n","2     9239594870393759636.jpg    [8.863755]\n","3    11144427466904541752.jpg    [4.996565]\n","4    16974554766317036034.jpg   [5.0754685]\n","..                        ...           ...\n","202   3928593347333653647.jpg  [12.4719715]\n","203  11582034330052535722.jpg   [12.949821]\n","204  14872476706053263416.jpg    [18.25111]\n","205   1158241960099300594.jpg   [14.810332]\n","206  10409101678672828001.jpg   [2.0703473]\n","\n","[207 rows x 2 columns]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["MiniBatch : 64\n","MAE64 :  2.141238687263019\n","True_df64 :                    Image Index  true_score\n","0    14564261561865340756.jpg         2.0\n","1    11736208667372564600.jpg         4.0\n","2     9239594870393759636.jpg         9.0\n","3    11144427466904541752.jpg         3.0\n","4    16974554766317036034.jpg         6.0\n","..                        ...         ...\n","202   3928593347333653647.jpg        14.0\n","203  11582034330052535722.jpg        15.0\n","204  14872476706053263416.jpg        15.0\n","205   1158241960099300594.jpg        18.0\n","206  10409101678672828001.jpg         1.0\n","\n","[207 rows x 2 columns]\n","Pred_df64 :                    Image Index    pred_score\n","0    14564261561865340756.jpg         [0.0]\n","1    11736208667372564600.jpg         [0.0]\n","2     9239594870393759636.jpg    [7.716168]\n","3    11144427466904541752.jpg  [0.23071167]\n","4    16974554766317036034.jpg   [4.8717413]\n","..                        ...           ...\n","202   3928593347333653647.jpg   [12.050993]\n","203  11582034330052535722.jpg    [8.412783]\n","204  14872476706053263416.jpg   [15.259328]\n","205   1158241960099300594.jpg  [15.0960245]\n","206  10409101678672828001.jpg    [4.034321]\n","\n","[207 rows x 2 columns]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["MiniBatch : 128\n","MAE128 :  2.543211863812617\n","True_df128 :                    Image Index  true_score\n","0    14564261561865340756.jpg         2.0\n","1    11736208667372564600.jpg         4.0\n","2     9239594870393759636.jpg         9.0\n","3    11144427466904541752.jpg         3.0\n","4    16974554766317036034.jpg         6.0\n","..                        ...         ...\n","202   3928593347333653647.jpg        14.0\n","203  11582034330052535722.jpg        15.0\n","204  14872476706053263416.jpg        15.0\n","205   1158241960099300594.jpg        18.0\n","206  10409101678672828001.jpg         1.0\n","\n","[207 rows x 2 columns]\n","Pred_df128 :                    Image Index   pred_score\n","0    14564261561865340756.jpg        [0.0]\n","1    11736208667372564600.jpg        [0.0]\n","2     9239594870393759636.jpg  [6.9198055]\n","3    11144427466904541752.jpg        [0.0]\n","4    16974554766317036034.jpg        [0.0]\n","..                        ...          ...\n","202   3928593347333653647.jpg  [7.6096992]\n","203  11582034330052535722.jpg  [10.107171]\n","204  14872476706053263416.jpg  [15.608132]\n","205   1158241960099300594.jpg  [13.770421]\n","206  10409101678672828001.jpg        [0.0]\n","\n","[207 rows x 2 columns]\n","[1.9615079369305988, 2.048825287156635, 2.0151413777024274, 2.141238687263019, 2.543211863812617]\n","성능 :  2.1419850305730597\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","## 일부 출력\n","pd.options.display.max_rows = 30\n","pd.options.display.max_columns = 5\n","\n","# 예측값 table(각 방법별 예측값 + var(uncertainty) + mean(예측값 평균))\n","preds_outer = pd.merge(preds8, preds16, how=\"outer\", on=\"Image Index\")\n","preds_outer = pd.merge(preds_outer, preds, how=\"outer\", on=\"Image Index\")\n","preds_outer = pd.merge(preds_outer, preds64, how=\"outer\", on=\"Image Index\")\n","preds_outer = pd.merge(preds_outer, preds128, how=\"outer\", on=\"Image Index\")\n","\n","print(preds_outer) \n","print(type(preds_outer))\n","print(preds_outer.columns)\n","\n","# 각 image 별 variance, mean 추가\n","preds_outer['Var']= preds_outer[['pred_score_x','pred_score_y','pred_score_x','pred_score_y','pred_score']].var(axis=1).values\n","preds_outer['Mean']= preds_outer[['pred_score_x','pred_score_y','pred_score_x','pred_score_y','pred_score']].mean(axis=1).values\n","\n","print(preds_outer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mnMYzgjHacfb","executionInfo":{"status":"ok","timestamp":1639622229178,"user_tz":-540,"elapsed":378,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"df102364-c170-4ab3-c9d3-74431fc62063"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["                  Image Index pred_score_x  ...  pred_score_y   pred_score\n","0    14564261561865340756.jpg  [1.2356411]  ...         [0.0]        [0.0]\n","1    11736208667372564600.jpg  [2.4630685]  ...         [0.0]        [0.0]\n","2     9239594870393759636.jpg   [9.261431]  ...    [7.716168]  [6.9198055]\n","3    11144427466904541752.jpg  [3.5178986]  ...  [0.23071167]        [0.0]\n","4    16974554766317036034.jpg   [6.280107]  ...   [4.8717413]        [0.0]\n","..                        ...          ...  ...           ...          ...\n","202   3928593347333653647.jpg  [11.351019]  ...   [12.050993]  [7.6096992]\n","203  11582034330052535722.jpg  [12.253511]  ...    [8.412783]  [10.107171]\n","204  14872476706053263416.jpg   [17.10063]  ...   [15.259328]  [15.608132]\n","205   1158241960099300594.jpg  [14.815291]  ...  [15.0960245]  [13.770421]\n","206  10409101678672828001.jpg  [5.5535517]  ...    [4.034321]        [0.0]\n","\n","[207 rows x 6 columns]\n","<class 'pandas.core.frame.DataFrame'>\n","Index(['Image Index', 'pred_score_x', 'pred_score_y', 'pred_score_x',\n","       'pred_score_y', 'pred_score'],\n","      dtype='object')\n","                  Image Index pred_score_x  ...       Var       Mean\n","0    14564261561865340756.jpg  [1.2356411]  ...  0.296880   0.274587\n","1    11736208667372564600.jpg  [2.4630685]  ...  1.241232   0.903982\n","2     9239594870393759636.jpg   [9.261431]  ...  1.127066   8.727316\n","3    11144427466904541752.jpg  [3.5178986]  ...  4.754164   2.968207\n","4    16974554766317036034.jpg   [6.280107]  ...  3.650778   4.876094\n","..                        ...          ...  ...       ...        ...\n","202   3928593347333653647.jpg  [11.351019]  ...  2.602917  11.011463\n","203  11582034330052535722.jpg  [12.253511]  ...  3.139298  11.128192\n","204  14872476706053263416.jpg   [17.10063]  ...  1.415513  16.525901\n","205   1158241960099300594.jpg  [14.815291]  ...  0.293887  14.934757\n","206  10409101678672828001.jpg  [5.5535517]  ...  3.198107   3.449377\n","\n","[207 rows x 8 columns]\n"]}]},{"cell_type":"code","source":["var_array = preds_outer['Var']\n","print(var_array)\n","sort_var = var_array.sort_values(ascending=False) # 내림차순\n","print(\"\\n불확실성 높은 순 : \\n\",sort_var)\n","\n","## Data : 4695\n","# test Data : 총 207개\n","# 95% --> 약 10개 / 4685\n","# 90% --> 약 20개 / 4675\n","# 85% --> 약 31개 / 4664\n","drop_table = preds_outer.loc[:,['Image Index','Var']]\n","print(drop_table)\n","sort_drop = drop_table.sort_values('Var',ascending=False) # 내림차순\n","print(\"정렬\\n\")\n","print(sort_drop)\n","\n","# 95%\n","uncertainty_up95 = sort_drop.iloc[0:10]\n","print(uncertainty_up95)\n","print(len(uncertainty_up95))\n","image_95 = uncertainty_up95.iloc[:,0]\n","print(image_95)\n","\n","# 90%\n","uncertainty_up90 = sort_drop.iloc[0:20]\n","print(uncertainty_up90)\n","print(len(uncertainty_up90))\n","drop_90 = uncertainty_up90.iloc[:,0]\n","print(\"90% (drop 20) : \\n\", drop_90)\n","\n","# 85%\n","uncertainty_up85 = sort_drop.iloc[0:31]\n","print(uncertainty_up85)\n","print(len(uncertainty_up85))\n","drop_85 = uncertainty_up85.iloc[:,0]\n","print(\"85% (drop 31) : \\n\", drop_85)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rePRFpBxdKCt","executionInfo":{"status":"ok","timestamp":1639297478267,"user_tz":-540,"elapsed":404,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"ee026085-b5e0-4b92-e989-21fe86d3e9ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0      0.296880\n","1      0.997309\n","2      1.233327\n","3      3.909514\n","4      3.825397\n","5      1.167432\n","6      0.809735\n","7      3.235005\n","8      1.940406\n","9      0.852822\n","10     1.259965\n","11     0.638908\n","12     0.847727\n","13     4.044429\n","14     2.010911\n","15     1.028820\n","16     3.558188\n","17     2.648007\n","18     1.120037\n","19     1.076356\n","20     1.022617\n","21     1.199220\n","22     0.860516\n","23     1.719177\n","24     0.103107\n","25     0.492851\n","26     0.234481\n","27     0.722257\n","28     1.358653\n","29     0.300983\n","30     1.302330\n","31     0.641032\n","32     1.233441\n","33     3.360182\n","34     0.396289\n","35     1.430544\n","36     1.323866\n","37     0.276338\n","38     1.706542\n","39     0.688836\n","40     0.399468\n","41     0.411899\n","42     1.207128\n","43     0.592344\n","44     4.309613\n","45     0.222536\n","46     0.554163\n","47     0.395325\n","48     0.952275\n","49     1.243906\n","50     0.660906\n","51     3.603541\n","52     0.712730\n","53     0.338131\n","54     2.481598\n","55     1.389394\n","56     0.195849\n","57     0.148296\n","58     0.422795\n","59     0.999428\n","60     2.022508\n","61     0.457070\n","62     0.365732\n","63     1.113683\n","64     1.916532\n","65     0.557848\n","66     0.566569\n","67     2.638176\n","68     0.732253\n","69     0.073706\n","70     0.511747\n","71     0.330607\n","72     0.733573\n","73     4.995554\n","74     0.758691\n","75     1.230492\n","76     0.127974\n","77     0.485869\n","78     0.607003\n","79     0.320623\n","80     2.239305\n","81     0.247298\n","82     1.047867\n","83     0.244786\n","84     0.567124\n","85     0.529228\n","86     0.581501\n","87     0.583810\n","88     0.810877\n","89     1.206022\n","90     0.842269\n","91     0.738910\n","92     1.478099\n","93     2.559822\n","94     0.916909\n","95     0.932682\n","96     1.344577\n","97     0.925050\n","98     1.558820\n","99     0.191369\n","100    2.332112\n","101    0.896439\n","102    0.677584\n","103    1.959660\n","104    2.343223\n","105    0.317697\n","106    0.735248\n","107    0.265342\n","108    0.667413\n","109    0.025978\n","110    0.615127\n","111    2.037523\n","112    0.764883\n","113    0.597053\n","114    1.367698\n","115    2.679042\n","116    0.825046\n","117    2.357230\n","118    0.231159\n","119    0.074517\n","120    1.336802\n","121    1.604619\n","122    3.566314\n","123    0.142002\n","124    0.635341\n","125    1.400194\n","126    0.085529\n","127    0.445604\n","128    1.631707\n","129    1.292863\n","130    2.011997\n","131    1.277738\n","132    0.803084\n","133    0.547939\n","134    1.349733\n","135    0.170965\n","136    0.549309\n","137    1.130642\n","138    0.754264\n","139    0.544119\n","140    0.554455\n","141    1.097103\n","142    0.687150\n","143    0.544428\n","144    1.180275\n","145    0.536749\n","146    3.396419\n","147    2.245409\n","148    3.096751\n","149    1.099302\n","150    2.874520\n","151    1.560920\n","152    0.547426\n","153    1.849718\n","154    2.910501\n","155    1.164713\n","156    1.086776\n","157    1.457269\n","158    1.918836\n","159    0.591223\n","160    2.544469\n","161    1.243606\n","162    3.014063\n","163    1.576291\n","164    2.161035\n","165    5.643303\n","166    2.266304\n","167    2.177395\n","168    0.553563\n","169    0.712501\n","170    0.739293\n","171    1.314045\n","172    0.392964\n","173    0.503551\n","174    1.077039\n","175    0.388898\n","176    0.831070\n","177    0.000617\n","178    2.843887\n","179    0.288715\n","180    2.607583\n","181    4.357726\n","182    1.471692\n","183    1.714699\n","184    0.641343\n","185    0.234161\n","186    0.631147\n","187    1.049082\n","188    1.079956\n","189    1.332214\n","190    1.061081\n","191    0.631727\n","192    0.577297\n","193    0.660426\n","194    0.228535\n","195    1.546669\n","196    0.633972\n","197    0.596662\n","198    0.471860\n","199    0.161317\n","200    0.820181\n","201    0.957440\n","202    1.924291\n","203    2.339050\n","204    0.549863\n","205    0.390804\n","206    2.758062\n","Name: Var, dtype: float64\n","\n","불확실성 높은 순 : \n"," 165    5.643303\n","73     4.995554\n","181    4.357726\n","44     4.309613\n","13     4.044429\n","3      3.909514\n","4      3.825397\n","51     3.603541\n","122    3.566314\n","16     3.558188\n","146    3.396419\n","33     3.360182\n","7      3.235005\n","148    3.096751\n","162    3.014063\n","154    2.910501\n","150    2.874520\n","178    2.843887\n","206    2.758062\n","115    2.679042\n","17     2.648007\n","67     2.638176\n","180    2.607583\n","93     2.559822\n","160    2.544469\n","54     2.481598\n","117    2.357230\n","104    2.343223\n","203    2.339050\n","100    2.332112\n","166    2.266304\n","147    2.245409\n","80     2.239305\n","167    2.177395\n","164    2.161035\n","111    2.037523\n","60     2.022508\n","130    2.011997\n","14     2.010911\n","103    1.959660\n","8      1.940406\n","202    1.924291\n","158    1.918836\n","64     1.916532\n","153    1.849718\n","23     1.719177\n","183    1.714699\n","38     1.706542\n","128    1.631707\n","121    1.604619\n","163    1.576291\n","151    1.560920\n","98     1.558820\n","195    1.546669\n","92     1.478099\n","182    1.471692\n","157    1.457269\n","35     1.430544\n","125    1.400194\n","55     1.389394\n","114    1.367698\n","28     1.358653\n","134    1.349733\n","96     1.344577\n","120    1.336802\n","189    1.332214\n","36     1.323866\n","171    1.314045\n","30     1.302330\n","129    1.292863\n","131    1.277738\n","10     1.259965\n","49     1.243906\n","161    1.243606\n","32     1.233441\n","2      1.233327\n","75     1.230492\n","42     1.207128\n","89     1.206022\n","21     1.199220\n","144    1.180275\n","5      1.167432\n","155    1.164713\n","137    1.130642\n","18     1.120037\n","63     1.113683\n","149    1.099302\n","141    1.097103\n","156    1.086776\n","188    1.079956\n","174    1.077039\n","19     1.076356\n","190    1.061081\n","187    1.049082\n","82     1.047867\n","15     1.028820\n","20     1.022617\n","59     0.999428\n","1      0.997309\n","201    0.957440\n","48     0.952275\n","95     0.932682\n","97     0.925050\n","94     0.916909\n","101    0.896439\n","22     0.860516\n","9      0.852822\n","12     0.847727\n","90     0.842269\n","176    0.831070\n","116    0.825046\n","200    0.820181\n","88     0.810877\n","6      0.809735\n","132    0.803084\n","112    0.764883\n","74     0.758691\n","138    0.754264\n","170    0.739293\n","91     0.738910\n","106    0.735248\n","72     0.733573\n","68     0.732253\n","27     0.722257\n","52     0.712730\n","169    0.712501\n","39     0.688836\n","142    0.687150\n","102    0.677584\n","108    0.667413\n","50     0.660906\n","193    0.660426\n","184    0.641343\n","31     0.641032\n","11     0.638908\n","124    0.635341\n","196    0.633972\n","191    0.631727\n","186    0.631147\n","110    0.615127\n","78     0.607003\n","113    0.597053\n","197    0.596662\n","43     0.592344\n","159    0.591223\n","87     0.583810\n","86     0.581501\n","192    0.577297\n","84     0.567124\n","66     0.566569\n","65     0.557848\n","140    0.554455\n","46     0.554163\n","168    0.553563\n","204    0.549863\n","136    0.549309\n","133    0.547939\n","152    0.547426\n","143    0.544428\n","139    0.544119\n","145    0.536749\n","85     0.529228\n","70     0.511747\n","173    0.503551\n","25     0.492851\n","77     0.485869\n","198    0.471860\n","61     0.457070\n","127    0.445604\n","58     0.422795\n","41     0.411899\n","40     0.399468\n","34     0.396289\n","47     0.395325\n","172    0.392964\n","205    0.390804\n","175    0.388898\n","62     0.365732\n","53     0.338131\n","71     0.330607\n","79     0.320623\n","105    0.317697\n","29     0.300983\n","0      0.296880\n","179    0.288715\n","37     0.276338\n","107    0.265342\n","81     0.247298\n","83     0.244786\n","26     0.234481\n","185    0.234161\n","118    0.231159\n","194    0.228535\n","45     0.222536\n","56     0.195849\n","99     0.191369\n","135    0.170965\n","199    0.161317\n","57     0.148296\n","123    0.142002\n","76     0.127974\n","24     0.103107\n","126    0.085529\n","119    0.074517\n","69     0.073706\n","109    0.025978\n","177    0.000617\n","Name: Var, dtype: float64\n","165    5.643303\n","73     4.995554\n","181    4.357726\n","44     4.309613\n","13     4.044429\n","3      3.909514\n","4      3.825397\n","51     3.603541\n","122    3.566314\n","16     3.558188\n","Name: Var, dtype: float64\n","10\n","95% (drop 10) : \n"," [165  73 181  44  13   3   4  51 122  16]\n","165    5.643303\n","73     4.995554\n","181    4.357726\n","44     4.309613\n","13     4.044429\n","3      3.909514\n","4      3.825397\n","51     3.603541\n","122    3.566314\n","16     3.558188\n","146    3.396419\n","33     3.360182\n","7      3.235005\n","148    3.096751\n","162    3.014063\n","154    2.910501\n","150    2.874520\n","178    2.843887\n","206    2.758062\n","115    2.679042\n","Name: Var, dtype: float64\n","20\n","90% (drop 20) : \n"," [165  73 181  44  13   3   4  51 122  16 146  33   7 148 162 154 150 178\n"," 206 115]\n","165    5.643303\n","73     4.995554\n","181    4.357726\n","44     4.309613\n","13     4.044429\n","3      3.909514\n","4      3.825397\n","51     3.603541\n","122    3.566314\n","16     3.558188\n","146    3.396419\n","33     3.360182\n","7      3.235005\n","148    3.096751\n","162    3.014063\n","154    2.910501\n","150    2.874520\n","178    2.843887\n","206    2.758062\n","115    2.679042\n","17     2.648007\n","67     2.638176\n","180    2.607583\n","93     2.559822\n","160    2.544469\n","54     2.481598\n","117    2.357230\n","104    2.343223\n","203    2.339050\n","100    2.332112\n","166    2.266304\n","Name: Var, dtype: float64\n","31\n","85% (drop 31) : \n"," [165  73 181  44  13   3   4  51 122  16 146  33   7 148 162 154 150 178\n"," 206 115  17  67 180  93 160  54 117 104 203 100 166]\n"]}]},{"cell_type":"code","source":["pd.set_option('display.max_rows',None) # 모든 행 출력\n","pd.set_option('display.max_columns',None) # 모든 열 출력\n","\n","## 초기화\n","# pd.options.display.max_rows = 60\n","# pd.options.display.max_columns = 20\n","\n","## 결과 table (image index + pred + uncertainty + ground_truth)\n","results_outer = preds_outer.loc[:,['Image Index','Mean','Var']]\n","results_outer = pd.merge(results_outer, ground_truths, how=\"outer\", on=\"Image Index\")\n","\n","error = abs(results_outer['true_score'].values - results_outer['Mean'].values)\n","results_outer['error'] = error\n","error_mean = error.mean()\n","print(\"error_mean : \", error_mean)\n","\n","print(results_outer)\n","\n","# 오차 순으로 정렬\n","sort_results = results_outer.sort_values('error',ascending=False) # 내림차순\n","print(\"\\n오차 높은 순 : \\n\",sort_results)"],"metadata":{"id":"WgEIG_xzgZpD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639622246831,"user_tz":-540,"elapsed":7,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"309410c2-e447-4a80-9ad3-db488949a5a7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["error_mean :  1.9047028178322143\n","                  Image Index       Mean       Var  true_score     error\n","0    14564261561865340756.jpg   0.274587  0.296880         2.0  1.725413\n","1    11736208667372564600.jpg   0.903982  1.241232         4.0  3.096018\n","2     9239594870393759636.jpg   8.727316  1.127066         9.0  0.272684\n","3    11144427466904541752.jpg   2.968207  4.754164         3.0  0.031793\n","4    16974554766317036034.jpg   4.876094  3.650778         6.0  1.123906\n","5     4142434283091893131.jpg   7.057880  0.873438         7.0  0.057880\n","6    16991136107639463000.jpg  10.890803  1.164713        12.0  1.109197\n","7    12639377867861591927.jpg   2.984726  2.974663         2.0  0.984726\n","8     3454636689776663066.jpg   3.336102  2.109632         0.0  3.336102\n","9     2039478727196349394.jpg   4.020396  0.980665         2.0  2.020396\n","10    2983640989459679908.jpg   2.750362  1.207443         6.0  3.249638\n","11   13803068907556703133.jpg   6.402873  0.584864         5.0  1.402873\n","12     572300713838543755.jpg   3.320623  0.522579         1.0  2.320623\n","13   15762036970094951713.jpg   3.821921  4.012786         2.0  1.821921\n","14   15425337615821266218.jpg   3.883770  2.636331         2.0  1.883770\n","15     294424044587444419.jpg   2.272795  1.053488         2.0  0.272795\n","16   10373203610977600991.jpg   4.871645  3.580422         3.0  1.871645\n","17    5535439021548898675.jpg   3.377072  2.927080         3.0  0.377072\n","18   10205244906786130777.jpg   5.949589  1.310107         3.0  2.949589\n","19     526494555634334802.jpg   7.314265  2.157262         2.0  5.314265\n","20    5016964555276229843.jpg   6.066085  0.995588         3.0  3.066085\n","21    8937742010989582869.jpg   2.864238  1.231459         6.0  3.135762\n","22    5036118436046592072.jpg   8.171839  1.510464         7.0  1.171839\n","23   11285701017062321210.jpg   1.799961  2.205868         2.0  0.200039\n","24     879069476603009827.jpg   5.802473  0.148394         5.0  0.802473\n","25    4511684798608619195.jpg   6.268357  0.636590         5.0  1.268357\n","26    3680690099872516380.jpg   5.195108  0.269248         6.0  0.804892\n","27   16124499320403313494.jpg   7.194760  1.414577         6.0  1.194760\n","28   12855383870084290348.jpg   6.904984  1.467601         6.0  0.904984\n","29   13995121267812712941.jpg   6.881953  0.222599         7.0  0.118047\n","30   17366091821823979653.jpg   6.219835  0.822238         7.0  0.780165\n","31   12404538278390586303.jpg   7.375770  0.895856         1.0  6.375770\n","32   13492091318378263421.jpg   1.013057  1.471829         5.0  3.986943\n","33    1327698747439336495.jpg   4.111759  3.853938         6.0  1.888241\n","34     422370180926276168.jpg   9.771172  0.496113         2.0  7.771172\n","35   14453742178151491113.jpg   4.210753  1.630160         3.0  1.210753\n","36    5668867232073089145.jpg   8.217237  1.261302         8.0  0.217237\n","37    7369641999034162857.jpg   6.168263  0.280488         5.0  1.168263\n","38   10296006099944433448.jpg   4.547107  1.830617         5.0  0.452893\n","39    3525622492256443693.jpg   5.218732  0.622839         6.0  0.781268\n","40   17379184182832557461.jpg   9.670144  0.347894         6.0  3.670144\n","41   14524260746371325603.jpg   9.032310  0.455985         6.0  3.032310\n","42    8547843019812361193.jpg   1.513480  0.885781         6.0  4.486520\n","43   12759961121952434610.jpg  11.867275  0.443828         6.0  5.867275\n","44     701539317072750190.jpg   5.428299  4.304240         7.0  1.571701\n","45    7918905941581655079.jpg   5.682246  0.639442         7.0  1.317754\n","46    2901493158366632509.jpg   5.307454  0.474523         9.0  3.692546\n","47   16968721655500901026.jpg   6.466841  0.386430         7.0  0.533159\n","48   13820293605776796256.jpg  11.422395  0.918409         8.0  3.422395\n","49   11055529703622995004.jpg  10.707703  1.238001         9.0  1.707703\n","50    7206473749624196973.jpg   8.779889  0.637671         9.0  0.220111\n","51    6885968685089799661.jpg   3.638351  3.943203         5.0  1.361649\n","52    3487129271708740243.jpg   7.179449  0.771373         5.0  2.179449\n","53   16327605739648055591.jpg  11.473260  0.404429         8.0  3.473260\n","54    4685635530768467994.jpg   7.933101  2.053404         6.0  1.933101\n","55    3486727694297422605.jpg   7.183604  1.540281         3.0  4.183604\n","56    8512646813904294057.jpg   7.543149  0.300083         5.0  2.543149\n","57    5679274923605146168.jpg   4.988801  0.107539         5.0  0.011199\n","58    3009200414969053735.jpg   5.706876  0.582683         8.0  2.293124\n","59   12930357210006582284.jpg   8.019775  1.231342         7.0  1.019775\n","60   15911053845348818443.jpg   8.210548  1.981179         7.0  1.210548\n","61   13960148861309105875.jpg   8.844958  0.158033         8.0  0.844958\n","62     700501741455133899.jpg   7.835677  0.284288         8.0  0.164323\n","63   15009619040643585516.jpg   5.786263  1.124649         8.0  2.213737\n","64    3058842895220545959.jpg  11.952572  1.102078         9.0  2.952572\n","65    8214560074603274352.jpg  13.065091  0.908305         9.0  4.065091\n","66   15846637757672649797.jpg   9.581193  0.683071         8.0  1.581193\n","67   12402574645219118293.jpg   5.475940  2.854900         6.0  0.524060\n","68   13708500480503630353.jpg   7.501075  0.936219         8.0  0.498925\n","69   10407083070451963222.jpg   7.563910  0.077232         9.0  1.436090\n","70   16101741193325832695.jpg   9.059058  0.355253        10.0  0.940942\n","71   13511283918805529640.jpg   0.445156  0.496357         2.0  1.554844\n","72    4089832714512704162.jpg   5.431002  0.749146         4.0  1.431002\n","73    1885702324025473837.jpg   3.770543  5.724043         4.0  0.229457\n","74    1438457779960319055.jpg  10.362720  1.016891         6.0  4.362720\n","75     551646877960726377.jpg   5.005683  0.915237         7.0  1.994317\n","76   11076610434242984246.jpg   8.403335  0.091732         6.0  2.403335\n","77   14387810907388261211.jpg   9.447529  0.347245         8.0  1.447529\n","78    1224301723771025234.jpg  11.966296  0.261664         8.0  3.966296\n","79   10398834035371500301.jpg   8.890017  0.495313        10.0  1.109983\n","80    9294106011252864899.jpg   7.581612  1.999440        10.0  2.418388\n","81    2456695361432041109.jpg   6.073030  0.279891        10.0  3.926970\n","82    8918563447893123953.jpg  10.523099  1.279238        10.0  0.523099\n","83    2856323440009083731.jpg   9.327632  0.189826         7.0  2.327632\n","84    9169168747345842200.jpg   8.342979  1.102098         7.0  1.342979\n","85   17443536986879183445.jpg   6.880061  0.951424         7.0  0.119939\n","86    7844469693574928360.jpg   9.514283  0.447739         8.0  1.514283\n","87    5336318065350528678.jpg  12.379876  0.527023         9.0  3.379876\n","88    7855502441156508375.jpg   6.528817  0.758345         5.0  1.528817\n","89   15234431379186037959.jpg   7.849267  1.197504         7.0  0.849267\n","90    5044535569812504047.jpg  11.798993  0.914237         8.0  3.798993\n","91   11225080889583929682.jpg   6.125007  0.661786         8.0  1.874993\n","92   10235319580258757961.jpg   6.915118  1.207585         9.0  2.084882\n","93    1404677752780903194.jpg   8.097331  2.401968        10.0  1.902669\n","94   10448608101256035142.jpg  12.652505  1.310761        11.0  1.652505\n","95     127680341107383304.jpg  10.569143  1.632133        11.0  0.430857\n","96   15122067243893514166.jpg   9.455745  2.632139        11.0  1.544255\n","97    9574093710263620216.jpg   7.121769  1.920885        11.0  3.878231\n","98    2210116834028526015.jpg   7.739047  1.112836        11.0  3.260953\n","99    6431759861562598072.jpg  10.254121  0.238299        11.0  0.745879\n","100    789773703383221230.jpg   8.374731  1.976838        12.0  3.625269\n","101    724052992658679116.jpg   7.106773  0.420985        12.0  4.893227\n","102  15530405318545258995.jpg  12.585166  0.633562        13.0  0.414834\n","103  14039895209958512122.jpg   8.115658  1.757117        11.0  2.884342\n","104  11595347288921948716.jpg  10.916750  2.109051        12.0  1.083250\n","105    740674916090105554.jpg  10.316505  0.135440         9.0  1.316505\n","106  17401037664688497891.jpg  11.466783  0.642985        12.0  0.533217\n","107  13571083746881062249.jpg  12.922640  0.828278        12.0  0.922640\n","108  17448594535873890811.jpg  12.873589  1.601806        13.0  0.126411\n","109   1502218282231063179.jpg  11.866269  0.254659        14.0  2.133731\n","110  11396538255811764407.jpg  16.060262  0.987859        14.0  2.060262\n","111   4371317283545880205.jpg  14.487328  2.531243        14.0  0.487328\n","112   6603826747595991577.jpg  13.256309  0.745975        15.0  1.743691\n","113   5205747043902377958.jpg  10.839132  0.535721        14.0  3.160868\n","114   7112367707399906424.jpg   8.965076  1.732242         7.0  1.965076\n","115  17012320413194239489.jpg  10.897670  1.638131        11.0  0.102330\n","116  18263543377997239110.jpg   5.812474  0.849756         4.0  1.812474\n","117   4417384638105538618.jpg   9.909203  1.677648        11.0  1.090797\n","118   1567705254240928761.jpg   9.336268  0.316476        11.0  1.663732\n","119  14913714808821522050.jpg  10.273304  0.075858        11.0  0.726696\n","120   7935788416185633259.jpg  13.413503  1.002276        12.0  1.413503\n","121   3908991455051408917.jpg  11.133611  1.622820        13.0  1.866389\n","122  11233008470219230724.jpg  12.438715  2.782909        14.0  1.561285\n","123  13174995223987338592.jpg  11.079822  0.135267        11.0  0.079822\n","124   7509680904508750014.jpg  12.712936  0.580256        14.0  1.287064\n","125   5721602360069692395.jpg  14.171481  1.496773        15.0  0.828519\n","126   3199381738999034391.jpg  11.761236  0.080393        15.0  3.238764\n","127  11274492157852958103.jpg  11.986325  0.248730        15.0  3.013675\n","128  15317126879413595288.jpg  11.482011  1.703915        12.0  0.517989\n","129  14210947516529320000.jpg  11.443909  0.675117        15.0  3.556091\n","130    724555669548785696.jpg  12.138299  1.927955        16.0  3.861701\n","131   4996010669635053871.jpg   9.498536  1.248390        13.0  3.501464\n","132  10428894458154086946.jpg  14.221216  0.943917        14.0  0.221216\n","133   4456005988868558577.jpg  16.357522  0.822811        15.0  1.357522\n","134  11013666393812119538.jpg  15.372628  0.906100        15.0  0.372628\n","135  12987010928073159787.jpg   9.156210  0.535052        10.0  0.843790\n","136   7939785837873120478.jpg   9.843081  0.659306        10.0  0.156919\n","137   7646167143408301074.jpg  11.892431  1.246160        11.0  0.892431\n","138   9869423143784974903.jpg  13.600413  1.049901        14.0  0.399587\n","139   9802840775783361565.jpg  10.390715  0.477256        14.0  3.609285\n","140    827773125951719560.jpg  13.066413  0.523072        17.0  3.933587\n","141  18410028717932798285.jpg  12.259460  1.312538        15.0  2.740540\n","142  14024777547462937721.jpg  12.707118  0.743049        16.0  3.292882\n","143   9339187043384491302.jpg  12.972907  1.208166        17.0  4.027093\n","144    631300793353391324.jpg  11.706585  0.404638        17.0  5.293415\n","145   1679513937101985928.jpg   1.958780  0.579526         0.0  1.958780\n","146   8263499986124197662.jpg   1.350110  3.396419         2.0  0.649890\n","147   4956613661345261232.jpg   4.865680  1.025418         2.0  2.865680\n","148   8499693214839855078.jpg   3.580817  2.601943         2.0  1.580817\n","149   1946623206506299537.jpg   7.925511  1.089138         4.0  3.925511\n","150    919954549896890902.jpg   4.126206  2.639681         4.0  0.126206\n","151   8052633053063443866.jpg   1.722314  1.732101         4.0  2.277686\n","152   2299335082352189080.jpg   6.211588  1.283647         2.0  4.211588\n","153   5367185614279251937.jpg   3.111600  1.640502         3.0  0.111600\n","154  17412341137186697628.jpg   2.076023  3.630358         2.0  0.076023\n","155  11898288065396820524.jpg   5.611980  1.202215         4.0  1.611980\n","156  12317395772950405702.jpg   3.484791  1.081154         5.0  1.515209\n","157  17778812536699433843.jpg   1.608062  1.704620         2.0  0.391938\n","158  15433824899881041434.jpg   1.528865  1.840171         2.0  0.471135\n","159   1320783291039324280.jpg   9.536018  1.182632         6.0  3.536018\n","160   9664315593676699896.jpg   4.169513  2.942758         5.0  0.830487\n","161  11057106957076205843.jpg   5.178778  1.434667         4.0  1.178778\n","162  11667591376836088663.jpg   1.371715  3.014063         3.0  1.628285\n","163   4381670668809352049.jpg   1.419602  1.585555         4.0  2.580398\n","164  16265679835822322441.jpg   3.917459  2.314210         4.0  0.082541\n","165  13568664663659470751.jpg   2.975263  5.497498         3.0  0.024737\n","166    391309107667684375.jpg   2.719009  1.502734         5.0  2.280991\n","167   7655485135798185122.jpg   7.136621  2.384601         7.0  0.136621\n","168   2664387616865925699.jpg   9.991925  0.759580         7.0  2.991925\n","169   9387547234153642404.jpg   8.912017  0.935245         7.0  1.912017\n","170   3937785176372682134.jpg   8.842835  0.366076         6.0  2.842835\n","171  13179048926650529025.jpg  11.449543  0.960299        10.0  1.449543\n","172   7714170411015443263.jpg   3.096898  0.407054         4.0  0.903102\n","173   6595464291274038880.jpg   5.957536  0.560878         8.0  2.042464\n","174   2221707185060046658.jpg   8.174343  1.227284         9.0  0.825657\n","175   6528469873635988809.jpg   8.941565  0.373783         9.0  0.058435\n","176  14762394268464634507.jpg   8.775340  0.822487         8.0  0.775340\n","177  16353113009188377219.jpg   0.012522  0.000617         1.0  0.987478\n","178  10941207710788663390.jpg   2.434258  3.340519         6.0  3.565742\n","179   6857181049706395877.jpg   6.741313  0.475775         8.0  1.258687\n","180   3227504669736347658.jpg   2.612794  1.831654         6.0  3.387206\n","181   6218186895970335354.jpg   8.123298  3.918607         7.0  1.123298\n","182  15596267132617691306.jpg  10.999973  1.454302         8.0  2.999973\n","183  13687843357647023628.jpg   6.873161  0.927786         8.0  1.126839\n","184   7058990005718775646.jpg   8.528536  0.785340         8.0  0.528536\n","185   2332022953375514885.jpg  12.027948  0.133500        10.0  2.027948\n","186   1555856448268447860.jpg   6.992030  0.522523         8.0  1.007970\n","187   1806007435224850850.jpg  10.213517  1.277701         9.0  1.213517\n","188   8815870952627201549.jpg   6.861323  0.900839         9.0  2.138677\n","189   3448830342143519057.jpg   8.507163  1.320200        10.0  1.492837\n","190      5032497707401895.jpg   6.922451  1.767245        11.0  4.077549\n","191   2626575196097470781.jpg  11.196317  1.343904        11.0  0.196317\n","192  16581884217027400434.jpg  11.009347  0.662212        12.0  0.990653\n","193   1009292177809823491.jpg  11.997317  0.135068        10.0  1.997317\n","194   1900650254908000249.jpg   8.271333  0.179476        13.0  4.728667\n","195   5848391000473468482.jpg   9.248475  1.817159        13.0  3.751525\n","196  13390998274239226475.jpg  11.523837  0.507680        14.0  2.476163\n","197    574758887276955038.jpg  16.000254  1.383461        14.0  2.000254\n","198  17611152957847534092.jpg  13.697689  0.560177        13.0  0.697689\n","199  13396045927013612210.jpg  11.161703  0.861478        15.0  3.838297\n","200  15398562563511216080.jpg  12.463780  0.944357        16.0  3.536220\n","201   8852704667215819661.jpg  10.675629  0.961029         9.0  1.675629\n","202   3928593347333653647.jpg  11.011463  2.602917        14.0  2.988537\n","203  11582034330052535722.jpg  11.128192  3.139298        15.0  3.871808\n","204  14872476706053263416.jpg  16.525901  1.415513        15.0  1.525901\n","205   1158241960099300594.jpg  14.934757  0.293887        18.0  3.065243\n","206  10409101678672828001.jpg   3.449377  3.198107         1.0  2.449377\n","\n","오차 높은 순 : \n","                   Image Index       Mean       Var  true_score     error\n","34     422370180926276168.jpg   9.771172  0.496113         2.0  7.771172\n","31   12404538278390586303.jpg   7.375770  0.895856         1.0  6.375770\n","43   12759961121952434610.jpg  11.867275  0.443828         6.0  5.867275\n","19     526494555634334802.jpg   7.314265  2.157262         2.0  5.314265\n","144    631300793353391324.jpg  11.706585  0.404638        17.0  5.293415\n","101    724052992658679116.jpg   7.106773  0.420985        12.0  4.893227\n","194   1900650254908000249.jpg   8.271333  0.179476        13.0  4.728667\n","42    8547843019812361193.jpg   1.513480  0.885781         6.0  4.486520\n","74    1438457779960319055.jpg  10.362720  1.016891         6.0  4.362720\n","152   2299335082352189080.jpg   6.211588  1.283647         2.0  4.211588\n","55    3486727694297422605.jpg   7.183604  1.540281         3.0  4.183604\n","190      5032497707401895.jpg   6.922451  1.767245        11.0  4.077549\n","65    8214560074603274352.jpg  13.065091  0.908305         9.0  4.065091\n","143   9339187043384491302.jpg  12.972907  1.208166        17.0  4.027093\n","32   13492091318378263421.jpg   1.013057  1.471829         5.0  3.986943\n","78    1224301723771025234.jpg  11.966296  0.261664         8.0  3.966296\n","140    827773125951719560.jpg  13.066413  0.523072        17.0  3.933587\n","81    2456695361432041109.jpg   6.073030  0.279891        10.0  3.926970\n","149   1946623206506299537.jpg   7.925511  1.089138         4.0  3.925511\n","97    9574093710263620216.jpg   7.121769  1.920885        11.0  3.878231\n","203  11582034330052535722.jpg  11.128192  3.139298        15.0  3.871808\n","130    724555669548785696.jpg  12.138299  1.927955        16.0  3.861701\n","199  13396045927013612210.jpg  11.161703  0.861478        15.0  3.838297\n","90    5044535569812504047.jpg  11.798993  0.914237         8.0  3.798993\n","195   5848391000473468482.jpg   9.248475  1.817159        13.0  3.751525\n","46    2901493158366632509.jpg   5.307454  0.474523         9.0  3.692546\n","40   17379184182832557461.jpg   9.670144  0.347894         6.0  3.670144\n","100    789773703383221230.jpg   8.374731  1.976838        12.0  3.625269\n","139   9802840775783361565.jpg  10.390715  0.477256        14.0  3.609285\n","178  10941207710788663390.jpg   2.434258  3.340519         6.0  3.565742\n","129  14210947516529320000.jpg  11.443909  0.675117        15.0  3.556091\n","200  15398562563511216080.jpg  12.463780  0.944357        16.0  3.536220\n","159   1320783291039324280.jpg   9.536018  1.182632         6.0  3.536018\n","131   4996010669635053871.jpg   9.498536  1.248390        13.0  3.501464\n","53   16327605739648055591.jpg  11.473260  0.404429         8.0  3.473260\n","48   13820293605776796256.jpg  11.422395  0.918409         8.0  3.422395\n","180   3227504669736347658.jpg   2.612794  1.831654         6.0  3.387206\n","87    5336318065350528678.jpg  12.379876  0.527023         9.0  3.379876\n","8     3454636689776663066.jpg   3.336102  2.109632         0.0  3.336102\n","142  14024777547462937721.jpg  12.707118  0.743049        16.0  3.292882\n","98    2210116834028526015.jpg   7.739047  1.112836        11.0  3.260953\n","10    2983640989459679908.jpg   2.750362  1.207443         6.0  3.249638\n","126   3199381738999034391.jpg  11.761236  0.080393        15.0  3.238764\n","113   5205747043902377958.jpg  10.839132  0.535721        14.0  3.160868\n","21    8937742010989582869.jpg   2.864238  1.231459         6.0  3.135762\n","1    11736208667372564600.jpg   0.903982  1.241232         4.0  3.096018\n","20    5016964555276229843.jpg   6.066085  0.995588         3.0  3.066085\n","205   1158241960099300594.jpg  14.934757  0.293887        18.0  3.065243\n","41   14524260746371325603.jpg   9.032310  0.455985         6.0  3.032310\n","127  11274492157852958103.jpg  11.986325  0.248730        15.0  3.013675\n","182  15596267132617691306.jpg  10.999973  1.454302         8.0  2.999973\n","168   2664387616865925699.jpg   9.991925  0.759580         7.0  2.991925\n","202   3928593347333653647.jpg  11.011463  2.602917        14.0  2.988537\n","64    3058842895220545959.jpg  11.952572  1.102078         9.0  2.952572\n","18   10205244906786130777.jpg   5.949589  1.310107         3.0  2.949589\n","103  14039895209958512122.jpg   8.115658  1.757117        11.0  2.884342\n","147   4956613661345261232.jpg   4.865680  1.025418         2.0  2.865680\n","170   3937785176372682134.jpg   8.842835  0.366076         6.0  2.842835\n","141  18410028717932798285.jpg  12.259460  1.312538        15.0  2.740540\n","163   4381670668809352049.jpg   1.419602  1.585555         4.0  2.580398\n","56    8512646813904294057.jpg   7.543149  0.300083         5.0  2.543149\n","196  13390998274239226475.jpg  11.523837  0.507680        14.0  2.476163\n","206  10409101678672828001.jpg   3.449377  3.198107         1.0  2.449377\n","80    9294106011252864899.jpg   7.581612  1.999440        10.0  2.418388\n","76   11076610434242984246.jpg   8.403335  0.091732         6.0  2.403335\n","83    2856323440009083731.jpg   9.327632  0.189826         7.0  2.327632\n","12     572300713838543755.jpg   3.320623  0.522579         1.0  2.320623\n","58    3009200414969053735.jpg   5.706876  0.582683         8.0  2.293124\n","166    391309107667684375.jpg   2.719009  1.502734         5.0  2.280991\n","151   8052633053063443866.jpg   1.722314  1.732101         4.0  2.277686\n","63   15009619040643585516.jpg   5.786263  1.124649         8.0  2.213737\n","52    3487129271708740243.jpg   7.179449  0.771373         5.0  2.179449\n","188   8815870952627201549.jpg   6.861323  0.900839         9.0  2.138677\n","109   1502218282231063179.jpg  11.866269  0.254659        14.0  2.133731\n","92   10235319580258757961.jpg   6.915118  1.207585         9.0  2.084882\n","110  11396538255811764407.jpg  16.060262  0.987859        14.0  2.060262\n","173   6595464291274038880.jpg   5.957536  0.560878         8.0  2.042464\n","185   2332022953375514885.jpg  12.027948  0.133500        10.0  2.027948\n","9     2039478727196349394.jpg   4.020396  0.980665         2.0  2.020396\n","197    574758887276955038.jpg  16.000254  1.383461        14.0  2.000254\n","193   1009292177809823491.jpg  11.997317  0.135068        10.0  1.997317\n","75     551646877960726377.jpg   5.005683  0.915237         7.0  1.994317\n","114   7112367707399906424.jpg   8.965076  1.732242         7.0  1.965076\n","145   1679513937101985928.jpg   1.958780  0.579526         0.0  1.958780\n","54    4685635530768467994.jpg   7.933101  2.053404         6.0  1.933101\n","169   9387547234153642404.jpg   8.912017  0.935245         7.0  1.912017\n","93    1404677752780903194.jpg   8.097331  2.401968        10.0  1.902669\n","33    1327698747439336495.jpg   4.111759  3.853938         6.0  1.888241\n","14   15425337615821266218.jpg   3.883770  2.636331         2.0  1.883770\n","91   11225080889583929682.jpg   6.125007  0.661786         8.0  1.874993\n","16   10373203610977600991.jpg   4.871645  3.580422         3.0  1.871645\n","121   3908991455051408917.jpg  11.133611  1.622820        13.0  1.866389\n","13   15762036970094951713.jpg   3.821921  4.012786         2.0  1.821921\n","116  18263543377997239110.jpg   5.812474  0.849756         4.0  1.812474\n","112   6603826747595991577.jpg  13.256309  0.745975        15.0  1.743691\n","0    14564261561865340756.jpg   0.274587  0.296880         2.0  1.725413\n","49   11055529703622995004.jpg  10.707703  1.238001         9.0  1.707703\n","201   8852704667215819661.jpg  10.675629  0.961029         9.0  1.675629\n","118   1567705254240928761.jpg   9.336268  0.316476        11.0  1.663732\n","94   10448608101256035142.jpg  12.652505  1.310761        11.0  1.652505\n","162  11667591376836088663.jpg   1.371715  3.014063         3.0  1.628285\n","155  11898288065396820524.jpg   5.611980  1.202215         4.0  1.611980\n","66   15846637757672649797.jpg   9.581193  0.683071         8.0  1.581193\n","148   8499693214839855078.jpg   3.580817  2.601943         2.0  1.580817\n","44     701539317072750190.jpg   5.428299  4.304240         7.0  1.571701\n","122  11233008470219230724.jpg  12.438715  2.782909        14.0  1.561285\n","71   13511283918805529640.jpg   0.445156  0.496357         2.0  1.554844\n","96   15122067243893514166.jpg   9.455745  2.632139        11.0  1.544255\n","88    7855502441156508375.jpg   6.528817  0.758345         5.0  1.528817\n","204  14872476706053263416.jpg  16.525901  1.415513        15.0  1.525901\n","156  12317395772950405702.jpg   3.484791  1.081154         5.0  1.515209\n","86    7844469693574928360.jpg   9.514283  0.447739         8.0  1.514283\n","189   3448830342143519057.jpg   8.507163  1.320200        10.0  1.492837\n","171  13179048926650529025.jpg  11.449543  0.960299        10.0  1.449543\n","77   14387810907388261211.jpg   9.447529  0.347245         8.0  1.447529\n","69   10407083070451963222.jpg   7.563910  0.077232         9.0  1.436090\n","72    4089832714512704162.jpg   5.431002  0.749146         4.0  1.431002\n","120   7935788416185633259.jpg  13.413503  1.002276        12.0  1.413503\n","11   13803068907556703133.jpg   6.402873  0.584864         5.0  1.402873\n","51    6885968685089799661.jpg   3.638351  3.943203         5.0  1.361649\n","133   4456005988868558577.jpg  16.357522  0.822811        15.0  1.357522\n","84    9169168747345842200.jpg   8.342979  1.102098         7.0  1.342979\n","45    7918905941581655079.jpg   5.682246  0.639442         7.0  1.317754\n","105    740674916090105554.jpg  10.316505  0.135440         9.0  1.316505\n","124   7509680904508750014.jpg  12.712936  0.580256        14.0  1.287064\n","25    4511684798608619195.jpg   6.268357  0.636590         5.0  1.268357\n","179   6857181049706395877.jpg   6.741313  0.475775         8.0  1.258687\n","187   1806007435224850850.jpg  10.213517  1.277701         9.0  1.213517\n","35   14453742178151491113.jpg   4.210753  1.630160         3.0  1.210753\n","60   15911053845348818443.jpg   8.210548  1.981179         7.0  1.210548\n","27   16124499320403313494.jpg   7.194760  1.414577         6.0  1.194760\n","161  11057106957076205843.jpg   5.178778  1.434667         4.0  1.178778\n","22    5036118436046592072.jpg   8.171839  1.510464         7.0  1.171839\n","37    7369641999034162857.jpg   6.168263  0.280488         5.0  1.168263\n","183  13687843357647023628.jpg   6.873161  0.927786         8.0  1.126839\n","4    16974554766317036034.jpg   4.876094  3.650778         6.0  1.123906\n","181   6218186895970335354.jpg   8.123298  3.918607         7.0  1.123298\n","79   10398834035371500301.jpg   8.890017  0.495313        10.0  1.109983\n","6    16991136107639463000.jpg  10.890803  1.164713        12.0  1.109197\n","117   4417384638105538618.jpg   9.909203  1.677648        11.0  1.090797\n","104  11595347288921948716.jpg  10.916750  2.109051        12.0  1.083250\n","59   12930357210006582284.jpg   8.019775  1.231342         7.0  1.019775\n","186   1555856448268447860.jpg   6.992030  0.522523         8.0  1.007970\n","192  16581884217027400434.jpg  11.009347  0.662212        12.0  0.990653\n","177  16353113009188377219.jpg   0.012522  0.000617         1.0  0.987478\n","7    12639377867861591927.jpg   2.984726  2.974663         2.0  0.984726\n","70   16101741193325832695.jpg   9.059058  0.355253        10.0  0.940942\n","107  13571083746881062249.jpg  12.922640  0.828278        12.0  0.922640\n","28   12855383870084290348.jpg   6.904984  1.467601         6.0  0.904984\n","172   7714170411015443263.jpg   3.096898  0.407054         4.0  0.903102\n","137   7646167143408301074.jpg  11.892431  1.246160        11.0  0.892431\n","89   15234431379186037959.jpg   7.849267  1.197504         7.0  0.849267\n","61   13960148861309105875.jpg   8.844958  0.158033         8.0  0.844958\n","135  12987010928073159787.jpg   9.156210  0.535052        10.0  0.843790\n","160   9664315593676699896.jpg   4.169513  2.942758         5.0  0.830487\n","125   5721602360069692395.jpg  14.171481  1.496773        15.0  0.828519\n","174   2221707185060046658.jpg   8.174343  1.227284         9.0  0.825657\n","26    3680690099872516380.jpg   5.195108  0.269248         6.0  0.804892\n","24     879069476603009827.jpg   5.802473  0.148394         5.0  0.802473\n","39    3525622492256443693.jpg   5.218732  0.622839         6.0  0.781268\n","30   17366091821823979653.jpg   6.219835  0.822238         7.0  0.780165\n","176  14762394268464634507.jpg   8.775340  0.822487         8.0  0.775340\n","99    6431759861562598072.jpg  10.254121  0.238299        11.0  0.745879\n","119  14913714808821522050.jpg  10.273304  0.075858        11.0  0.726696\n","198  17611152957847534092.jpg  13.697689  0.560177        13.0  0.697689\n","146   8263499986124197662.jpg   1.350110  3.396419         2.0  0.649890\n","106  17401037664688497891.jpg  11.466783  0.642985        12.0  0.533217\n","47   16968721655500901026.jpg   6.466841  0.386430         7.0  0.533159\n","184   7058990005718775646.jpg   8.528536  0.785340         8.0  0.528536\n","67   12402574645219118293.jpg   5.475940  2.854900         6.0  0.524060\n","82    8918563447893123953.jpg  10.523099  1.279238        10.0  0.523099\n","128  15317126879413595288.jpg  11.482011  1.703915        12.0  0.517989\n","68   13708500480503630353.jpg   7.501075  0.936219         8.0  0.498925\n","111   4371317283545880205.jpg  14.487328  2.531243        14.0  0.487328\n","158  15433824899881041434.jpg   1.528865  1.840171         2.0  0.471135\n","38   10296006099944433448.jpg   4.547107  1.830617         5.0  0.452893\n","95     127680341107383304.jpg  10.569143  1.632133        11.0  0.430857\n","102  15530405318545258995.jpg  12.585166  0.633562        13.0  0.414834\n","138   9869423143784974903.jpg  13.600413  1.049901        14.0  0.399587\n","157  17778812536699433843.jpg   1.608062  1.704620         2.0  0.391938\n","17    5535439021548898675.jpg   3.377072  2.927080         3.0  0.377072\n","134  11013666393812119538.jpg  15.372628  0.906100        15.0  0.372628\n","15     294424044587444419.jpg   2.272795  1.053488         2.0  0.272795\n","2     9239594870393759636.jpg   8.727316  1.127066         9.0  0.272684\n","73    1885702324025473837.jpg   3.770543  5.724043         4.0  0.229457\n","132  10428894458154086946.jpg  14.221216  0.943917        14.0  0.221216\n","50    7206473749624196973.jpg   8.779889  0.637671         9.0  0.220111\n","36    5668867232073089145.jpg   8.217237  1.261302         8.0  0.217237\n","23   11285701017062321210.jpg   1.799961  2.205868         2.0  0.200039\n","191   2626575196097470781.jpg  11.196317  1.343904        11.0  0.196317\n","62     700501741455133899.jpg   7.835677  0.284288         8.0  0.164323\n","136   7939785837873120478.jpg   9.843081  0.659306        10.0  0.156919\n","167   7655485135798185122.jpg   7.136621  2.384601         7.0  0.136621\n","108  17448594535873890811.jpg  12.873589  1.601806        13.0  0.126411\n","150    919954549896890902.jpg   4.126206  2.639681         4.0  0.126206\n","85   17443536986879183445.jpg   6.880061  0.951424         7.0  0.119939\n","29   13995121267812712941.jpg   6.881953  0.222599         7.0  0.118047\n","153   5367185614279251937.jpg   3.111600  1.640502         3.0  0.111600\n","115  17012320413194239489.jpg  10.897670  1.638131        11.0  0.102330\n","164  16265679835822322441.jpg   3.917459  2.314210         4.0  0.082541\n","123  13174995223987338592.jpg  11.079822  0.135267        11.0  0.079822\n","154  17412341137186697628.jpg   2.076023  3.630358         2.0  0.076023\n","175   6528469873635988809.jpg   8.941565  0.373783         9.0  0.058435\n","5     4142434283091893131.jpg   7.057880  0.873438         7.0  0.057880\n","3    11144427466904541752.jpg   2.968207  4.754164         3.0  0.031793\n","165  13568664663659470751.jpg   2.975263  5.497498         3.0  0.024737\n","57    5679274923605146168.jpg   4.988801  0.107539         5.0  0.011199\n"]}]},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/ColabNotebooks/brixia"],"metadata":{"id":"ZbnqJ47MTsgp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip Brixia_jpg.zip -d /content/gdrive/MyDrive/ColabNotebooks/brixia/DB/mini_per85"],"metadata":{"id":"uV9CaLsDTgbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip Brixia_jpg.zip -d /content/gdrive/MyDrive/ColabNotebooks/brixia/DB/mini_per90"],"metadata":{"id":"d2aWTjd2TpRt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip Brixia_jpg.zip -d /content/gdrive/MyDrive/ColabNotebooks/brixia/DB/mini_per95"],"metadata":{"id":"lp1CI5GiTpVD"},"execution_count":null,"outputs":[]}]}