{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MC-Dropout.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO8/JLjt3hR9duMI7X/qGxd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0wHEijayUl-","executionInfo":{"status":"ok","timestamp":1639622487422,"user_tz":-540,"elapsed":21539,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"15924ee9-0553-4170-a5a1-bf92a5190844"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01BYNxqeyYxD","executionInfo":{"status":"ok","timestamp":1639622487423,"user_tz":-540,"elapsed":7,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"7dc8bb5b-4ab3-457c-8ace-468c837a6dfc"},"source":["%cd /content/gdrive/MyDrive/ColabNotebooks/brixia\n","# root = \"/content/gdrive/MyDrive/ColabNotebooks/brixia\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/ColabNotebooks/brixia\n"]}]},{"cell_type":"code","metadata":{"id":"4Kcz3IVmyY3E"},"source":["#!unzip Brixia_jpg.zip -d /content/gdrive/MyDrive/ColabNotebooks/brixia/DB"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBxD7UzByY7N","executionInfo":{"status":"ok","timestamp":1639506876128,"user_tz":-540,"elapsed":3869,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"8fabdf43-7b11-40c9-8f0c-4083eb207fed"},"source":["!pip install tensorboardX"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorboardX\n","  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n","\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 92 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 122 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 12.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.4.1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EnePcR7wydOV","executionInfo":{"status":"ok","timestamp":1636010780989,"user_tz":-540,"elapsed":383,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"804bbe52-6098-4d5b-ebab-59d214d2370a"},"source":["!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/ColabNotebooks/brixia\n"]}]},{"cell_type":"code","metadata":{"id":"C4QYAc0MydQb"},"source":["import jhmodel32 as M\n","\n","# Train from ChestXpert\n","PATH_TO_IMAGES = \"/content/gdrive/MyDrive/ColabNotebooks/brixia/DB\"\n","WEIGHT_DECAY = 0\n","LEARNING_RATE = 0.00001\n","M.train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY, fine_tune=True, regression=True, freeze=True, adam=True, initial_model_path='/content/gdrive/MyDrive/ColabNotebooks/brixia/checkpoints/checkpoint_best')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BnuRzTppydST","executionInfo":{"status":"ok","timestamp":1638439840918,"user_tz":-540,"elapsed":3477098,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"d4df99ae-6a50-4bc1-f2eb-ac00baa87e30"},"source":["import jhmodel32_mc as M10\n","\n","# Train from ChestXpert\n","PATH_TO_IMAGES = \"/content/gdrive/MyDrive/ColabNotebooks/brixia/DB\"\n","WEIGHT_DECAY = 0\n","LEARNING_RATE = 0.00001\n","M10.train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY, fine_tune=True, regression=True, freeze=True, adam=True, initial_model_path='/content/gdrive/MyDrive/ColabNotebooks/brixia/checkpoints/checkpoint_best')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["DataParallel(\n","  (module): DenseNet(\n","    (features): Sequential(\n","      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (denseblock1): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition1): _Transition(\n","        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock2): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition2): _Transition(\n","        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock3): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer17): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer18): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer19): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer20): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer21): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer22): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer23): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer24): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition3): _Transition(\n","        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock4): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (classifier): Sequential(\n","      (0): Linear(in_features=1024, out_features=1, bias=True)\n","      (1): ReLU(inplace=True)\n","      (2): Dropout2d(p=0.3, inplace=False)\n","    )\n","  )\n",")\n","Epoch 1/50\n","----------\n","train epoch 1:loss 67.4530 with data size 3924\n","MAE:  6.333985672312531\n","val epoch 1:loss 50.5787 with data size 564\n","saving\n","Training complete in 2m 37s\n","Epoch 2/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 2:loss 48.2491 with data size 3924\n","MAE:  4.909449607774563\n","val epoch 2:loss 31.5012 with data size 564\n","saving\n","Training complete in 3m 45s\n","Epoch 3/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 3:loss 39.7337 with data size 3924\n","MAE:  4.426383056380647\n","val epoch 3:loss 26.0978 with data size 564\n","saving\n","Training complete in 4m 52s\n","Epoch 4/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 4:loss 36.7614 with data size 3924\n","MAE:  4.1366468975254405\n","val epoch 4:loss 23.1036 with data size 564\n","saving\n","Training complete in 5m 60s\n","Epoch 5/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 5:loss 34.4488 with data size 3924\n","MAE:  3.7627433659971183\n","val epoch 5:loss 19.5611 with data size 564\n","saving\n","Training complete in 7m 8s\n","Epoch 6/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 6:loss 34.2494 with data size 3924\n","MAE:  3.519798114644826\n","val epoch 6:loss 17.3746 with data size 564\n","saving\n","Training complete in 8m 15s\n","Epoch 7/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 7:loss 31.7766 with data size 3924\n","MAE:  3.4999721001648734\n","val epoch 7:loss 17.1261 with data size 564\n","saving\n","Training complete in 9m 23s\n","Epoch 8/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 8:loss 31.6635 with data size 3924\n","MAE:  3.4273680176929378\n","val epoch 8:loss 16.4916 with data size 564\n","saving\n","Training complete in 10m 31s\n","Epoch 9/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 9:loss 30.3544 with data size 3924\n","MAE:  3.369916489968697\n","val epoch 9:loss 15.9293 with data size 564\n","saving\n","Training complete in 11m 38s\n","Epoch 10/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 10:loss 31.1897 with data size 3924\n","MAE:  3.042058580416314\n","val epoch 10:loss 13.3850 with data size 564\n","saving\n","Training complete in 12m 46s\n","Epoch 11/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 11:loss 30.2566 with data size 3924\n","MAE:  2.853022791749091\n","val epoch 11:loss 11.9482 with data size 564\n","saving\n","Training complete in 13m 53s\n","Epoch 12/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 12:loss 30.3195 with data size 3924\n","MAE:  3.057685634662919\n","val epoch 12:loss 13.6314 with data size 564\n","Training complete in 15m 1s\n","Epoch 13/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 13:loss 30.5172 with data size 3924\n","MAE:  3.0644537378300694\n","val epoch 13:loss 13.6783 with data size 564\n","Training complete in 16m 8s\n","Epoch 14/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 14:loss 29.9971 with data size 3924\n","MAE:  2.897304151550675\n","val epoch 14:loss 12.3799 with data size 564\n","Training complete in 17m 15s\n","Epoch 15/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 15:loss 30.3063 with data size 3924\n","MAE:  3.04210740164028\n","val epoch 15:loss 13.5386 with data size 564\n","Training complete in 18m 23s\n","Epoch 16/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 16:loss 29.5109 with data size 3924\n","MAE:  3.1172661385399865\n","val epoch 16:loss 14.1896 with data size 564\n","Training complete in 19m 30s\n","Epoch 17/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 17:loss 29.6908 with data size 3924\n","MAE:  2.9579547349865556\n","val epoch 17:loss 12.9999 with data size 564\n","Training complete in 20m 37s\n","Epoch 18/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 18:loss 29.9629 with data size 3924\n","MAE:  3.0699084463218846\n","val epoch 18:loss 13.9753 with data size 564\n","Training complete in 21m 44s\n","Epoch 19/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 19:loss 30.4214 with data size 3924\n","MAE:  2.866304263525398\n","val epoch 19:loss 12.2381 with data size 564\n","Training complete in 22m 52s\n","Epoch 20/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 20:loss 27.3499 with data size 3924\n","MAE:  2.8542416259540735\n","val epoch 20:loss 12.2445 with data size 564\n","Training complete in 23m 59s\n","Epoch 21/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 21:loss 28.0475 with data size 3924\n","MAE:  2.9167835078175766\n","val epoch 21:loss 12.7285 with data size 564\n","Training complete in 25m 7s\n","Epoch 22/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 22:loss 27.9471 with data size 3924\n","MAE:  2.8033089694824627\n","val epoch 22:loss 11.9185 with data size 564\n","saving\n","Training complete in 26m 15s\n","Epoch 23/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 23:loss 28.5299 with data size 3924\n","MAE:  2.841587196586403\n","val epoch 23:loss 12.2200 with data size 564\n","Training complete in 27m 22s\n","Epoch 24/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 24:loss 26.5118 with data size 3924\n","MAE:  2.7016890385049455\n","val epoch 24:loss 11.1305 with data size 564\n","saving\n","Training complete in 28m 30s\n","Epoch 25/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 25:loss 28.1394 with data size 3924\n","MAE:  2.724805064869265\n","val epoch 25:loss 11.2087 with data size 564\n","Training complete in 29m 37s\n","Epoch 26/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 26:loss 27.6995 with data size 3924\n","MAE:  2.874077800529223\n","val epoch 26:loss 12.5766 with data size 564\n","Training complete in 30m 45s\n","Epoch 27/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 27:loss 28.2320 with data size 3924\n","MAE:  2.7502508757913366\n","val epoch 27:loss 11.5379 with data size 564\n","Training complete in 31m 53s\n","Epoch 28/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 28:loss 26.3504 with data size 3924\n","MAE:  2.8353039582147668\n","val epoch 28:loss 12.1769 with data size 564\n","Training complete in 33m 1s\n","Epoch 29/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 29:loss 28.4647 with data size 3924\n","MAE:  2.853116507754258\n","val epoch 29:loss 12.3360 with data size 564\n","Training complete in 34m 8s\n","Epoch 30/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 30:loss 27.3420 with data size 3924\n","MAE:  2.896815693151232\n","val epoch 30:loss 12.7007 with data size 564\n","Training complete in 35m 15s\n","Epoch 31/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 31:loss 26.8311 with data size 3924\n","MAE:  2.8506049023452382\n","val epoch 31:loss 12.3208 with data size 564\n","Training complete in 36m 23s\n","Epoch 32/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 32:loss 27.3424 with data size 3924\n","MAE:  3.0074624505871577\n","val epoch 32:loss 13.5499 with data size 564\n","Training complete in 37m 31s\n","Epoch 33/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 33:loss 26.7212 with data size 3924\n","MAE:  2.90701850339876\n","val epoch 33:loss 12.7351 with data size 564\n","Training complete in 38m 38s\n","Epoch 34/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 34:loss 28.0409 with data size 3924\n","MAE:  2.8114862476099045\n","val epoch 34:loss 12.0017 with data size 564\n","Training complete in 39m 46s\n","Epoch 35/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 35:loss 27.9185 with data size 3924\n","MAE:  2.8949226090777005\n","val epoch 35:loss 12.6020 with data size 564\n","Training complete in 40m 56s\n","Epoch 36/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 36:loss 28.2504 with data size 3924\n","MAE:  2.746967909183908\n","val epoch 36:loss 11.5541 with data size 564\n","Training complete in 42m 8s\n","Epoch 37/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 37:loss 28.6056 with data size 3924\n","MAE:  2.794195823582774\n","val epoch 37:loss 11.8742 with data size 564\n","Training complete in 43m 16s\n","Epoch 38/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 38:loss 25.7861 with data size 3924\n","MAE:  2.776694302968945\n","val epoch 38:loss 11.7402 with data size 564\n","Training complete in 44m 24s\n","Epoch 39/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 39:loss 28.2385 with data size 3924\n","MAE:  2.7969322819659053\n","val epoch 39:loss 11.8959 with data size 564\n","Training complete in 45m 31s\n","Epoch 40/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 40:loss 27.9578 with data size 3924\n","MAE:  2.9537034280175436\n","val epoch 40:loss 13.1325 with data size 564\n","Training complete in 46m 39s\n","Epoch 41/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 41:loss 27.9696 with data size 3924\n","MAE:  2.8096358654558236\n","val epoch 41:loss 11.9975 with data size 564\n","Training complete in 47m 46s\n","Epoch 42/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 42:loss 28.3732 with data size 3924\n","MAE:  2.858340105926948\n","val epoch 42:loss 12.2418 with data size 564\n","Training complete in 48m 54s\n","Epoch 43/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 43:loss 25.7891 with data size 3924\n","MAE:  2.8004726326317653\n","val epoch 43:loss 11.9685 with data size 564\n","Training complete in 50m 1s\n","Epoch 44/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 44:loss 28.8540 with data size 3924\n","MAE:  2.731978018603004\n","val epoch 44:loss 11.4277 with data size 564\n","Training complete in 51m 9s\n","Epoch 45/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 45:loss 26.0439 with data size 3924\n","MAE:  2.7590157944999687\n","val epoch 45:loss 11.6892 with data size 564\n","Training complete in 52m 16s\n","Epoch 46/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 46:loss 27.0115 with data size 3924\n","MAE:  2.8778304562289665\n","val epoch 46:loss 12.4976 with data size 564\n","Training complete in 53m 24s\n","Epoch 47/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 47:loss 28.4398 with data size 3924\n","MAE:  2.740471249248119\n","val epoch 47:loss 11.5449 with data size 564\n","Training complete in 54m 32s\n","Epoch 48/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 48:loss 27.7935 with data size 3924\n","MAE:  2.6569380525578845\n","val epoch 48:loss 10.8204 with data size 564\n","saving\n","Training complete in 55m 40s\n","Epoch 49/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 49:loss 27.2606 with data size 3924\n","MAE:  2.7992930639508766\n","val epoch 49:loss 11.9456 with data size 564\n","Training complete in 56m 47s\n","Epoch 50/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 50:loss 26.8219 with data size 3924\n","MAE:  2.7512392538025026\n","val epoch 50:loss 11.5465 with data size 564\n","Training complete in 57m 55s\n","Training complete in 57m 55s\n"]}]},{"cell_type":"code","source":["import jhmodel32_mc1 as M11\n","\n","# Train from ChestXpert\n","PATH_TO_IMAGES = \"/content/gdrive/MyDrive/ColabNotebooks/brixia/DB\"\n","WEIGHT_DECAY = 0\n","LEARNING_RATE = 0.00001\n","M11.train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY, fine_tune=True, regression=True, freeze=True, adam=True, initial_model_path='/content/gdrive/MyDrive/ColabNotebooks/brixia/checkpoints/checkpoint_best')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwe_DosCOYdN","executionInfo":{"status":"ok","timestamp":1639510613833,"user_tz":-540,"elapsed":3725489,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"d57e4a6f-da7c-4770-fda6-62552afd0f3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Available GPU count:1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["DataParallel(\n","  (module): DenseNet(\n","    (features): Sequential(\n","      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (denseblock1): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition1): _Transition(\n","        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock2): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition2): _Transition(\n","        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock3): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer17): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer18): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer19): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer20): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer21): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer22): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer23): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer24): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition3): _Transition(\n","        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock4): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (classifier): Sequential(\n","      (0): Linear(in_features=1024, out_features=1, bias=True)\n","      (1): ReLU(inplace=True)\n","      (2): Dropout2d(p=0.3, inplace=False)\n","    )\n","  )\n",")\n","Epoch 1/50\n","----------\n","train epoch 1:loss 76.6277 with data size 3924\n","MAE:  6.382273955614115\n","val epoch 1:loss 54.9330 with data size 564\n","saving\n","Training complete in 5m 15s\n","Epoch 2/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 2:loss 54.1277 with data size 3924\n","MAE:  5.200638465173807\n","val epoch 2:loss 35.6022 with data size 564\n","saving\n","Training complete in 6m 24s\n","Epoch 3/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 3:loss 42.7432 with data size 3924\n","MAE:  4.86957672240992\n","val epoch 3:loss 30.8936 with data size 564\n","saving\n","Training complete in 7m 32s\n","Epoch 4/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 4:loss 39.3412 with data size 3924\n","MAE:  4.189501799180681\n","val epoch 4:loss 23.6659 with data size 564\n","saving\n","Training complete in 8m 42s\n","Epoch 5/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 5:loss 35.4185 with data size 3924\n","MAE:  4.072242530789358\n","val epoch 5:loss 22.2955 with data size 564\n","saving\n","Training complete in 9m 50s\n","Epoch 6/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 6:loss 32.7915 with data size 3924\n","MAE:  3.8061522635902074\n","val epoch 6:loss 19.7687 with data size 564\n","saving\n","Training complete in 10m 59s\n","Epoch 7/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 7:loss 32.4139 with data size 3924\n","MAE:  3.794542643406712\n","val epoch 7:loss 19.7845 with data size 564\n","Training complete in 12m 8s\n","Epoch 8/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 8:loss 32.3620 with data size 3924\n","MAE:  3.5044840477235244\n","val epoch 8:loss 17.2976 with data size 564\n","saving\n","Training complete in 13m 17s\n","Epoch 9/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 9:loss 31.3622 with data size 3924\n","MAE:  3.402131365440416\n","val epoch 9:loss 16.5910 with data size 564\n","saving\n","Training complete in 14m 26s\n","Epoch 10/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 10:loss 30.9014 with data size 3924\n","MAE:  3.274993167463558\n","val epoch 10:loss 15.3971 with data size 564\n","saving\n","Training complete in 15m 35s\n","Epoch 11/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 11:loss 31.5042 with data size 3924\n","MAE:  3.217695767575122\n","val epoch 11:loss 14.7553 with data size 564\n","saving\n","Training complete in 16m 45s\n","Epoch 12/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 12:loss 29.7162 with data size 3924\n","MAE:  3.144548473274707\n","val epoch 12:loss 14.3193 with data size 564\n","saving\n","Training complete in 17m 54s\n","Epoch 13/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 13:loss 30.0807 with data size 3924\n","MAE:  3.093747136275555\n","val epoch 13:loss 14.0421 with data size 564\n","saving\n","Training complete in 19m 3s\n","Epoch 14/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 14:loss 29.2416 with data size 3924\n","MAE:  2.9778694232516254\n","val epoch 14:loss 13.0975 with data size 564\n","saving\n","Training complete in 20m 12s\n","Epoch 15/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 15:loss 29.8221 with data size 3924\n","MAE:  3.0235093886970628\n","val epoch 15:loss 13.5586 with data size 564\n","Training complete in 21m 21s\n","Epoch 16/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 16:loss 28.1404 with data size 3924\n","MAE:  2.9210666752559074\n","val epoch 16:loss 12.8770 with data size 564\n","saving\n","Training complete in 22m 31s\n","Epoch 17/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 17:loss 27.8877 with data size 3924\n","MAE:  3.1288852046395763\n","val epoch 17:loss 14.4800 with data size 564\n","Training complete in 23m 40s\n","Epoch 18/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 18:loss 29.1791 with data size 3924\n","MAE:  2.976148257791626\n","val epoch 18:loss 13.2037 with data size 564\n","Training complete in 24m 48s\n","Epoch 19/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 19:loss 29.2488 with data size 3924\n","MAE:  3.0092864757047053\n","val epoch 19:loss 13.5019 with data size 564\n","Training complete in 25m 57s\n","Epoch 20/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 20:loss 27.2251 with data size 3924\n","MAE:  2.850263703370084\n","val epoch 20:loss 12.2271 with data size 564\n","saving\n","Training complete in 27m 7s\n","Epoch 21/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 21:loss 28.1373 with data size 3924\n","MAE:  2.811101879448967\n","val epoch 21:loss 12.0585 with data size 564\n","saving\n","Training complete in 28m 15s\n","Epoch 22/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 22:loss 27.8766 with data size 3924\n","MAE:  2.7091455960263175\n","val epoch 22:loss 11.2420 with data size 564\n","saving\n","Training complete in 29m 25s\n","Epoch 23/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 23:loss 30.4094 with data size 3924\n","MAE:  2.7370393880187196\n","val epoch 23:loss 11.4824 with data size 564\n","Training complete in 30m 34s\n","Epoch 24/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 24:loss 27.6584 with data size 3924\n","MAE:  2.975299584593439\n","val epoch 24:loss 13.3643 with data size 564\n","Training complete in 31m 42s\n","Epoch 25/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 25:loss 28.5687 with data size 3924\n","MAE:  2.6912607702803104\n","val epoch 25:loss 11.1303 with data size 564\n","saving\n","Training complete in 32m 52s\n","Epoch 26/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 26:loss 28.7147 with data size 3924\n","MAE:  2.853738676360313\n","val epoch 26:loss 12.3941 with data size 564\n","Training complete in 34m 1s\n","Epoch 27/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 27:loss 27.5713 with data size 3924\n","MAE:  3.020672379352205\n","val epoch 27:loss 13.7478 with data size 564\n","Training complete in 35m 10s\n","Epoch 28/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 28:loss 27.1747 with data size 3924\n","MAE:  2.9099782184823186\n","val epoch 28:loss 12.8225 with data size 564\n","Training complete in 36m 19s\n","Epoch 29/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 29:loss 27.6887 with data size 3924\n","MAE:  2.87742060034516\n","val epoch 29:loss 12.5430 with data size 564\n","Training complete in 37m 28s\n","Epoch 30/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 30:loss 26.4526 with data size 3924\n","MAE:  2.79868808016181\n","val epoch 30:loss 11.9558 with data size 564\n","Training complete in 38m 37s\n","Epoch 31/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 31:loss 28.8728 with data size 3924\n","MAE:  2.8032761165172073\n","val epoch 31:loss 11.9460 with data size 564\n","Training complete in 39m 46s\n","Epoch 32/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 32:loss 26.7009 with data size 3924\n","MAE:  2.858197215231175\n","val epoch 32:loss 12.4572 with data size 564\n","Training complete in 40m 54s\n","Epoch 33/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 33:loss 28.4236 with data size 3924\n","MAE:  2.984598245048988\n","val epoch 33:loss 13.4348 with data size 564\n","Training complete in 42m 4s\n","Epoch 34/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 34:loss 29.0120 with data size 3924\n","MAE:  2.848919694622358\n","val epoch 34:loss 12.3649 with data size 564\n","Training complete in 43m 13s\n","Epoch 35/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 35:loss 26.6434 with data size 3924\n","MAE:  2.8192901091108507\n","val epoch 35:loss 12.1107 with data size 564\n","Training complete in 44m 22s\n","Epoch 36/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 36:loss 27.9253 with data size 3924\n","MAE:  2.814512968089775\n","val epoch 36:loss 12.0190 with data size 564\n","Training complete in 45m 31s\n","Epoch 37/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 37:loss 26.0730 with data size 3924\n","MAE:  2.803280179527529\n","val epoch 37:loss 11.9154 with data size 564\n","Training complete in 46m 40s\n","Epoch 38/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 38:loss 28.0902 with data size 3924\n","MAE:  2.7704892714395593\n","val epoch 38:loss 11.6537 with data size 564\n","Training complete in 47m 48s\n","Epoch 39/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 39:loss 27.0626 with data size 3924\n","MAE:  2.8139536140820156\n","val epoch 39:loss 11.9870 with data size 564\n","Training complete in 48m 57s\n","Epoch 40/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 40:loss 27.7487 with data size 3924\n","MAE:  2.903346074996054\n","val epoch 40:loss 12.7139 with data size 564\n","Training complete in 50m 6s\n","Epoch 41/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 41:loss 26.6482 with data size 3924\n","MAE:  2.912579654590458\n","val epoch 41:loss 12.7876 with data size 564\n","Training complete in 51m 15s\n","Epoch 42/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 42:loss 27.6884 with data size 3924\n","MAE:  2.831618779411553\n","val epoch 42:loss 12.1591 with data size 564\n","Training complete in 52m 24s\n","Epoch 43/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 43:loss 27.3227 with data size 3924\n","MAE:  2.8873001661495112\n","val epoch 43:loss 12.5938 with data size 564\n","Training complete in 53m 33s\n","Epoch 44/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 44:loss 28.1965 with data size 3924\n","MAE:  2.8026469477736358\n","val epoch 44:loss 11.8691 with data size 564\n","Training complete in 54m 42s\n","Epoch 45/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 45:loss 27.4818 with data size 3924\n","MAE:  2.8914405565101204\n","val epoch 45:loss 12.6136 with data size 564\n","Training complete in 55m 51s\n","Epoch 46/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 46:loss 27.5345 with data size 3924\n","MAE:  2.862770144652628\n","val epoch 46:loss 12.3005 with data size 564\n","Training complete in 57m 0s\n","Epoch 47/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 47:loss 27.8022 with data size 3924\n","MAE:  2.7472035145083216\n","val epoch 47:loss 11.4632 with data size 564\n","Training complete in 58m 9s\n","Epoch 48/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 48:loss 26.4018 with data size 3924\n","MAE:  2.776785768674198\n","val epoch 48:loss 11.6136 with data size 564\n","Training complete in 59m 18s\n","Epoch 49/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 49:loss 27.3055 with data size 3924\n","MAE:  2.939604414785479\n","val epoch 49:loss 12.9401 with data size 564\n","Training complete in 60m 27s\n","Epoch 50/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 50:loss 27.3064 with data size 3924\n","MAE:  2.694892127238584\n","val epoch 50:loss 11.0686 with data size 564\n","saving\n","Training complete in 61m 36s\n","Training complete in 61m 36s\n"]}]},{"cell_type":"code","source":["import jhmodel32_mc2 as M12\n","\n","# Train from ChestXpert\n","PATH_TO_IMAGES = \"/content/gdrive/MyDrive/ColabNotebooks/brixia/DB\"\n","WEIGHT_DECAY = 0\n","LEARNING_RATE = 0.00001\n","M12.train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY, fine_tune=True, regression=True, freeze=True, adam=True, initial_model_path='/content/gdrive/MyDrive/ColabNotebooks/brixia/checkpoints/checkpoint_best')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mnvqmSAgOcys","executionInfo":{"status":"ok","timestamp":1639514150933,"user_tz":-540,"elapsed":3473738,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"344071d5-bc44-42c7-cf4e-e0067810a6e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Available GPU count:1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["DataParallel(\n","  (module): DenseNet(\n","    (features): Sequential(\n","      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (denseblock1): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition1): _Transition(\n","        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock2): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition2): _Transition(\n","        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock3): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer17): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer18): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer19): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer20): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer21): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer22): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer23): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer24): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition3): _Transition(\n","        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock4): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (classifier): Sequential(\n","      (0): Linear(in_features=1024, out_features=1, bias=True)\n","      (1): ReLU(inplace=True)\n","      (2): Dropout2d(p=0.3, inplace=False)\n","    )\n","  )\n",")\n","Epoch 1/50\n","----------\n","train epoch 1:loss 71.1592 with data size 3924\n","MAE:  6.489338520717814\n","val epoch 1:loss 52.5663 with data size 564\n","saving\n","Training complete in 1m 9s\n","Epoch 2/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 2:loss 49.8358 with data size 3924\n","MAE:  5.413365957637628\n","val epoch 2:loss 37.1154 with data size 564\n","saving\n","Training complete in 2m 19s\n","Epoch 3/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 3:loss 40.9506 with data size 3924\n","MAE:  4.645745120566101\n","val epoch 3:loss 28.3114 with data size 564\n","saving\n","Training complete in 3m 29s\n","Epoch 4/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 4:loss 36.4917 with data size 3924\n","MAE:  4.279946688305031\n","val epoch 4:loss 24.4007 with data size 564\n","saving\n","Training complete in 4m 38s\n","Epoch 5/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 5:loss 33.7749 with data size 3924\n","MAE:  3.903107396842159\n","val epoch 5:loss 20.6502 with data size 564\n","saving\n","Training complete in 5m 47s\n","Epoch 6/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 6:loss 35.0563 with data size 3924\n","MAE:  3.9927125614968704\n","val epoch 6:loss 21.4512 with data size 564\n","Training complete in 6m 57s\n","Epoch 7/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 7:loss 32.8622 with data size 3924\n","MAE:  3.525785457501386\n","val epoch 7:loss 17.3084 with data size 564\n","saving\n","Training complete in 8m 6s\n","Epoch 8/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 8:loss 33.6626 with data size 3924\n","MAE:  3.4919763181279313\n","val epoch 8:loss 17.0569 with data size 564\n","saving\n","Training complete in 9m 15s\n","Epoch 9/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 9:loss 31.9228 with data size 3924\n","MAE:  3.2698499732221484\n","val epoch 9:loss 15.1421 with data size 564\n","saving\n","Training complete in 10m 25s\n","Epoch 10/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 10:loss 31.2772 with data size 3924\n","MAE:  3.1432333208475236\n","val epoch 10:loss 14.2316 with data size 564\n","saving\n","Training complete in 11m 34s\n","Epoch 11/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 11:loss 31.7457 with data size 3924\n","MAE:  3.235534328604051\n","val epoch 11:loss 15.0499 with data size 564\n","Training complete in 12m 44s\n","Epoch 12/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 12:loss 28.7164 with data size 3924\n","MAE:  3.2296659132954497\n","val epoch 12:loss 14.8239 with data size 564\n","Training complete in 13m 53s\n","Epoch 13/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 13:loss 30.2040 with data size 3924\n","MAE:  3.051569586632945\n","val epoch 13:loss 13.6664 with data size 564\n","saving\n","Training complete in 15m 3s\n","Epoch 14/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 14:loss 30.4711 with data size 3924\n","MAE:  3.046245887030101\n","val epoch 14:loss 13.6374 with data size 564\n","saving\n","Training complete in 16m 12s\n","Epoch 15/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 15:loss 28.7342 with data size 3924\n","MAE:  2.886455409560709\n","val epoch 15:loss 12.3709 with data size 564\n","saving\n","Training complete in 17m 21s\n","Epoch 16/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 16:loss 29.1054 with data size 3924\n","MAE:  2.929357004186786\n","val epoch 16:loss 12.7942 with data size 564\n","Training complete in 18m 30s\n","Epoch 17/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 17:loss 30.0849 with data size 3924\n","MAE:  2.9957157818613744\n","val epoch 17:loss 13.3445 with data size 564\n","Training complete in 19m 40s\n","Epoch 18/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 18:loss 28.9935 with data size 3924\n","MAE:  2.8443794648116785\n","val epoch 18:loss 12.2032 with data size 564\n","saving\n","Training complete in 20m 50s\n","Epoch 19/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 19:loss 27.3971 with data size 3924\n","MAE:  2.828091323322861\n","val epoch 19:loss 12.2033 with data size 564\n","Training complete in 21m 59s\n","Epoch 20/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 20:loss 28.8672 with data size 3924\n","MAE:  2.877037379283009\n","val epoch 20:loss 12.5592 with data size 564\n","Training complete in 23m 8s\n","Epoch 21/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 21:loss 28.7386 with data size 3924\n","MAE:  2.862043080758964\n","val epoch 21:loss 12.3794 with data size 564\n","Training complete in 24m 17s\n","Epoch 22/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 22:loss 27.8177 with data size 3924\n","MAE:  2.8813929026351963\n","val epoch 22:loss 12.5919 with data size 564\n","Training complete in 25m 26s\n","Epoch 23/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 23:loss 27.7251 with data size 3924\n","MAE:  2.8820149781089937\n","val epoch 23:loss 12.5619 with data size 564\n","Training complete in 26m 35s\n","Epoch 24/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 24:loss 27.9307 with data size 3924\n","MAE:  3.029214799456985\n","val epoch 24:loss 13.8539 with data size 564\n","Training complete in 27m 45s\n","Epoch 25/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 25:loss 28.7642 with data size 3924\n","MAE:  2.7217544099012163\n","val epoch 25:loss 11.3064 with data size 564\n","saving\n","Training complete in 28m 54s\n","Epoch 26/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 26:loss 26.5119 with data size 3924\n","MAE:  3.2218672633963696\n","val epoch 26:loss 15.3542 with data size 564\n","Training complete in 30m 4s\n","Epoch 27/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 27:loss 26.6278 with data size 3924\n","MAE:  2.739349102059789\n","val epoch 27:loss 11.4658 with data size 564\n","Training complete in 31m 13s\n","Epoch 28/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 28:loss 26.2280 with data size 3924\n","MAE:  2.8190410342199583\n","val epoch 28:loss 12.0832 with data size 564\n","Training complete in 32m 22s\n","Epoch 29/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 29:loss 29.2959 with data size 3924\n","MAE:  2.8783734651110695\n","val epoch 29:loss 12.6751 with data size 564\n","Training complete in 33m 31s\n","Epoch 30/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 30:loss 27.8916 with data size 3924\n","MAE:  2.9271794263114956\n","val epoch 30:loss 12.9993 with data size 564\n","Training complete in 34m 40s\n","Epoch 31/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 31:loss 28.2387 with data size 3924\n","MAE:  2.6112025314611746\n","val epoch 31:loss 10.5576 with data size 564\n","saving\n","Training complete in 35m 49s\n","Epoch 32/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 32:loss 27.7806 with data size 3924\n","MAE:  2.6856211642767733\n","val epoch 32:loss 11.2007 with data size 564\n","Training complete in 36m 59s\n","Epoch 33/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 33:loss 27.5215 with data size 3924\n","MAE:  2.854341267112722\n","val epoch 33:loss 12.4140 with data size 564\n","Training complete in 38m 9s\n","Epoch 34/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 34:loss 27.5474 with data size 3924\n","MAE:  2.640369621680138\n","val epoch 34:loss 10.7980 with data size 564\n","Training complete in 39m 18s\n","Epoch 35/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 35:loss 27.4459 with data size 3924\n","MAE:  2.8602336230060312\n","val epoch 35:loss 12.4002 with data size 564\n","Training complete in 40m 28s\n","Epoch 36/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 36:loss 26.9512 with data size 3924\n","MAE:  2.793805632291111\n","val epoch 36:loss 11.9632 with data size 564\n","Training complete in 41m 38s\n","Epoch 37/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 37:loss 26.5860 with data size 3924\n","MAE:  2.8025141278057233\n","val epoch 37:loss 12.0814 with data size 564\n","Training complete in 42m 47s\n","Epoch 38/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 38:loss 28.7831 with data size 3924\n","MAE:  2.8573235735191522\n","val epoch 38:loss 12.4546 with data size 564\n","Training complete in 43m 57s\n","Epoch 39/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 39:loss 27.1622 with data size 3924\n","MAE:  2.8371607695197594\n","val epoch 39:loss 12.2219 with data size 564\n","Training complete in 45m 6s\n","Epoch 40/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 40:loss 26.7763 with data size 3924\n","MAE:  2.835483673616504\n","val epoch 40:loss 12.1503 with data size 564\n","Training complete in 46m 16s\n","Epoch 41/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 41:loss 26.4586 with data size 3924\n","MAE:  2.7078387894740343\n","val epoch 41:loss 11.2602 with data size 564\n","Training complete in 47m 26s\n","Epoch 42/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 42:loss 27.9078 with data size 3924\n","MAE:  2.9251399665331164\n","val epoch 42:loss 13.0176 with data size 564\n","Training complete in 48m 35s\n","Epoch 43/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 43:loss 28.3254 with data size 3924\n","MAE:  2.723975901643858\n","val epoch 43:loss 11.4295 with data size 564\n","Training complete in 49m 45s\n","Epoch 44/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 44:loss 26.4399 with data size 3924\n","MAE:  2.9453706049115946\n","val epoch 44:loss 13.0983 with data size 564\n","Training complete in 50m 55s\n","Epoch 45/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 45:loss 27.5181 with data size 3924\n","MAE:  2.7860921965845935\n","val epoch 45:loss 11.8571 with data size 564\n","Training complete in 52m 4s\n","Epoch 46/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 46:loss 27.0704 with data size 3924\n","MAE:  2.754507369943069\n","val epoch 46:loss 11.5604 with data size 564\n","Training complete in 53m 14s\n","Epoch 47/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 47:loss 27.4172 with data size 3924\n","MAE:  2.756161284943422\n","val epoch 47:loss 11.4835 with data size 564\n","Training complete in 54m 23s\n","Epoch 48/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 48:loss 27.2403 with data size 3924\n","MAE:  2.81749083767546\n","val epoch 48:loss 12.0112 with data size 564\n","Training complete in 55m 33s\n","Epoch 49/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 49:loss 25.9828 with data size 3924\n","MAE:  2.960041503293162\n","val epoch 49:loss 13.2203 with data size 564\n","Training complete in 56m 43s\n","Epoch 50/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 50:loss 26.0128 with data size 3924\n","MAE:  2.857358276315615\n","val epoch 50:loss 12.4593 with data size 564\n","Training complete in 57m 52s\n","Training complete in 57m 52s\n"]}]},{"cell_type":"code","source":["import jhmodel32_mc3 as M13\n","\n","# Train from ChestXpert\n","PATH_TO_IMAGES = \"/content/gdrive/MyDrive/ColabNotebooks/brixia/DB\"\n","WEIGHT_DECAY = 0\n","LEARNING_RATE = 0.00001\n","M13.train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY, fine_tune=True, regression=True, freeze=True, adam=True, initial_model_path='/content/gdrive/MyDrive/ColabNotebooks/brixia/checkpoints/checkpoint_best')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N3VNAOJrOdG8","executionInfo":{"status":"ok","timestamp":1639517651739,"user_tz":-540,"elapsed":3495489,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"869cdef8-5dc1-4a47-d742-6d689c0d0824"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Available GPU count:1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["DataParallel(\n","  (module): DenseNet(\n","    (features): Sequential(\n","      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (denseblock1): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition1): _Transition(\n","        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock2): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition2): _Transition(\n","        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock3): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer17): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer18): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer19): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer20): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer21): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer22): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer23): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer24): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition3): _Transition(\n","        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock4): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (classifier): Sequential(\n","      (0): Linear(in_features=1024, out_features=1, bias=True)\n","      (1): ReLU(inplace=True)\n","      (2): Dropout2d(p=0.3, inplace=False)\n","    )\n","  )\n",")\n","Epoch 1/50\n","----------\n","train epoch 1:loss 68.1144 with data size 3924\n","MAE:  6.27126403200995\n","val epoch 1:loss 49.6139 with data size 564\n","saving\n","Training complete in 1m 10s\n","Epoch 2/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 2:loss 48.7074 with data size 3924\n","MAE:  5.076027404523541\n","val epoch 2:loss 33.4609 with data size 564\n","saving\n","Training complete in 2m 20s\n","Epoch 3/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 3:loss 42.8293 with data size 3924\n","MAE:  4.60207390403276\n","val epoch 3:loss 28.0276 with data size 564\n","saving\n","Training complete in 3m 30s\n","Epoch 4/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 4:loss 36.0283 with data size 3924\n","MAE:  4.186302413371332\n","val epoch 4:loss 23.4647 with data size 564\n","saving\n","Training complete in 4m 40s\n","Epoch 5/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 5:loss 34.0311 with data size 3924\n","MAE:  4.035415716125821\n","val epoch 5:loss 21.8924 with data size 564\n","saving\n","Training complete in 5m 49s\n","Epoch 6/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 6:loss 33.7903 with data size 3924\n","MAE:  3.825746188111973\n","val epoch 6:loss 19.8661 with data size 564\n","saving\n","Training complete in 6m 60s\n","Epoch 7/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 7:loss 32.4233 with data size 3924\n","MAE:  3.5658418427333762\n","val epoch 7:loss 17.6582 with data size 564\n","saving\n","Training complete in 8m 10s\n","Epoch 8/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 8:loss 32.8450 with data size 3924\n","MAE:  3.4606927698015744\n","val epoch 8:loss 16.6613 with data size 564\n","saving\n","Training complete in 9m 20s\n","Epoch 9/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 9:loss 30.7763 with data size 3924\n","MAE:  3.257493554488987\n","val epoch 9:loss 15.0520 with data size 564\n","saving\n","Training complete in 10m 30s\n","Epoch 10/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 10:loss 31.3295 with data size 3924\n","MAE:  3.358957846906591\n","val epoch 10:loss 15.8888 with data size 564\n","Training complete in 11m 40s\n","Epoch 11/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 11:loss 30.2341 with data size 3924\n","MAE:  3.240686594805819\n","val epoch 11:loss 14.8950 with data size 564\n","saving\n","Training complete in 12m 50s\n","Epoch 12/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 12:loss 30.2475 with data size 3924\n","MAE:  2.996075549671519\n","val epoch 12:loss 13.1689 with data size 564\n","saving\n","Training complete in 14m 1s\n","Epoch 13/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 13:loss 30.8234 with data size 3924\n","MAE:  3.108830803569327\n","val epoch 13:loss 14.0253 with data size 564\n","Training complete in 15m 10s\n","Epoch 14/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 14:loss 30.1814 with data size 3924\n","MAE:  2.800106665027057\n","val epoch 14:loss 11.5766 with data size 564\n","saving\n","Training complete in 16m 20s\n","Epoch 15/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 15:loss 28.3802 with data size 3924\n","MAE:  2.8352707794847642\n","val epoch 15:loss 11.8948 with data size 564\n","Training complete in 17m 30s\n","Epoch 16/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 16:loss 27.9705 with data size 3924\n","MAE:  2.8402151380870357\n","val epoch 16:loss 11.9241 with data size 564\n","Training complete in 18m 40s\n","Epoch 17/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 17:loss 28.6820 with data size 3924\n","MAE:  2.8637330102476666\n","val epoch 17:loss 12.1694 with data size 564\n","Training complete in 19m 50s\n","Epoch 18/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 18:loss 28.3331 with data size 3924\n","MAE:  3.0164183795399278\n","val epoch 18:loss 13.4486 with data size 564\n","Training complete in 21m 0s\n","Epoch 19/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 19:loss 27.4935 with data size 3924\n","MAE:  2.9393353422060082\n","val epoch 19:loss 12.8410 with data size 564\n","Training complete in 22m 10s\n","Epoch 20/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 20:loss 25.9697 with data size 3924\n","MAE:  2.9789978350836335\n","val epoch 20:loss 13.1144 with data size 564\n","Training complete in 23m 19s\n","Epoch 21/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 21:loss 26.7749 with data size 3924\n","MAE:  2.8870968388538834\n","val epoch 21:loss 12.2883 with data size 564\n","Training complete in 24m 29s\n","Epoch 22/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 22:loss 28.1425 with data size 3924\n","MAE:  2.9749103695788284\n","val epoch 22:loss 12.9952 with data size 564\n","Training complete in 25m 39s\n","Epoch 23/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 23:loss 27.9249 with data size 3924\n","MAE:  2.9427851410516612\n","val epoch 23:loss 12.8872 with data size 564\n","Training complete in 26m 48s\n","Epoch 24/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 24:loss 25.8250 with data size 3924\n","MAE:  2.8203483031573873\n","val epoch 24:loss 11.9177 with data size 564\n","Training complete in 27m 58s\n","Epoch 25/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 25:loss 28.0342 with data size 3924\n","MAE:  2.828533371395253\n","val epoch 25:loss 11.9346 with data size 564\n","Training complete in 29m 8s\n","Epoch 26/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 26:loss 27.8961 with data size 3924\n","MAE:  2.758075365994839\n","val epoch 26:loss 11.5261 with data size 564\n","saving\n","Training complete in 30m 18s\n","Epoch 27/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 27:loss 27.1666 with data size 3924\n","MAE:  2.973719467455191\n","val epoch 27:loss 13.1610 with data size 564\n","Training complete in 31m 28s\n","Epoch 28/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 28:loss 27.1065 with data size 3924\n","MAE:  2.9217024602471513\n","val epoch 28:loss 12.7055 with data size 564\n","Training complete in 32m 38s\n","Epoch 29/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 29:loss 27.1242 with data size 3924\n","MAE:  2.8176040124174553\n","val epoch 29:loss 11.9847 with data size 564\n","Training complete in 33m 47s\n","Epoch 30/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 30:loss 27.5588 with data size 3924\n","MAE:  2.772323709536106\n","val epoch 30:loss 11.5904 with data size 564\n","Training complete in 34m 56s\n","Epoch 31/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 31:loss 26.9518 with data size 3924\n","MAE:  2.8058848540093884\n","val epoch 31:loss 11.7721 with data size 564\n","Training complete in 36m 6s\n","Epoch 32/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 32:loss 26.9746 with data size 3924\n","MAE:  2.827180661208241\n","val epoch 32:loss 12.0726 with data size 564\n","Training complete in 37m 16s\n","Epoch 33/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 33:loss 27.2492 with data size 3924\n","MAE:  2.9184322912668716\n","val epoch 33:loss 12.6927 with data size 564\n","Training complete in 38m 26s\n","Epoch 34/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 34:loss 28.4503 with data size 3924\n","MAE:  2.737227986124179\n","val epoch 34:loss 11.3147 with data size 564\n","saving\n","Training complete in 39m 36s\n","Epoch 35/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 35:loss 27.3354 with data size 3924\n","MAE:  2.907597720966483\n","val epoch 35:loss 12.6127 with data size 564\n","Training complete in 40m 46s\n","Epoch 36/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 36:loss 27.7059 with data size 3924\n","MAE:  2.8356602841340903\n","val epoch 36:loss 12.1013 with data size 564\n","Training complete in 41m 56s\n","Epoch 37/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 37:loss 28.3922 with data size 3924\n","MAE:  2.980369441766054\n","val epoch 37:loss 13.2139 with data size 564\n","Training complete in 43m 5s\n","Epoch 38/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 38:loss 27.0680 with data size 3924\n","MAE:  2.8359441185726766\n","val epoch 38:loss 12.0336 with data size 564\n","Training complete in 44m 15s\n","Epoch 39/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 39:loss 27.8021 with data size 3924\n","MAE:  2.7890923391525626\n","val epoch 39:loss 11.7192 with data size 564\n","Training complete in 45m 25s\n","Epoch 40/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 40:loss 26.9722 with data size 3924\n","MAE:  2.8606879362400544\n","val epoch 40:loss 12.2822 with data size 564\n","Training complete in 46m 35s\n","Epoch 41/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 41:loss 26.4740 with data size 3924\n","MAE:  2.8645400560282646\n","val epoch 41:loss 12.2665 with data size 564\n","Training complete in 47m 45s\n","Epoch 42/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 42:loss 26.7645 with data size 3924\n","MAE:  2.819908276438079\n","val epoch 42:loss 11.9274 with data size 564\n","Training complete in 48m 55s\n","Epoch 43/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 43:loss 27.3603 with data size 3924\n","MAE:  2.842485558849277\n","val epoch 43:loss 12.1193 with data size 564\n","Training complete in 50m 5s\n","Epoch 44/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 44:loss 26.2982 with data size 3924\n","MAE:  2.7781942453539226\n","val epoch 44:loss 11.5475 with data size 564\n","Training complete in 51m 14s\n","Epoch 45/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 45:loss 27.4645 with data size 3924\n","MAE:  2.7652434945529234\n","val epoch 45:loss 11.4624 with data size 564\n","Training complete in 52m 24s\n","Epoch 46/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 46:loss 27.9752 with data size 3924\n","MAE:  2.77439724532425\n","val epoch 46:loss 11.5618 with data size 564\n","Training complete in 53m 34s\n","Epoch 47/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 47:loss 27.7846 with data size 3924\n","MAE:  2.8104770524387663\n","val epoch 47:loss 11.8870 with data size 564\n","Training complete in 54m 45s\n","Epoch 48/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 48:loss 26.2608 with data size 3924\n","MAE:  2.8729618096341056\n","val epoch 48:loss 12.3722 with data size 564\n","Training complete in 55m 54s\n","Epoch 49/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 49:loss 27.4074 with data size 3924\n","MAE:  2.7988145281659795\n","val epoch 49:loss 11.9073 with data size 564\n","Training complete in 57m 4s\n","Epoch 50/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 50:loss 26.8212 with data size 3924\n","MAE:  2.8861797668356846\n","val epoch 50:loss 12.3900 with data size 564\n","Training complete in 58m 14s\n","Training complete in 58m 14s\n"]}]},{"cell_type":"code","source":["import jhmodel32_mc4 as M14\n","\n","# Train from ChestXpert\n","PATH_TO_IMAGES = \"/content/gdrive/MyDrive/ColabNotebooks/brixia/DB\"\n","WEIGHT_DECAY = 0\n","LEARNING_RATE = 0.00001\n","M14.train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY, fine_tune=True, regression=True, freeze=True, adam=True, initial_model_path='/content/gdrive/MyDrive/ColabNotebooks/brixia/checkpoints/checkpoint_best')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qEEpIxaSOdKb","executionInfo":{"status":"ok","timestamp":1639521322769,"user_tz":-540,"elapsed":3490345,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"87f845b9-4668-423a-f55c-cf37236d7d5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Available GPU count:1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["DataParallel(\n","  (module): DenseNet(\n","    (features): Sequential(\n","      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (denseblock1): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition1): _Transition(\n","        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock2): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition2): _Transition(\n","        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock3): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer17): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer18): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer19): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer20): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer21): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer22): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer23): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer24): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (transition3): _Transition(\n","        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","      )\n","      (denseblock4): _DenseBlock(\n","        (denselayer1): _DenseLayer(\n","          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer2): _DenseLayer(\n","          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer3): _DenseLayer(\n","          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer4): _DenseLayer(\n","          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer5): _DenseLayer(\n","          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer6): _DenseLayer(\n","          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer7): _DenseLayer(\n","          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer8): _DenseLayer(\n","          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer9): _DenseLayer(\n","          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer10): _DenseLayer(\n","          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer11): _DenseLayer(\n","          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer12): _DenseLayer(\n","          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer13): _DenseLayer(\n","          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer14): _DenseLayer(\n","          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer15): _DenseLayer(\n","          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (denselayer16): _DenseLayer(\n","          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu1): ReLU(inplace=True)\n","          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu2): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (classifier): Sequential(\n","      (0): Linear(in_features=1024, out_features=1, bias=True)\n","      (1): ReLU(inplace=True)\n","      (2): Dropout2d(p=0.3, inplace=False)\n","    )\n","  )\n",")\n","Epoch 1/50\n","----------\n","train epoch 1:loss 73.2656 with data size 3924\n","MAE:  6.208262842009835\n","val epoch 1:loss 49.6558 with data size 564\n","saving\n","Training complete in 1m 10s\n","Epoch 2/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 2:loss 50.3926 with data size 3924\n","MAE:  5.108866974883151\n","val epoch 2:loss 33.7899 with data size 564\n","saving\n","Training complete in 2m 20s\n","Epoch 3/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 3:loss 41.3535 with data size 3924\n","MAE:  4.756444049325395\n","val epoch 3:loss 29.4492 with data size 564\n","saving\n","Training complete in 3m 30s\n","Epoch 4/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 4:loss 36.5597 with data size 3924\n","MAE:  4.238501724608718\n","val epoch 4:loss 24.0595 with data size 564\n","saving\n","Training complete in 4m 40s\n","Epoch 5/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 5:loss 34.7490 with data size 3924\n","MAE:  3.909350368782138\n","val epoch 5:loss 20.8342 with data size 564\n","saving\n","Training complete in 5m 49s\n","Epoch 6/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 6:loss 34.2685 with data size 3924\n","MAE:  3.839891493796034\n","val epoch 6:loss 20.2457 with data size 564\n","saving\n","Training complete in 6m 59s\n","Epoch 7/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 7:loss 34.2007 with data size 3924\n","MAE:  3.497679175663713\n","val epoch 7:loss 17.0527 with data size 564\n","saving\n","Training complete in 8m 9s\n","Epoch 8/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 8:loss 32.1510 with data size 3924\n","MAE:  3.2372741045575615\n","val epoch 8:loss 14.8075 with data size 564\n","saving\n","Training complete in 9m 18s\n","Epoch 9/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 9:loss 32.8299 with data size 3924\n","MAE:  3.392420696134263\n","val epoch 9:loss 16.1450 with data size 564\n","Training complete in 10m 28s\n","Epoch 10/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 10:loss 31.9163 with data size 3924\n","MAE:  3.4145702111752745\n","val epoch 10:loss 16.3353 with data size 564\n","Training complete in 11m 37s\n","Epoch 11/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 11:loss 30.2467 with data size 3924\n","MAE:  3.2274868942744344\n","val epoch 11:loss 14.8030 with data size 564\n","saving\n","Training complete in 12m 47s\n","Epoch 12/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 12:loss 31.2971 with data size 3924\n","MAE:  3.047575331378913\n","val epoch 12:loss 13.5659 with data size 564\n","saving\n","Training complete in 13m 57s\n","Epoch 13/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 13:loss 30.9844 with data size 3924\n","MAE:  3.2073564353986836\n","val epoch 13:loss 14.6898 with data size 564\n","Training complete in 15m 6s\n","Epoch 14/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 14:loss 30.1685 with data size 3924\n","MAE:  2.9716625029736377\n","val epoch 14:loss 12.9138 with data size 564\n","saving\n","Training complete in 16m 16s\n","Epoch 15/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 15:loss 30.4382 with data size 3924\n","MAE:  2.938204986816074\n","val epoch 15:loss 12.7251 with data size 564\n","saving\n","Training complete in 17m 25s\n","Epoch 16/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 16:loss 29.0060 with data size 3924\n","MAE:  3.021797550802535\n","val epoch 16:loss 13.3362 with data size 564\n","Training complete in 18m 35s\n","Epoch 17/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 17:loss 29.3917 with data size 3924\n","MAE:  3.011117855575702\n","val epoch 17:loss 13.3621 with data size 564\n","Training complete in 19m 45s\n","Epoch 18/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 18:loss 28.5633 with data size 3924\n","MAE:  2.913523132496692\n","val epoch 18:loss 12.6763 with data size 564\n","saving\n","Training complete in 20m 55s\n","Epoch 19/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 19:loss 27.3128 with data size 3924\n","MAE:  2.9971497568123517\n","val epoch 19:loss 13.3986 with data size 564\n","Training complete in 22m 5s\n","Epoch 20/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 20:loss 27.8365 with data size 3924\n","MAE:  2.8255228291906365\n","val epoch 20:loss 12.0585 with data size 564\n","saving\n","Training complete in 23m 15s\n","Epoch 21/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 21:loss 27.2901 with data size 3924\n","MAE:  2.9599797644830765\n","val epoch 21:loss 13.0686 with data size 564\n","Training complete in 24m 25s\n","Epoch 22/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 22:loss 28.1656 with data size 3924\n","MAE:  2.881182244377779\n","val epoch 22:loss 12.4774 with data size 564\n","Training complete in 25m 35s\n","Epoch 23/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 23:loss 27.0279 with data size 3924\n","MAE:  2.8295828129647047\n","val epoch 23:loss 12.1200 with data size 564\n","Training complete in 26m 44s\n","Epoch 24/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 24:loss 27.3984 with data size 3924\n","MAE:  2.866445003936054\n","val epoch 24:loss 12.2483 with data size 564\n","Training complete in 27m 54s\n","Epoch 25/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 25:loss 28.3367 with data size 3924\n","MAE:  2.9044838399751813\n","val epoch 25:loss 12.6320 with data size 564\n","Training complete in 29m 4s\n","Epoch 26/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 26:loss 27.1851 with data size 3924\n","MAE:  2.9024459660740187\n","val epoch 26:loss 12.5875 with data size 564\n","Training complete in 30m 13s\n","Epoch 27/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 27:loss 27.0704 with data size 3924\n","MAE:  2.7927462428491165\n","val epoch 27:loss 11.7343 with data size 564\n","saving\n","Training complete in 31m 23s\n","Epoch 28/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 28:loss 26.5296 with data size 3924\n","MAE:  2.774206552628084\n","val epoch 28:loss 11.6078 with data size 564\n","saving\n","Training complete in 32m 33s\n","Epoch 29/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 29:loss 27.7321 with data size 3924\n","MAE:  2.848362794793244\n","val epoch 29:loss 12.3022 with data size 564\n","Training complete in 33m 43s\n","Epoch 30/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 30:loss 28.3375 with data size 3924\n","MAE:  2.7875902637945\n","val epoch 30:loss 11.7421 with data size 564\n","Training complete in 34m 52s\n","Epoch 31/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 31:loss 28.2055 with data size 3924\n","MAE:  2.9602542519635446\n","val epoch 31:loss 13.0652 with data size 564\n","Training complete in 36m 2s\n","Epoch 32/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 32:loss 28.6516 with data size 3924\n","MAE:  2.9492424337048058\n","val epoch 32:loss 13.0553 with data size 564\n","Training complete in 37m 11s\n","Epoch 33/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 33:loss 26.8646 with data size 3924\n","MAE:  2.98022583084749\n","val epoch 33:loss 13.3543 with data size 564\n","Training complete in 38m 21s\n","Epoch 34/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 34:loss 27.1282 with data size 3924\n","MAE:  2.722780185969586\n","val epoch 34:loss 11.2114 with data size 564\n","saving\n","Training complete in 39m 31s\n","Epoch 35/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 35:loss 29.3237 with data size 3924\n","MAE:  2.831082125187766\n","val epoch 35:loss 12.0930 with data size 564\n","Training complete in 40m 41s\n","Epoch 36/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 36:loss 27.5011 with data size 3924\n","MAE:  2.8071789325129055\n","val epoch 36:loss 11.8713 with data size 564\n","Training complete in 41m 50s\n","Epoch 37/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 37:loss 28.4974 with data size 3924\n","MAE:  2.784034385009015\n","val epoch 37:loss 11.6928 with data size 564\n","Training complete in 42m 60s\n","Epoch 38/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 38:loss 26.5215 with data size 3924\n","MAE:  2.8978546489135804\n","val epoch 38:loss 12.6341 with data size 564\n","Training complete in 44m 10s\n","Epoch 39/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 39:loss 27.1194 with data size 3924\n","MAE:  2.8069502613011825\n","val epoch 39:loss 11.8198 with data size 564\n","Training complete in 45m 19s\n","Epoch 40/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 40:loss 27.0254 with data size 3924\n","MAE:  2.8989192104720054\n","val epoch 40:loss 12.6024 with data size 564\n","Training complete in 46m 29s\n","Epoch 41/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 41:loss 27.8531 with data size 3924\n","MAE:  2.7635945028869817\n","val epoch 41:loss 11.5194 with data size 564\n","Training complete in 47m 40s\n","Epoch 42/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 42:loss 27.3107 with data size 3924\n","MAE:  2.7982767801335515\n","val epoch 42:loss 11.8412 with data size 564\n","Training complete in 48m 50s\n","Epoch 43/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 43:loss 26.3976 with data size 3924\n","MAE:  2.7704192737368403\n","val epoch 43:loss 11.6548 with data size 564\n","Training complete in 49m 60s\n","Epoch 44/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 44:loss 27.8370 with data size 3924\n","MAE:  2.823303023868419\n","val epoch 44:loss 12.0735 with data size 564\n","Training complete in 51m 10s\n","Epoch 45/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 45:loss 26.9315 with data size 3924\n","MAE:  2.7727547467204063\n","val epoch 45:loss 11.5908 with data size 564\n","Training complete in 52m 19s\n","Epoch 46/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 46:loss 27.1884 with data size 3924\n","MAE:  2.7972281966754733\n","val epoch 46:loss 11.8054 with data size 564\n","Training complete in 53m 29s\n","Epoch 47/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 47:loss 27.2478 with data size 3924\n","MAE:  2.8637609238742936\n","val epoch 47:loss 12.3106 with data size 564\n","Training complete in 54m 39s\n","Epoch 48/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 48:loss 26.1609 with data size 3924\n","MAE:  2.8587509572822998\n","val epoch 48:loss 12.2989 with data size 564\n","Training complete in 55m 49s\n","Epoch 49/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 49:loss 27.0403 with data size 3924\n","MAE:  2.800975485434029\n","val epoch 49:loss 11.7865 with data size 564\n","Training complete in 56m 59s\n","Epoch 50/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train epoch 50:loss 27.1285 with data size 3924\n","MAE:  2.7755977143104196\n","val epoch 50:loss 11.6493 with data size 564\n","Training complete in 58m 9s\n","Training complete in 58m 9s\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KijD9_PUydUH","executionInfo":{"status":"ok","timestamp":1639622551875,"user_tz":-540,"elapsed":53171,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"a9279c14-cbaa-4193-8fe0-82cce8072896"},"source":["#Eval Brixia\n","import torch\n","from torchvision import transforms\n","import cxr_dataset as CXR\n","\n","import eval_model as E\n","\n","import numpy\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","PATH_TO_IMAGES = \"/content/gdrive/MyDrive/ColabNotebooks/brixia/DB\"\n","# MODEL_PATH = '/content/gdrive/MyDrive/ColabNotebooks/brixia/results/regression_checkpoint_best'\n","MODEL_PATH10 = '/content/gdrive/MyDrive/ColabNotebooks/brixia/10results/regression_checkpoint_best'\n","MODEL_PATH11 = '/content/gdrive/MyDrive/ColabNotebooks/brixia/11results/regression_checkpoint_best'\n","MODEL_PATH12 = '/content/gdrive/MyDrive/ColabNotebooks/brixia/12results/regression_checkpoint_best'\n","MODEL_PATH13 = '/content/gdrive/MyDrive/ColabNotebooks/brixia/13results/regression_checkpoint_best'\n","MODEL_PATH14 = '/content/gdrive/MyDrive/ColabNotebooks/brixia/14results/regression_checkpoint_best'\n","\n","\n","\n","# use imagenet mean,std for normalization\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize(224),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(224),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ]),\n","}\n","\n","# create dataloader\n","dataset = CXR.CXRDataset(\n","    path_to_images=PATH_TO_IMAGES,\n","    fold='test',\n","    transform=data_transforms['val'],\n","    fine_tune=True,\n","    regression=True)\n","dataloader = torch.utils.data.DataLoader(\n","    dataset, 32, shuffle=False, num_workers=8)\n","\n","# saved_model = torch.load(MODEL_PATH)\n","# model = saved_model['model']\n","# model.to(device)\n","# del saved_model\n","\n","saved_model10 = torch.load(MODEL_PATH10)\n","model10 = saved_model10['model']\n","model10.to(device)\n","del saved_model10\n","\n","saved_model11 = torch.load(MODEL_PATH11)\n","model11 = saved_model11['model']\n","model11.to(device)\n","del saved_model11\n","\n","saved_model12 = torch.load(MODEL_PATH12)\n","model12 = saved_model12['model']\n","model12.to(device)\n","del saved_model12\n","\n","saved_model13 = torch.load(MODEL_PATH13)\n","model13 = saved_model13['model']\n","model13.to(device)\n","del saved_model13\n","\n","saved_model14 = torch.load(MODEL_PATH14)\n","model14 = saved_model14['model']\n","model14.to(device)\n","del saved_model14\n","\n","# mae, ground_truths, preds = E.evaluate_mae(dataloader, model)\n","# print('MAE : ', mae)\n","# print('True_df : ',ground_truths)\n","# print('Pred_df : ', preds)\n","\n","mae10, ground_truths10, preds10 = E.evaluate_mae(dataloader, model10)\n","print('MAE10 : ', mae10)\n","print('True_df10 : ',ground_truths10)\n","print('Pred_df10 : ', preds10)\n","\n","mae11, ground_truths11, preds11 = E.evaluate_mae(dataloader, model11)\n","print('MAE11 : ', mae11)\n","print('True_df11 : ',ground_truths11)\n","print('Pred_df11 : ', preds11)\n","\n","mae12, ground_truths12, preds12 = E.evaluate_mae(dataloader, model12)\n","print('MAE12 : ', mae12)\n","print('True_df12 : ',ground_truths12)\n","print('Pred_df12 : ', preds12)\n","\n","mae13, ground_truths13, preds13 = E.evaluate_mae(dataloader, model13)\n","print('MAE13 : ', mae13)\n","print('True_df13 : ',ground_truths13)\n","print('Pred_df13 : ', preds13)\n","\n","mae14, ground_truths14, preds14 = E.evaluate_mae(dataloader, model14)\n","print('MAE14 : ', mae14)\n","print('True_df14 : ',ground_truths14)\n","print('Pred_df14 : ', preds14)\n","\n","\n","maes = []\n","maes.append(mae10)\n","maes.extend([mae11,mae12,mae13,mae14])\n","print(maes)\n","mae_mean = numpy.mean(maes)\n","print(\"성능 : \", mae_mean)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["MAE10 :  2.6104558200939842\n","True_df10 :                    Image Index  true_score\n","0    14564261561865340756.jpg         2.0\n","1    11736208667372564600.jpg         4.0\n","2     9239594870393759636.jpg         9.0\n","3    11144427466904541752.jpg         3.0\n","4    16974554766317036034.jpg         6.0\n","..                        ...         ...\n","202   3928593347333653647.jpg        14.0\n","203  11582034330052535722.jpg        15.0\n","204  14872476706053263416.jpg        15.0\n","205   1158241960099300594.jpg        18.0\n","206  10409101678672828001.jpg         1.0\n","\n","[207 rows x 2 columns]\n","Pred_df10 :                    Image Index   pred_score\n","0    14564261561865340756.jpg        [0.0]\n","1    11736208667372564600.jpg        [0.0]\n","2     9239594870393759636.jpg   [6.954803]\n","3    11144427466904541752.jpg  [2.8496277]\n","4    16974554766317036034.jpg  [3.8621354]\n","..                        ...          ...\n","202   3928593347333653647.jpg  [7.6085105]\n","203  11582034330052535722.jpg   [9.436718]\n","204  14872476706053263416.jpg  [11.681939]\n","205   1158241960099300594.jpg  [10.918933]\n","206  10409101678672828001.jpg  [1.9247411]\n","\n","[207 rows x 2 columns]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["MAE11 :  2.717447531684009\n","True_df11 :                    Image Index  true_score\n","0    14564261561865340756.jpg         2.0\n","1    11736208667372564600.jpg         4.0\n","2     9239594870393759636.jpg         9.0\n","3    11144427466904541752.jpg         3.0\n","4    16974554766317036034.jpg         6.0\n","..                        ...         ...\n","202   3928593347333653647.jpg        14.0\n","203  11582034330052535722.jpg        15.0\n","204  14872476706053263416.jpg        15.0\n","205   1158241960099300594.jpg        18.0\n","206  10409101678672828001.jpg         1.0\n","\n","[207 rows x 2 columns]\n","Pred_df11 :                    Image Index   pred_score\n","0    14564261561865340756.jpg  [4.1382194]\n","1    11736208667372564600.jpg  [2.8722217]\n","2     9239594870393759636.jpg  [7.0070944]\n","3    11144427466904541752.jpg  [3.4236646]\n","4    16974554766317036034.jpg  [4.1092873]\n","..                        ...          ...\n","202   3928593347333653647.jpg   [8.303848]\n","203  11582034330052535722.jpg   [9.577904]\n","204  14872476706053263416.jpg  [10.631948]\n","205   1158241960099300594.jpg   [9.945515]\n","206  10409101678672828001.jpg  [3.1518376]\n","\n","[207 rows x 2 columns]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["MAE12 :  2.616178752431547\n","True_df12 :                    Image Index  true_score\n","0    14564261561865340756.jpg         2.0\n","1    11736208667372564600.jpg         4.0\n","2     9239594870393759636.jpg         9.0\n","3    11144427466904541752.jpg         3.0\n","4    16974554766317036034.jpg         6.0\n","..                        ...         ...\n","202   3928593347333653647.jpg        14.0\n","203  11582034330052535722.jpg        15.0\n","204  14872476706053263416.jpg        15.0\n","205   1158241960099300594.jpg        18.0\n","206  10409101678672828001.jpg         1.0\n","\n","[207 rows x 2 columns]\n","Pred_df12 :                    Image Index   pred_score\n","0    14564261561865340756.jpg        [0.0]\n","1    11736208667372564600.jpg   [2.039786]\n","2     9239594870393759636.jpg  [6.9008274]\n","3    11144427466904541752.jpg  [2.4216886]\n","4    16974554766317036034.jpg  [4.0344796]\n","..                        ...          ...\n","202   3928593347333653647.jpg   [8.024012]\n","203  11582034330052535722.jpg    [9.20744]\n","204  14872476706053263416.jpg   [12.54229]\n","205   1158241960099300594.jpg  [10.376337]\n","206  10409101678672828001.jpg   [2.672809]\n","\n","[207 rows x 2 columns]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["MAE13 :  2.7080195794142963\n","True_df13 :                    Image Index  true_score\n","0    14564261561865340756.jpg         2.0\n","1    11736208667372564600.jpg         4.0\n","2     9239594870393759636.jpg         9.0\n","3    11144427466904541752.jpg         3.0\n","4    16974554766317036034.jpg         6.0\n","..                        ...         ...\n","202   3928593347333653647.jpg        14.0\n","203  11582034330052535722.jpg        15.0\n","204  14872476706053263416.jpg        15.0\n","205   1158241960099300594.jpg        18.0\n","206  10409101678672828001.jpg         1.0\n","\n","[207 rows x 2 columns]\n","Pred_df13 :                    Image Index   pred_score\n","0    14564261561865340756.jpg        [0.0]\n","1    11736208667372564600.jpg  [1.2352879]\n","2     9239594870393759636.jpg  [6.5882554]\n","3    11144427466904541752.jpg  [2.7132149]\n","4    16974554766317036034.jpg   [3.983834]\n","..                        ...          ...\n","202   3928593347333653647.jpg   [7.140296]\n","203  11582034330052535722.jpg   [10.55148]\n","204  14872476706053263416.jpg  [11.617017]\n","205   1158241960099300594.jpg  [11.054333]\n","206  10409101678672828001.jpg  [2.2194102]\n","\n","[207 rows x 2 columns]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["MAE14 :  2.749016890203319\n","True_df14 :                    Image Index  true_score\n","0    14564261561865340756.jpg         2.0\n","1    11736208667372564600.jpg         4.0\n","2     9239594870393759636.jpg         9.0\n","3    11144427466904541752.jpg         3.0\n","4    16974554766317036034.jpg         6.0\n","..                        ...         ...\n","202   3928593347333653647.jpg        14.0\n","203  11582034330052535722.jpg        15.0\n","204  14872476706053263416.jpg        15.0\n","205   1158241960099300594.jpg        18.0\n","206  10409101678672828001.jpg         1.0\n","\n","[207 rows x 2 columns]\n","Pred_df14 :                    Image Index   pred_score\n","0    14564261561865340756.jpg        [0.0]\n","1    11736208667372564600.jpg        [0.0]\n","2     9239594870393759636.jpg  [6.6589656]\n","3    11144427466904541752.jpg  [2.9542997]\n","4    16974554766317036034.jpg  [3.5490096]\n","..                        ...          ...\n","202   3928593347333653647.jpg    [8.75477]\n","203  11582034330052535722.jpg   [8.620264]\n","204  14872476706053263416.jpg  [9.8975935]\n","205   1158241960099300594.jpg  [10.804035]\n","206  10409101678672828001.jpg  [3.1407874]\n","\n","[207 rows x 2 columns]\n","[2.6104558200939842, 2.717447531684009, 2.616178752431547, 2.7080195794142963, 2.749016890203319]\n","성능 :  2.680223714765431\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# ## 일부 출력\n","# pd.options.display.max_rows = 30\n","# pd.options.display.max_columns = 5\n","\n","# 예측값 table(각 방법별 예측값 + var(uncertainty) + mean(예측값 평균))\n","preds_outer = pd.merge(preds10, preds11, how=\"outer\", on=\"Image Index\")\n","preds_outer = pd.merge(preds_outer, preds12, how=\"outer\", on=\"Image Index\")\n","preds_outer = pd.merge(preds_outer, preds13, how=\"outer\", on=\"Image Index\")\n","preds_outer = pd.merge(preds_outer, preds14, how=\"outer\", on=\"Image Index\")\n","\n","print(preds_outer) \n","print(type(preds_outer))\n","print(preds_outer.columns)\n","\n","# 각 image 별 variance, mean 추가\n","preds_outer['Var']= preds_outer[['pred_score_x','pred_score_y','pred_score_x','pred_score_y','pred_score']].var(axis=1).values\n","preds_outer['Mean']= preds_outer[['pred_score_x','pred_score_y','pred_score_x','pred_score_y','pred_score']].mean(axis=1).values\n","\n","print(preds_outer)\n"],"metadata":{"id":"UDYx9qa5P2ee","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639622554631,"user_tz":-540,"elapsed":260,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"56c0915b-477e-4d85-a241-df502df87793"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                  Image Index pred_score_x  ... pred_score_y   pred_score\n","0    14564261561865340756.jpg        [0.0]  ...        [0.0]        [0.0]\n","1    11736208667372564600.jpg        [0.0]  ...  [1.2352879]        [0.0]\n","2     9239594870393759636.jpg   [6.954803]  ...  [6.5882554]  [6.6589656]\n","3    11144427466904541752.jpg  [2.8496277]  ...  [2.7132149]  [2.9542997]\n","4    16974554766317036034.jpg  [3.8621354]  ...   [3.983834]  [3.5490096]\n","..                        ...          ...  ...          ...          ...\n","202   3928593347333653647.jpg  [7.6085105]  ...   [7.140296]    [8.75477]\n","203  11582034330052535722.jpg   [9.436718]  ...   [10.55148]   [8.620264]\n","204  14872476706053263416.jpg  [11.681939]  ...  [11.617017]  [9.8975935]\n","205   1158241960099300594.jpg  [10.918933]  ...  [11.054333]  [10.804035]\n","206  10409101678672828001.jpg  [1.9247411]  ...  [2.2194102]  [3.1407874]\n","\n","[207 rows x 6 columns]\n","<class 'pandas.core.frame.DataFrame'>\n","Index(['Image Index', 'pred_score_x', 'pred_score_y', 'pred_score_x',\n","       'pred_score_y', 'pred_score'],\n","      dtype='object')\n","                  Image Index pred_score_x  ...       Var       Mean\n","0    14564261561865340756.jpg        [0.0]  ...  3.329834   0.919604\n","1    11736208667372564600.jpg        [0.0]  ...  1.384678   1.366066\n","2     9239594870393759636.jpg   [6.954803]  ...  0.031141   6.840103\n","3    11144427466904541752.jpg  [2.8496277]  ...  0.133971   2.863410\n","4    16974554766317036034.jpg  [3.8621354]  ...  0.030436   3.947609\n","..                        ...          ...  ...       ...        ...\n","202   3928593347333653647.jpg  [7.6085105]  ...  0.300965   7.878678\n","203  11582034330052535722.jpg   [9.436718]  ...  0.390875   9.574150\n","204  14872476706053263416.jpg  [11.681939]  ...  0.786655  11.427109\n","205   1158241960099300594.jpg  [10.918933]  ...  0.201831  10.599364\n","206  10409101678672828001.jpg  [1.9247411]  ...  0.262782   2.564265\n","\n","[207 rows x 8 columns]\n"]}]},{"cell_type":"code","metadata":{"id":"R9FU3KOE2tdr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639622778229,"user_tz":-540,"elapsed":3,"user":{"displayName":"‍김지현[학생](소프트웨어융합대학 컴퓨터공학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10635065983091343623"}},"outputId":"7f9dc237-ac16-4c44-cffb-88f8df2d5c81"},"source":["pd.set_option('display.max_rows',None) # 모든 행 출력\n","pd.set_option('display.max_columns',None) # 모든 열 출력\n","\n","## 초기화\n","# pd.options.display.max_rows = 60\n","# pd.options.display.max_columns = 20\n","\n","## 결과 table (image index + pred + uncertainty + ground_truth)\n","results_outer = preds_outer.loc[:,['Image Index','Mean','Var']]\n","results_outer = pd.merge(results_outer, ground_truths10, how=\"outer\", on=\"Image Index\")\n","\n","error = abs(results_outer['true_score'].values - results_outer['Mean'].values)\n","results_outer['error'] = error\n","error_mean = error.mean()\n","print(\"error_mean : \", error_mean)\n","\n","print(results_outer)\n","\n","# 오차 순으로 정렬\n","sort_results = results_outer.sort_values('error',ascending=False) # 내림차순\n","print(\"\\n오차 높은 순 : \\n\",sort_results)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["error_mean :  2.615488037522992\n","                  Image Index       Mean       Var  true_score     error\n","0    14564261561865340756.jpg   0.919604  3.329834         2.0  1.080396\n","1    11736208667372564600.jpg   1.366066  1.384678         4.0  2.633934\n","2     9239594870393759636.jpg   6.840103  0.031141         9.0  2.159897\n","3    11144427466904541752.jpg   2.863410  0.133971         3.0  0.136590\n","4    16974554766317036034.jpg   3.947609  0.030436         6.0  2.052391\n","5     4142434283091893131.jpg   4.981846  0.091250         7.0  2.018154\n","6    16991136107639463000.jpg   8.572841  0.430458        12.0  3.427159\n","7    12639377867861591927.jpg   3.087121  0.524023         2.0  1.087121\n","8     3454636689776663066.jpg   2.449304  0.648927         0.0  2.449304\n","9     2039478727196349394.jpg   2.664124  0.206266         2.0  0.664124\n","10    2983640989459679908.jpg   1.273836  0.700673         6.0  4.726164\n","11   13803068907556703133.jpg   4.866043  0.620948         5.0  0.133957\n","12     572300713838543755.jpg   1.852240  0.160257         1.0  0.852240\n","13   15762036970094951713.jpg   3.451200  0.265646         2.0  1.451200\n","14   15425337615821266218.jpg   2.861951  0.320182         2.0  0.861951\n","15     294424044587444419.jpg   1.800451  0.364135         2.0  0.199549\n","16   10373203610977600991.jpg   3.457144  0.029761         3.0  0.457144\n","17    5535439021548898675.jpg   2.434390  0.120664         3.0  0.565610\n","18   10205244906786130777.jpg   4.808744  0.459399         3.0  1.808744\n","19     526494555634334802.jpg   5.145140  0.048502         2.0  3.145140\n","20    5016964555276229843.jpg   4.284600  0.138894         3.0  1.284600\n","21    8937742010989582869.jpg   1.571414  0.924433         6.0  4.428586\n","22    5036118436046592072.jpg   6.238516  0.495452         7.0  0.761484\n","23   11285701017062321210.jpg   2.193288  0.738310         2.0  0.193288\n","24     879069476603009827.jpg   4.963134  0.534181         5.0  0.036866\n","25    4511684798608619195.jpg   4.700758  0.059349         5.0  0.299242\n","26    3680690099872516380.jpg   3.291167  0.205967         6.0  2.708833\n","27   16124499320403313494.jpg   5.445628  0.231451         6.0  0.554372\n","28   12855383870084290348.jpg   5.779583  0.369641         6.0  0.220417\n","29   13995121267812712941.jpg   4.753319  0.212716         7.0  2.246681\n","30   17366091821823979653.jpg   4.333747  0.952616         7.0  2.666253\n","31   12404538278390586303.jpg   5.199984  0.263384         1.0  4.199984\n","32   13492091318378263421.jpg   1.288166  0.473129         5.0  3.711834\n","33    1327698747439336495.jpg   3.324965  0.322434         6.0  2.675035\n","34     422370180926276168.jpg   7.010358  0.271310         2.0  5.010358\n","35   14453742178151491113.jpg   3.236825  0.274808         3.0  0.236825\n","36    5668867232073089145.jpg   6.780228  0.974692         8.0  1.219772\n","37    7369641999034162857.jpg   5.070231  0.237022         5.0  0.070231\n","38   10296006099944433448.jpg   3.917879  0.424151         5.0  1.082121\n","39    3525622492256443693.jpg   4.170854  0.255471         6.0  1.829146\n","40   17379184182832557461.jpg   6.859052  0.430893         6.0  0.859052\n","41   14524260746371325603.jpg   6.924786  0.098149         6.0  0.924786\n","42    8547843019812361193.jpg   1.978988  1.015643         6.0  4.021012\n","43   12759961121952434610.jpg   7.755245  0.385492         6.0  1.755245\n","44     701539317072750190.jpg   4.176209  0.149519         7.0  2.823791\n","45    7918905941581655079.jpg   4.032601  0.258562         7.0  2.967399\n","46    2901493158366632509.jpg   4.546970  0.570231         9.0  4.453030\n","47   16968721655500901026.jpg   5.002861  0.274075         7.0  1.997139\n","48   13820293605776796256.jpg   7.817309  0.040212         8.0  0.182691\n","49   11055529703622995004.jpg   8.123082  0.452046         9.0  0.876918\n","50    7206473749624196973.jpg   6.500760  0.853747         9.0  2.499240\n","51    6885968685089799661.jpg   3.643191  0.449372         5.0  1.356809\n","52    3487129271708740243.jpg   5.833191  0.332859         5.0  0.833191\n","53   16327605739648055591.jpg   8.093487  1.020123         8.0  0.093487\n","54    4685635530768467994.jpg   6.126462  0.536703         6.0  0.126462\n","55    3486727694297422605.jpg   4.764553  0.845187         3.0  1.764553\n","56    8512646813904294057.jpg   5.371155  0.586274         5.0  0.371155\n","57    5679274923605146168.jpg   3.129203  0.160576         5.0  1.870797\n","58    3009200414969053735.jpg   4.238074  0.733181         8.0  3.761926\n","59   12930357210006582284.jpg   5.664096  0.432452         7.0  1.335904\n","60   15911053845348818443.jpg   6.162035  1.273830         7.0  0.837965\n","61   13960148861309105875.jpg   6.666262  0.239682         8.0  1.333738\n","62     700501741455133899.jpg   5.920810  0.162068         8.0  2.079190\n","63   15009619040643585516.jpg   4.323803  1.319629         8.0  3.676197\n","64    3058842895220545959.jpg   9.154387  0.336131         9.0  0.154387\n","65    8214560074603274352.jpg   9.453115  0.339199         9.0  0.453115\n","66   15846637757672649797.jpg   7.212406  1.096005         8.0  0.787594\n","67   12402574645219118293.jpg   3.994083  0.157292         6.0  2.005917\n","68   13708500480503630353.jpg   5.667984  0.468135         8.0  2.332016\n","69   10407083070451963222.jpg   5.885489  0.095444         9.0  3.114511\n","70   16101741193325832695.jpg   6.413368  0.158353        10.0  3.586632\n","71   13511283918805529640.jpg   0.883723  0.683321         2.0  1.116277\n","72    4089832714512704162.jpg   3.811539  0.120654         4.0  0.188461\n","73    1885702324025473837.jpg   4.521259  0.284907         4.0  0.521259\n","74    1438457779960319055.jpg   7.927817  0.347564         6.0  1.927817\n","75     551646877960726377.jpg   4.271382  0.244182         7.0  2.728618\n","76   11076610434242984246.jpg   6.732038  0.090768         6.0  0.732038\n","77   14387810907388261211.jpg   6.789079  0.291317         8.0  1.210921\n","78    1224301723771025234.jpg   7.635756  0.235519         8.0  0.364244\n","79   10398834035371500301.jpg   5.891251  0.557438        10.0  4.108749\n","80    9294106011252864899.jpg   5.151598  0.492271        10.0  4.848402\n","81    2456695361432041109.jpg   4.559432  0.256239        10.0  5.440568\n","82    8918563447893123953.jpg   7.872875  0.095141        10.0  2.127125\n","83    2856323440009083731.jpg   7.510116  0.141961         7.0  0.510116\n","84    9169168747345842200.jpg   5.749379  0.255968         7.0  1.250621\n","85   17443536986879183445.jpg   6.047211  0.120435         7.0  0.952789\n","86    7844469693574928360.jpg   6.693530  0.101326         8.0  1.306470\n","87    5336318065350528678.jpg   8.219337  0.159154         9.0  0.780663\n","88    7855502441156508375.jpg   4.636971  0.081163         5.0  0.363029\n","89   15234431379186037959.jpg   5.911577  0.137192         7.0  1.088423\n","90    5044535569812504047.jpg   8.921316  0.577207         8.0  0.921316\n","91   11225080889583929682.jpg   4.908852  0.151299         8.0  3.091148\n","92   10235319580258757961.jpg   5.686966  0.441018         9.0  3.313034\n","93    1404677752780903194.jpg   6.080882  0.388299        10.0  3.919118\n","94   10448608101256035142.jpg   8.500245  0.691449        11.0  2.499755\n","95     127680341107383304.jpg   8.079474  0.244594        11.0  2.920526\n","96   15122067243893514166.jpg   6.835390  0.549907        11.0  4.164610\n","97    9574093710263620216.jpg   4.958447  0.530769        11.0  6.041553\n","98    2210116834028526015.jpg   5.309675  0.250731        11.0  5.690325\n","99    6431759861562598072.jpg   7.812029  0.415669        11.0  3.187971\n","100    789773703383221230.jpg   5.848468  0.878672        12.0  6.151532\n","101    724052992658679116.jpg   6.320733  0.086337        12.0  5.679267\n","102  15530405318545258995.jpg   9.365613  0.566783        13.0  3.634387\n","103  14039895209958512122.jpg   6.511731  0.441214        11.0  4.488269\n","104  11595347288921948716.jpg   8.522508  0.383296        12.0  3.477492\n","105    740674916090105554.jpg   7.449885  0.237642         9.0  1.550115\n","106  17401037664688497891.jpg   7.920073  0.424150        12.0  4.079927\n","107  13571083746881062249.jpg   9.261909  0.863758        12.0  2.738091\n","108  17448594535873890811.jpg   9.704287  0.264940        13.0  3.295713\n","109   1502218282231063179.jpg   8.887533  0.263973        14.0  5.112467\n","110  11396538255811764407.jpg  11.092542  0.779347        14.0  2.907458\n","111   4371317283545880205.jpg   9.949109  0.065560        14.0  4.050891\n","112   6603826747595991577.jpg   9.894940  0.147133        15.0  5.105060\n","113   5205747043902377958.jpg   7.809737  1.441059        14.0  6.190263\n","114   7112367707399906424.jpg   6.601475  0.467015         7.0  0.398525\n","115  17012320413194239489.jpg   8.191952  0.058589        11.0  2.808048\n","116  18263543377997239110.jpg   4.477358  0.392694         4.0  0.477358\n","117   4417384638105538618.jpg   7.994229  0.373424        11.0  3.005771\n","118   1567705254240928761.jpg   6.532880  0.043715        11.0  4.467120\n","119  14913714808821522050.jpg   6.988352  0.146471        11.0  4.011648\n","120   7935788416185633259.jpg  10.018019  0.651623        12.0  1.981981\n","121   3908991455051408917.jpg   8.143811  0.460961        13.0  4.856189\n","122  11233008470219230724.jpg   8.563897  0.434007        14.0  5.436103\n","123  13174995223987338592.jpg   8.332397  0.029354        11.0  2.667603\n","124   7509680904508750014.jpg   9.679603  0.378827        14.0  4.320397\n","125   5721602360069692395.jpg   9.178819  0.925260        15.0  5.821181\n","126   3199381738999034391.jpg   8.865958  0.537391        15.0  6.134042\n","127  11274492157852958103.jpg   9.329132  0.340805        15.0  5.670868\n","128  15317126879413595288.jpg   8.612810  0.237765        12.0  3.387190\n","129  14210947516529320000.jpg   8.256905  1.191671        15.0  6.743095\n","130    724555669548785696.jpg   9.166905  1.116918        16.0  6.833095\n","131   4996010669635053871.jpg   7.031710  0.168284        13.0  5.968290\n","132  10428894458154086946.jpg  10.655558  0.222755        14.0  3.344442\n","133   4456005988868558577.jpg  11.716331  0.308417        15.0  3.283669\n","134  11013666393812119538.jpg  11.668287  0.501435        15.0  3.331713\n","135  12987010928073159787.jpg   5.435742  0.510848        10.0  4.564258\n","136   7939785837873120478.jpg   7.000137  0.388850        10.0  2.999863\n","137   7646167143408301074.jpg   8.619029  0.367871        11.0  2.380971\n","138   9869423143784974903.jpg  10.223626  0.168055        14.0  3.776374\n","139   9802840775783361565.jpg   8.602101  0.495418        14.0  5.397899\n","140    827773125951719560.jpg   9.600615  0.209070        17.0  7.399385\n","141  18410028717932798285.jpg   9.405735  0.706941        15.0  5.594265\n","142  14024777547462937721.jpg   9.133057  0.371486        16.0  6.866943\n","143   9339187043384491302.jpg   9.503335  0.226909        17.0  7.496665\n","144    631300793353391324.jpg   7.674472  0.708686        17.0  9.325528\n","145   1679513937101985928.jpg   1.665093  0.133449         0.0  1.665093\n","146   8263499986124197662.jpg   2.058931  1.041254         2.0  0.058931\n","147   4956613661345261232.jpg   3.487528  0.764728         2.0  1.487528\n","148   8499693214839855078.jpg   3.525883  0.367627         2.0  1.525883\n","149   1946623206506299537.jpg   5.175691  0.154467         4.0  1.175691\n","150    919954549896890902.jpg   3.749157  0.084182         4.0  0.250843\n","151   8052633053063443866.jpg   2.223439  0.260840         4.0  1.776561\n","152   2299335082352189080.jpg   5.042733  0.207883         2.0  3.042733\n","153   5367185614279251937.jpg   2.410396  0.024906         3.0  0.589604\n","154  17412341137186697628.jpg   2.310610  0.296181         2.0  0.310610\n","155  11898288065396820524.jpg   4.382966  0.068024         4.0  0.382966\n","156  12317395772950405702.jpg   2.630502  0.085786         5.0  2.369498\n","157  17778812536699433843.jpg   1.386244  2.750195         2.0  0.613756\n","158  15433824899881041434.jpg   1.101978  0.459345         2.0  0.898022\n","159   1320783291039324280.jpg   7.580440  0.552720         6.0  1.580440\n","160   9664315593676699896.jpg   4.119251  0.288092         5.0  0.880749\n","161  11057106957076205843.jpg   3.239836  0.454626         4.0  0.760164\n","162  11667591376836088663.jpg   0.977214  2.057975         3.0  2.022786\n","163   4381670668809352049.jpg   1.638071  0.622876         4.0  2.361929\n","164  16265679835822322441.jpg   3.365088  0.199292         4.0  0.634912\n","165  13568664663659470751.jpg   2.459363  0.159748         3.0  0.540637\n","166    391309107667684375.jpg   2.649603  0.520386         5.0  2.350397\n","167   7655485135798185122.jpg   5.242184  0.352720         7.0  1.757816\n","168   2664387616865925699.jpg   7.803740  0.552647         7.0  0.803740\n","169   9387547234153642404.jpg   6.569881  0.186016         7.0  0.430119\n","170   3937785176372682134.jpg   6.956457  0.175727         6.0  0.956457\n","171  13179048926650529025.jpg   8.564314  0.737768        10.0  1.435686\n","172   7714170411015443263.jpg   1.723703  0.328468         4.0  2.276297\n","173   6595464291274038880.jpg   4.398137  0.347480         8.0  3.601863\n","174   2221707185060046658.jpg   5.802379  0.068598         9.0  3.197621\n","175   6528469873635988809.jpg   6.731375  0.343905         9.0  2.268625\n","176  14762394268464634507.jpg   6.402305  0.053876         8.0  1.597695\n","177  16353113009188377219.jpg   1.539410  4.013780         1.0  0.539410\n","178  10941207710788663390.jpg   2.605577  0.335602         6.0  3.394423\n","179   6857181049706395877.jpg   5.817174  0.125682         8.0  2.182826\n","180   3227504669736347658.jpg   2.985775  0.367575         6.0  3.014225\n","181   6218186895970335354.jpg   5.331259  0.572452         7.0  1.668741\n","182  15596267132617691306.jpg   8.513709  0.414621         8.0  0.513709\n","183  13687843357647023628.jpg   5.026025  0.302134         8.0  2.973975\n","184   7058990005718775646.jpg   6.994973  0.127611         8.0  1.005027\n","185   2332022953375514885.jpg   9.033936  0.297903        10.0  0.966064\n","186   1555856448268447860.jpg   4.146529  0.362787         8.0  3.853471\n","187   1806007435224850850.jpg   7.684007  0.242907         9.0  1.315993\n","188   8815870952627201549.jpg   4.241393  0.134263         9.0  4.758607\n","189   3448830342143519057.jpg   6.167860  0.587486        10.0  3.832140\n","190      5032497707401895.jpg   5.772202  0.078204        11.0  5.227798\n","191   2626575196097470781.jpg   7.378615  0.719131        11.0  3.621385\n","192  16581884217027400434.jpg   8.085709  0.352669        12.0  3.914291\n","193   1009292177809823491.jpg   8.508621  0.216809        10.0  1.491379\n","194   1900650254908000249.jpg   6.395360  0.398465        13.0  6.604640\n","195   5848391000473468482.jpg   6.571359  0.283439        13.0  6.428641\n","196  13390998274239226475.jpg   8.334121  0.342056        14.0  5.665879\n","197    574758887276955038.jpg  11.005363  0.300450        14.0  2.994637\n","198  17611152957847534092.jpg  10.080960  0.206315        13.0  2.919040\n","199  13396045927013612210.jpg   8.531158  0.201336        15.0  6.468842\n","200  15398562563511216080.jpg   9.455035  0.560625        16.0  6.544965\n","201   8852704667215819661.jpg   8.294603  1.459854         9.0  0.705397\n","202   3928593347333653647.jpg   7.878678  0.300965        14.0  6.121322\n","203  11582034330052535722.jpg   9.574150  0.390875        15.0  5.425850\n","204  14872476706053263416.jpg  11.427109  0.786655        15.0  3.572891\n","205   1158241960099300594.jpg  10.599364  0.201831        18.0  7.400636\n","206  10409101678672828001.jpg   2.564265  0.262782         1.0  1.564265\n","\n","오차 높은 순 : \n","                   Image Index       Mean       Var  true_score     error\n","144    631300793353391324.jpg   7.674472  0.708686        17.0  9.325528\n","143   9339187043384491302.jpg   9.503335  0.226909        17.0  7.496665\n","205   1158241960099300594.jpg  10.599364  0.201831        18.0  7.400636\n","140    827773125951719560.jpg   9.600615  0.209070        17.0  7.399385\n","142  14024777547462937721.jpg   9.133057  0.371486        16.0  6.866943\n","130    724555669548785696.jpg   9.166905  1.116918        16.0  6.833095\n","129  14210947516529320000.jpg   8.256905  1.191671        15.0  6.743095\n","194   1900650254908000249.jpg   6.395360  0.398465        13.0  6.604640\n","200  15398562563511216080.jpg   9.455035  0.560625        16.0  6.544965\n","199  13396045927013612210.jpg   8.531158  0.201336        15.0  6.468842\n","195   5848391000473468482.jpg   6.571359  0.283439        13.0  6.428641\n","113   5205747043902377958.jpg   7.809737  1.441059        14.0  6.190263\n","100    789773703383221230.jpg   5.848468  0.878672        12.0  6.151532\n","126   3199381738999034391.jpg   8.865958  0.537391        15.0  6.134042\n","202   3928593347333653647.jpg   7.878678  0.300965        14.0  6.121322\n","97    9574093710263620216.jpg   4.958447  0.530769        11.0  6.041553\n","131   4996010669635053871.jpg   7.031710  0.168284        13.0  5.968290\n","125   5721602360069692395.jpg   9.178819  0.925260        15.0  5.821181\n","98    2210116834028526015.jpg   5.309675  0.250731        11.0  5.690325\n","101    724052992658679116.jpg   6.320733  0.086337        12.0  5.679267\n","127  11274492157852958103.jpg   9.329132  0.340805        15.0  5.670868\n","196  13390998274239226475.jpg   8.334121  0.342056        14.0  5.665879\n","141  18410028717932798285.jpg   9.405735  0.706941        15.0  5.594265\n","81    2456695361432041109.jpg   4.559432  0.256239        10.0  5.440568\n","122  11233008470219230724.jpg   8.563897  0.434007        14.0  5.436103\n","203  11582034330052535722.jpg   9.574150  0.390875        15.0  5.425850\n","139   9802840775783361565.jpg   8.602101  0.495418        14.0  5.397899\n","190      5032497707401895.jpg   5.772202  0.078204        11.0  5.227798\n","109   1502218282231063179.jpg   8.887533  0.263973        14.0  5.112467\n","112   6603826747595991577.jpg   9.894940  0.147133        15.0  5.105060\n","34     422370180926276168.jpg   7.010358  0.271310         2.0  5.010358\n","121   3908991455051408917.jpg   8.143811  0.460961        13.0  4.856189\n","80    9294106011252864899.jpg   5.151598  0.492271        10.0  4.848402\n","188   8815870952627201549.jpg   4.241393  0.134263         9.0  4.758607\n","10    2983640989459679908.jpg   1.273836  0.700673         6.0  4.726164\n","135  12987010928073159787.jpg   5.435742  0.510848        10.0  4.564258\n","103  14039895209958512122.jpg   6.511731  0.441214        11.0  4.488269\n","118   1567705254240928761.jpg   6.532880  0.043715        11.0  4.467120\n","46    2901493158366632509.jpg   4.546970  0.570231         9.0  4.453030\n","21    8937742010989582869.jpg   1.571414  0.924433         6.0  4.428586\n","124   7509680904508750014.jpg   9.679603  0.378827        14.0  4.320397\n","31   12404538278390586303.jpg   5.199984  0.263384         1.0  4.199984\n","96   15122067243893514166.jpg   6.835390  0.549907        11.0  4.164610\n","79   10398834035371500301.jpg   5.891251  0.557438        10.0  4.108749\n","106  17401037664688497891.jpg   7.920073  0.424150        12.0  4.079927\n","111   4371317283545880205.jpg   9.949109  0.065560        14.0  4.050891\n","42    8547843019812361193.jpg   1.978988  1.015643         6.0  4.021012\n","119  14913714808821522050.jpg   6.988352  0.146471        11.0  4.011648\n","93    1404677752780903194.jpg   6.080882  0.388299        10.0  3.919118\n","192  16581884217027400434.jpg   8.085709  0.352669        12.0  3.914291\n","186   1555856448268447860.jpg   4.146529  0.362787         8.0  3.853471\n","189   3448830342143519057.jpg   6.167860  0.587486        10.0  3.832140\n","138   9869423143784974903.jpg  10.223626  0.168055        14.0  3.776374\n","58    3009200414969053735.jpg   4.238074  0.733181         8.0  3.761926\n","32   13492091318378263421.jpg   1.288166  0.473129         5.0  3.711834\n","63   15009619040643585516.jpg   4.323803  1.319629         8.0  3.676197\n","102  15530405318545258995.jpg   9.365613  0.566783        13.0  3.634387\n","191   2626575196097470781.jpg   7.378615  0.719131        11.0  3.621385\n","173   6595464291274038880.jpg   4.398137  0.347480         8.0  3.601863\n","70   16101741193325832695.jpg   6.413368  0.158353        10.0  3.586632\n","204  14872476706053263416.jpg  11.427109  0.786655        15.0  3.572891\n","104  11595347288921948716.jpg   8.522508  0.383296        12.0  3.477492\n","6    16991136107639463000.jpg   8.572841  0.430458        12.0  3.427159\n","178  10941207710788663390.jpg   2.605577  0.335602         6.0  3.394423\n","128  15317126879413595288.jpg   8.612810  0.237765        12.0  3.387190\n","132  10428894458154086946.jpg  10.655558  0.222755        14.0  3.344442\n","134  11013666393812119538.jpg  11.668287  0.501435        15.0  3.331713\n","92   10235319580258757961.jpg   5.686966  0.441018         9.0  3.313034\n","108  17448594535873890811.jpg   9.704287  0.264940        13.0  3.295713\n","133   4456005988868558577.jpg  11.716331  0.308417        15.0  3.283669\n","174   2221707185060046658.jpg   5.802379  0.068598         9.0  3.197621\n","99    6431759861562598072.jpg   7.812029  0.415669        11.0  3.187971\n","19     526494555634334802.jpg   5.145140  0.048502         2.0  3.145140\n","69   10407083070451963222.jpg   5.885489  0.095444         9.0  3.114511\n","91   11225080889583929682.jpg   4.908852  0.151299         8.0  3.091148\n","152   2299335082352189080.jpg   5.042733  0.207883         2.0  3.042733\n","180   3227504669736347658.jpg   2.985775  0.367575         6.0  3.014225\n","117   4417384638105538618.jpg   7.994229  0.373424        11.0  3.005771\n","136   7939785837873120478.jpg   7.000137  0.388850        10.0  2.999863\n","197    574758887276955038.jpg  11.005363  0.300450        14.0  2.994637\n","183  13687843357647023628.jpg   5.026025  0.302134         8.0  2.973975\n","45    7918905941581655079.jpg   4.032601  0.258562         7.0  2.967399\n","95     127680341107383304.jpg   8.079474  0.244594        11.0  2.920526\n","198  17611152957847534092.jpg  10.080960  0.206315        13.0  2.919040\n","110  11396538255811764407.jpg  11.092542  0.779347        14.0  2.907458\n","44     701539317072750190.jpg   4.176209  0.149519         7.0  2.823791\n","115  17012320413194239489.jpg   8.191952  0.058589        11.0  2.808048\n","107  13571083746881062249.jpg   9.261909  0.863758        12.0  2.738091\n","75     551646877960726377.jpg   4.271382  0.244182         7.0  2.728618\n","26    3680690099872516380.jpg   3.291167  0.205967         6.0  2.708833\n","33    1327698747439336495.jpg   3.324965  0.322434         6.0  2.675035\n","123  13174995223987338592.jpg   8.332397  0.029354        11.0  2.667603\n","30   17366091821823979653.jpg   4.333747  0.952616         7.0  2.666253\n","1    11736208667372564600.jpg   1.366066  1.384678         4.0  2.633934\n","94   10448608101256035142.jpg   8.500245  0.691449        11.0  2.499755\n","50    7206473749624196973.jpg   6.500760  0.853747         9.0  2.499240\n","8     3454636689776663066.jpg   2.449304  0.648927         0.0  2.449304\n","137   7646167143408301074.jpg   8.619029  0.367871        11.0  2.380971\n","156  12317395772950405702.jpg   2.630502  0.085786         5.0  2.369498\n","163   4381670668809352049.jpg   1.638071  0.622876         4.0  2.361929\n","166    391309107667684375.jpg   2.649603  0.520386         5.0  2.350397\n","68   13708500480503630353.jpg   5.667984  0.468135         8.0  2.332016\n","172   7714170411015443263.jpg   1.723703  0.328468         4.0  2.276297\n","175   6528469873635988809.jpg   6.731375  0.343905         9.0  2.268625\n","29   13995121267812712941.jpg   4.753319  0.212716         7.0  2.246681\n","179   6857181049706395877.jpg   5.817174  0.125682         8.0  2.182826\n","2     9239594870393759636.jpg   6.840103  0.031141         9.0  2.159897\n","82    8918563447893123953.jpg   7.872875  0.095141        10.0  2.127125\n","62     700501741455133899.jpg   5.920810  0.162068         8.0  2.079190\n","4    16974554766317036034.jpg   3.947609  0.030436         6.0  2.052391\n","162  11667591376836088663.jpg   0.977214  2.057975         3.0  2.022786\n","5     4142434283091893131.jpg   4.981846  0.091250         7.0  2.018154\n","67   12402574645219118293.jpg   3.994083  0.157292         6.0  2.005917\n","47   16968721655500901026.jpg   5.002861  0.274075         7.0  1.997139\n","120   7935788416185633259.jpg  10.018019  0.651623        12.0  1.981981\n","74    1438457779960319055.jpg   7.927817  0.347564         6.0  1.927817\n","57    5679274923605146168.jpg   3.129203  0.160576         5.0  1.870797\n","39    3525622492256443693.jpg   4.170854  0.255471         6.0  1.829146\n","18   10205244906786130777.jpg   4.808744  0.459399         3.0  1.808744\n","151   8052633053063443866.jpg   2.223439  0.260840         4.0  1.776561\n","55    3486727694297422605.jpg   4.764553  0.845187         3.0  1.764553\n","167   7655485135798185122.jpg   5.242184  0.352720         7.0  1.757816\n","43   12759961121952434610.jpg   7.755245  0.385492         6.0  1.755245\n","181   6218186895970335354.jpg   5.331259  0.572452         7.0  1.668741\n","145   1679513937101985928.jpg   1.665093  0.133449         0.0  1.665093\n","176  14762394268464634507.jpg   6.402305  0.053876         8.0  1.597695\n","159   1320783291039324280.jpg   7.580440  0.552720         6.0  1.580440\n","206  10409101678672828001.jpg   2.564265  0.262782         1.0  1.564265\n","105    740674916090105554.jpg   7.449885  0.237642         9.0  1.550115\n","148   8499693214839855078.jpg   3.525883  0.367627         2.0  1.525883\n","193   1009292177809823491.jpg   8.508621  0.216809        10.0  1.491379\n","147   4956613661345261232.jpg   3.487528  0.764728         2.0  1.487528\n","13   15762036970094951713.jpg   3.451200  0.265646         2.0  1.451200\n","171  13179048926650529025.jpg   8.564314  0.737768        10.0  1.435686\n","51    6885968685089799661.jpg   3.643191  0.449372         5.0  1.356809\n","59   12930357210006582284.jpg   5.664096  0.432452         7.0  1.335904\n","61   13960148861309105875.jpg   6.666262  0.239682         8.0  1.333738\n","187   1806007435224850850.jpg   7.684007  0.242907         9.0  1.315993\n","86    7844469693574928360.jpg   6.693530  0.101326         8.0  1.306470\n","20    5016964555276229843.jpg   4.284600  0.138894         3.0  1.284600\n","84    9169168747345842200.jpg   5.749379  0.255968         7.0  1.250621\n","36    5668867232073089145.jpg   6.780228  0.974692         8.0  1.219772\n","77   14387810907388261211.jpg   6.789079  0.291317         8.0  1.210921\n","149   1946623206506299537.jpg   5.175691  0.154467         4.0  1.175691\n","71   13511283918805529640.jpg   0.883723  0.683321         2.0  1.116277\n","89   15234431379186037959.jpg   5.911577  0.137192         7.0  1.088423\n","7    12639377867861591927.jpg   3.087121  0.524023         2.0  1.087121\n","38   10296006099944433448.jpg   3.917879  0.424151         5.0  1.082121\n","0    14564261561865340756.jpg   0.919604  3.329834         2.0  1.080396\n","184   7058990005718775646.jpg   6.994973  0.127611         8.0  1.005027\n","185   2332022953375514885.jpg   9.033936  0.297903        10.0  0.966064\n","170   3937785176372682134.jpg   6.956457  0.175727         6.0  0.956457\n","85   17443536986879183445.jpg   6.047211  0.120435         7.0  0.952789\n","41   14524260746371325603.jpg   6.924786  0.098149         6.0  0.924786\n","90    5044535569812504047.jpg   8.921316  0.577207         8.0  0.921316\n","158  15433824899881041434.jpg   1.101978  0.459345         2.0  0.898022\n","160   9664315593676699896.jpg   4.119251  0.288092         5.0  0.880749\n","49   11055529703622995004.jpg   8.123082  0.452046         9.0  0.876918\n","14   15425337615821266218.jpg   2.861951  0.320182         2.0  0.861951\n","40   17379184182832557461.jpg   6.859052  0.430893         6.0  0.859052\n","12     572300713838543755.jpg   1.852240  0.160257         1.0  0.852240\n","60   15911053845348818443.jpg   6.162035  1.273830         7.0  0.837965\n","52    3487129271708740243.jpg   5.833191  0.332859         5.0  0.833191\n","168   2664387616865925699.jpg   7.803740  0.552647         7.0  0.803740\n","66   15846637757672649797.jpg   7.212406  1.096005         8.0  0.787594\n","87    5336318065350528678.jpg   8.219337  0.159154         9.0  0.780663\n","22    5036118436046592072.jpg   6.238516  0.495452         7.0  0.761484\n","161  11057106957076205843.jpg   3.239836  0.454626         4.0  0.760164\n","76   11076610434242984246.jpg   6.732038  0.090768         6.0  0.732038\n","201   8852704667215819661.jpg   8.294603  1.459854         9.0  0.705397\n","9     2039478727196349394.jpg   2.664124  0.206266         2.0  0.664124\n","164  16265679835822322441.jpg   3.365088  0.199292         4.0  0.634912\n","157  17778812536699433843.jpg   1.386244  2.750195         2.0  0.613756\n","153   5367185614279251937.jpg   2.410396  0.024906         3.0  0.589604\n","17    5535439021548898675.jpg   2.434390  0.120664         3.0  0.565610\n","27   16124499320403313494.jpg   5.445628  0.231451         6.0  0.554372\n","165  13568664663659470751.jpg   2.459363  0.159748         3.0  0.540637\n","177  16353113009188377219.jpg   1.539410  4.013780         1.0  0.539410\n","73    1885702324025473837.jpg   4.521259  0.284907         4.0  0.521259\n","182  15596267132617691306.jpg   8.513709  0.414621         8.0  0.513709\n","83    2856323440009083731.jpg   7.510116  0.141961         7.0  0.510116\n","116  18263543377997239110.jpg   4.477358  0.392694         4.0  0.477358\n","16   10373203610977600991.jpg   3.457144  0.029761         3.0  0.457144\n","65    8214560074603274352.jpg   9.453115  0.339199         9.0  0.453115\n","169   9387547234153642404.jpg   6.569881  0.186016         7.0  0.430119\n","114   7112367707399906424.jpg   6.601475  0.467015         7.0  0.398525\n","155  11898288065396820524.jpg   4.382966  0.068024         4.0  0.382966\n","56    8512646813904294057.jpg   5.371155  0.586274         5.0  0.371155\n","78    1224301723771025234.jpg   7.635756  0.235519         8.0  0.364244\n","88    7855502441156508375.jpg   4.636971  0.081163         5.0  0.363029\n","154  17412341137186697628.jpg   2.310610  0.296181         2.0  0.310610\n","25    4511684798608619195.jpg   4.700758  0.059349         5.0  0.299242\n","150    919954549896890902.jpg   3.749157  0.084182         4.0  0.250843\n","35   14453742178151491113.jpg   3.236825  0.274808         3.0  0.236825\n","28   12855383870084290348.jpg   5.779583  0.369641         6.0  0.220417\n","15     294424044587444419.jpg   1.800451  0.364135         2.0  0.199549\n","23   11285701017062321210.jpg   2.193288  0.738310         2.0  0.193288\n","72    4089832714512704162.jpg   3.811539  0.120654         4.0  0.188461\n","48   13820293605776796256.jpg   7.817309  0.040212         8.0  0.182691\n","64    3058842895220545959.jpg   9.154387  0.336131         9.0  0.154387\n","3    11144427466904541752.jpg   2.863410  0.133971         3.0  0.136590\n","11   13803068907556703133.jpg   4.866043  0.620948         5.0  0.133957\n","54    4685635530768467994.jpg   6.126462  0.536703         6.0  0.126462\n","53   16327605739648055591.jpg   8.093487  1.020123         8.0  0.093487\n","37    7369641999034162857.jpg   5.070231  0.237022         5.0  0.070231\n","146   8263499986124197662.jpg   2.058931  1.041254         2.0  0.058931\n","24     879069476603009827.jpg   4.963134  0.534181         5.0  0.036866\n"]}]}]}